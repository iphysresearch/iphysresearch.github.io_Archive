---
title: IPhysResearch
date: 2018-08-21
---
<div align="center"><a href="http://iphysresearch.github.io"><img src="https://i.loli.net/2018/07/11/5b44e3a6a798a.jpg" alt="Background" /></a></div>
<h1 align="center">🍺 Teaching is Learning,  Writing is Thinking 🍺 </h1>

---

<div align="center"><strong>Physics | Gravitational Waves | Machine Learning | Deep Learning</strong></div>
<div align="center"><a href="http://iphysresearch.github.io"><img src="https://img.shields.io/badge/Update-2018.8.17-green.svg?style=plastic" alt="Update" /></a><a href="https://github.com/iphysresearch"><img src="https://img.shields.io/github/followers/iphysresearch.svg?style=social&label=Follow" alt="GITHUB" /></a><a href="http://weibo.com/IPhysresearch"><img src="https://img.shields.io/badge/Weibo-@iPHYSresearch-blue.svg?style=plastic" alt="Tweet" /></a><a href="https://twitter.com/Herb_hewang"><img src="https://img.shields.io/twitter/url/https/github.com/iphysresearch/iphysresearch.github.io.svg?style=social" alt="Tweet" /></a></div>



[TOC]



# Welcome！

## About

Thanks for visiting！

I'm a PhD candidate majoring in theoretical physics. I love to share knowledge and have a keen passion for scientific research on data analysis of ***gravitational-wave***(GW) detection and ***deep learning*** (DL) technologies. 

Most of blog posts and notes here are written in **Chinese** and any future updates for *completion*. After some years of study, I wish I could accumulate a sufficient body of knowledge and achieve a view of my own on. Thus, as *S. Chandrasekhar* notes, "I have the urge to present my point of view *ab initio*, in a coherent account with order, form, and structure."


> "My scientific work has followed a certain pattern motivated, principally, by *a quest after perspectives*"—— S. Chandrasekhar

This site is currently **under construction** and I will make updates weekly and look forward to resuming blog posts in the fall.

## How to comment

With use of the [hypothes.is](https://hypothes.is/) extension (right-sided), you can highlight, annote any comments and discuss these notes inline*at any pages*and *posts*.

*Please Feel Free* to Let Me Know and *Share* it Here.



---

# My Learning Notes on ...

> “*Men Learn While They Teach*” —— Seneca.

## Data Science Courses

### CS231n

- From lecture video and slices

  - [Lecture 1. Computer vision overview & Historical context](./cs231n/cs231n_1.html)
  - [Lecture 2. Image Classification & K-nearest neighbor](./cs231n/cs231n_2.html)
  - [Lecture 3. Loss Functions and Optimization](./cs231n/cs231n_3.html)
  - Lecture 4. Introduction to Neural Networks
  - Lecture 5. Convolutional Neural Networks
  - Lecture 6. Training Neural Networks, part I
  - Lecture 7. Training Neural Networks, part II
  - Lecture 8. Deep Learning Hardware and Software
  - Lecture 9. CNN Architectures
  - Lecture 10. Recurrent Neural Networks
  - Lecture 11. Detection and Segmentation
  - Lecture 12. Generative Models
  - Lecture 13. Visualizing and Understanding
  - Lecture 14. Deep Reinforcement Learning 
  - Lecture 15. 

- From course notes

  | 讲义笔记                                                    |                             简介                             |
  | ----------------------------------------------------------- | :----------------------------------------------------------: |
  | [图像分类](./cs231n/CS231n_image_classification_note.html)  |   L1/L2 distances, hyperparameter search, cross-validation   |
  | [线性分类](./cs231n/CS231n_linear_classification_note.html) | parameteric approach, bias trick, hinge loss, cross-entropy loss, L2 regularization, web demo |
  | [最优化](./cs231n/CS231n_optimization_note.html)            | optimization landscapes, local search, learning rate, analytic/numerical gradient |
  |                                                             |                                                              |

- Others

  - [一段关于神经网络的故事](./cs231n/cs231n_story_MLP.html)（**Original**，30671字 + 多图）



## Books

- [Python 基础教程（第3版）](./books/Beginning_Python.html)（**Annotations**）
- 



## Data Analysis in Gravitational Wave Detection



<section class="links">
  <!-- you can include the HTML from other pens in your pen -->
  [[[https://codepen.io/team/codepen/pen/PZOVdL/]]]
</section>



<!-- I am some comments
not end, not end...
here the comment ends -->





---

# Paper Summary

> **Please note that these posts are for my future self to review the materials on these papers without reading them all over again.** (Inspired by [Jae Duk Seo ](https://jaedukseo.me) and also refering to [Deep Learning Papers Reading Roadmap](https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap) & [Awesome - Most Cited Deep Learning Papers](https://github.com/terryum/awesome-deep-learning-papers))



<details>
  <summary>I have keys but no locks. I have space but no room. You can enter but can't leave. What am I?</summary>
      A keyboard.
  </details>



## :rainbow: GW astronomy

- 



## :surfer: Survey & Review

- [Paper Summary] LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. "**Deep learning**." **(Three Giants' Survey)**

## :running_man: ImageNet Evolution

> Deep Learning broke out from here

- [Paper Summary] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "**Imagenet classification with deep convolutional neural networks**." (2012). **(AlexNet, Deep Learning Breakthrough)**
- [Paper Summary] Simonyan, Karen, and Andrew Zisserman. "**Very deep convolutional networks for large-scale image recognition**." (2014).**(VGGNet,Neural Networks become very deep!)**
- [Paper Summary] Szegedy, Christian, et al. "**Going deeper with convolutions**." (2015).**(GoogLeNet)**
- [Paper Summary] He, Kaiming, et al. "**Deep residual learning for image recognition**." (2015).**(ResNet,Very very deep networks, CVPR best paper)**

## :goal_net: Model

- [Paper Summary] Hinton, Geoffrey E., et al. "**Improving neural networks by preventing co-adaptation of feature detectors**." (2012). **(Dropout)**
- [Paper Summary] Srivastava, Nitish, et al. "**Dropout: a simple way to prevent neural networks from overfitting**." (2014)
- [Paper Summary] Ioffe, Sergey, and Christian Szegedy. "**Batch normalization: Accelerating deep network training by reducing internal covariate shift**." (2015).**(An outstanding Work in 2015)**
- [Paper Summary] Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. "**Layer normalization**." (2016).**(Update of Batch Normalization)**
- [Paper Summary] Courbariaux, Matthieu, et al. "**Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or−1**." **(New Model,Fast)**
- [Paper Summary] Jaderberg, Max, et al. "**Decoupled neural interfaces using synthetic gradients**." (2016). **(Innovation of Training Method,Amazing Work)**
- [Paper Summary] Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. "Net2net: Accelerating learning via knowledge transfer."(2015).**(Modify previously trained network to reduce training epochs)**
- [Paper Summary] Wei, Tao, et al. "**Network Morphism.**" (2016). **(Modify previously trained network to reduce training epochs)**



## :skier: Optimization

- [Paper Summary] Sutskever, Ilya, et al. "**On the importance of initialization and momentum in deep learning**." (2013) **(Momentum optimizer)**
- [Paper Summary] Kingma, Diederik, and Jimmy Ba. "**Adam: A method for stochastic optimization**." (2014). **(Maybe used most often currently)**
- [Paper Summary] Andrychowicz, Marcin, et al. "**Learning to learn by gradient descent by gradient descent**." (2016).**(Neural Optimizer,Amazing Work)**
- [Paper Summary] Han, Song, Huizi Mao, and William J. Dally. "**Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding**." (2015). **(ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup)**
- [Paper Summary] Iandola, Forrest N., et al. "**SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 1MB model size**." (2016).**(Also a new direction to optimize NN,DeePhi Tech Startup)**









---

# My Blog Posts

- [2018年个人计划和目标](./posts/2018_flag.html)
- [数据科学入门之我谈(2017)](./posts/MyWay2017.html)
- [S_Dbw 聚类评估指标（代码全解析）](./posts/S_Dbw.html)
- [Training Neural Networks with Mixed Precision: Theory and Practice](./posts/Training_Neural_Networks_with_Mixed_Precision_Theory_and_Practice.html)



---

# My Github Projects



<script>
    new GitHubCalendar(".calendar", "your-username");
</script>


<div align="left"><a href="https://github.com/iphysresearch/DataSciComp/"><h4 align="left">DataSciComp</a> 
<a href="https://github.com/iphysresearch/DataSciComp/watchers"><img src="https://img.shields.io/github/watchers/iphysresearch/DataSciComp.svg?style=social" alt="Github Watch Badge" /></a><a href="https://github.com/iphysresearch/DataSciComp/stargazers"><img src="https://img.shields.io/github/stars/iphysresearch/DataSciComp.svg?style=social" alt="Github Star Badge" /></a> </h4 >

> A collection of popular Data Science Competitions

<div align="left"><a href="https://github.com/iphysresearch/TOP250movie_douban"><h4 align="left">TOP250movie_douban</a> 
<a href="https://github.com/iphysresearch/TOP250movie_douban/watchers"><img src="https://img.shields.io/github/watchers/iphysresearch/TOP250movie_douban.svg?style=social" alt="Github Watch Badge" /></a><a href="https://github.com/iphysresearch/TOP250movie_douban/stargazers"><img src="https://img.shields.io/github/stars/iphysresearch/TOP250movie_douban.svg?style=social" alt="Github Star Badge" /></a> </h4 >
> TOP250豆瓣电影短评：Scrapy 爬虫+数据清理/分析+构建中文文本情感分析模型

<div align="left"><a href="https://github.com/iphysresearch/S_Dbw_validity_index"><h4 align="left">S_Dbw_validity_index</a>
<a href="https://github.com/iphysresearch/S_Dbw_validity_index/watchers"><img src="https://img.shields.io/github/watchers/iphysresearch/S_Dbw_validity_index.svg?style=social" alt="Github Watch Badge" /></a><a href="https://github.com/iphysresearch/S_Dbw_validity_index/stargazers"><img src="https://img.shields.io/github/stars/iphysresearch/S_Dbw_validity_index.svg?style=social" alt="Github Star Badge" /></a> </h4 >
> S_Dbw validity index | 代码全解析，可另见[博文](./post/S_Dbw.html)。


<iframe width="600" height="600" src="https://ionicabizau.github.io/github-profile-languages/api.html?iphysresearch" frameborder="0"></iframe>



---
<div align="center"><a href="http://iphysresearch.github.io"><img src="https://i.loli.net/2018/07/11/5b44d8c9d094f.jpeg" alt="Background" /></a></div>
> 本站点内容系本作者原创，如有任何知识产权、版权问题或理论错误，还请指正。
>
> 转载请注明原作者及出处，谢谢配合。<footer><div align="left"><a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a></div><div align="left"><a rel="copyright" align="right" href="http://iphysresearch.github.io">IPhysResearch</a>·<a rel="copyright" href="http://iphysresearch.github.io">土豆</a>·<a rel="copyright" href="http://iphysresearch.github.io">Herb</a>·<a rel="copyright" href="http://iphysresearch.github.io">He Wang</a> © 2018        (under construction)<div></footer>

<script type="application/json" class="js-hypothesis-config">  {    "openSidebar": false,    "showHighlights": true,    "theme": classic,    "enableExperimentalNewNoteButton": true  }</script><script async src="https://hypothes.is/embed.js"></script>

