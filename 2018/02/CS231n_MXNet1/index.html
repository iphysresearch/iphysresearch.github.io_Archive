<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
  <meta name="description" content="Machine Learning, Deep Learning, Physics">
  <meta name="keyword" content="hexo-theme, vuejs">
  
    <link rel="shortcut icon" href="/css/images/logo.png">
  
  <title>
    
      CS231n 与 MXNet 实现：图像分类 | Teaching is Learning
    
  </title>
  <link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.css" rel="stylesheet">
  <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/tomorrow.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/plugins/gitment.css">
  <script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
  <script src="//cdn.bootcss.com/geopattern/1.2.3/js/geopattern.min.js"></script>
  <script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>
  <script src="/js/qrious.js"></script>
<script src="/js/gitment.js"></script>
  
  
    <!-- MathJax support START -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- MathJax support END -->
  


</head>
<div class="wechat-share">
  <img src="/css/images/logo.png" />
</div>

  <body>
    <header class="header fixed-header">
  <div class="header-container">
    <a class="home-link" href="/">
      <div class="logo"></div>
      <span>Teaching is Learning</span>
    </a>
    <ul class="right-list">
      
        <li class="list-item">
          
            <a href="/" class="item-link">Home</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/tags/" class="item-link">Tags</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/archives/" class="item-link">Archives</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/project/" class="item-link">Projects</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/about/" class="item-link">About</a>
          
        </li>
      
    </ul>
    <div class="menu">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </div>
    <div class="menu-mask">
      <ul class="menu-list">
        
          <li class="menu-item">
            
              <a href="/" class="menu-link">Home</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/tags/" class="menu-link">Tags</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/archives/" class="menu-link">Archives</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/project/" class="menu-link">Projects</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/about/" class="menu-link">About</a>
            
          </li>
        
      </ul>
    </div>
  </div>
</header>

    <div id="article-banner">
  <h2>CS231n 与 MXNet 实现：图像分类</h2>
  <p class="post-date">2018-02-15</p>
  <div class="arrow-down">
    <a href="javascript:;"></a>
  </div>
</div>
<main class="app-body flex-box">
  <!-- Article START -->
  <article class="post-article">
    <section class="markdown-content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>CS231n &amp; MXNet 复现</p>
<p>还需要进一步整理，待续中。。。</p>
<p><br></p>
<a id="more"></a>
<blockquote>
<p><strong>自注</strong>：此文是在转载的基础上，通过网络搜集其他相关学习资料，再结合自己理解前提下，填充并注释了更多的细节和内容，以此详尽的文本资料代替各种视频课程等资料，方便自己回头翻查。并希望借此自学打卡的机会，用 MXNet 复现所有的作业代码。</p>
<p><br></p>
<p>（个人填充的内容包括：下划线、注明“自注”）</p>
<p><br></p>
<p>转载请注明本文出处和原译文出处。</p>
<p><strong>译者注</strong>：本文<a href="https://zhuanlan.zhihu.com/intelligentunit" target="_blank" rel="noopener">智能单元</a>首发，译自斯坦福CS231n课程笔记<a href="http://link.zhihu.com/?target=http%3A//cs231n.github.io/linear-classify/" target="_blank" rel="noopener">Linear Classification Note</a>，课程教师<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" target="_blank" rel="noopener">Andrej Karpathy</a>授权翻译。本篇教程由<a href="https://www.zhihu.com/people/du-ke" target="_blank" rel="noopener">杜客</a>翻译完成，<a href="https://www.zhihu.com/people/gong-zi-jia-57" target="_blank" rel="noopener">巩子嘉</a>和<a href="https://www.zhihu.com/people/kun-kun-97-81" target="_blank" rel="noopener">堃堃</a>进行校对修改。译文含公式和代码，建议PC端阅读。</p>
</blockquote>
<p><br></p>
<p>这是一篇介绍性教程，面向非计算机视觉领域的同学。教程将向同学们介绍图像分类问题和数据驱动方法。下面是<strong>内容列表</strong>：</p>
<ul>
<li>图像分类、数据驱动方法和流程</li>
<li>Nearest Neighbor分类器</li>
<li><ul>
<li>k-Nearest Neighbor</li>
</ul>
</li>
<li>验证集、交叉验证集和超参数调参</li>
<li>Nearest Neighbor的优劣</li>
<li>小结</li>
<li>小结：应用kNN实践</li>
<li>拓展阅读</li>
<li>作业：k-Nearest Neighbor (kNN)</li>
</ul>
<p><br></p>
<h2 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h2><p><strong>目标</strong>：这一节我们将介绍图像分类问题。所谓图像分类问题，就是已有固定的分类标签集合，然后对于输入的图像，从分类标签集合中找出一个分类标签，最后把分类标签分配给该输入图像。虽然看起来挺简单的，但这可是计算机视觉领域的核心问题之一，并且有着各种各样的实际应用。<u>在后面的课程中，我们可以看到计算机视觉领域中很多看似不同的问题（比如物体检测和分割），都可以被归结为图像分类问题。</u></p>
<p><br></p>
<p><strong>例子</strong>：以下图为例，图像分类模型读取该图片，并生成该图片属于集合 {cat, dog, hat, mug}中各个标签的概率。需要注意的是，对于计算机来说，图像是一个由数字组成的巨大的3维数组。在这个例子中，猫的图像大小是宽248像素，高400像素，有3个颜色通道，分别是红、绿和蓝（简称RGB）。如此，该图像就包含了248X400X3=297600个数字，每个数字都是在范围0-255之间的整型，其中0表示全黑，255表示全白。我们的任务就是把这些上百万的数字变成一个简单的标签，比如“猫”。</p>
<hr>
<p><img src="https://pic2.zhimg.com/baab9e4b97aceb77ec70abeda6be022d_b.png" alt=""></p>
<p>图像分类的任务，就是对于一个给定的图像，预测它属于的那个分类标签（或者给出属于一系列不同标签的可能性）。图像是3维数组，数组元素是取值范围从0到255的整数。数组的尺寸是宽度x高度x3，其中这个3代表的是红、绿和蓝3个颜色通道。</p>
<hr>
<p><strong>困难和挑战</strong>：对于人来说，识别出一个像“猫”一样视觉概念是简单至极的，然而从计算机视觉算法的角度来看就值得深思了。我们在下面列举了计算机视觉算法在图像识别方面遇到的一些困难，<u>要记住图像是以3维数组来表示的，数组中的元素是亮度值</u>。</p>
<ul>
<li><strong>视角变化（Viewpoint variation）</strong>：同一个物体，摄像机可以从多个角度来展现。</li>
<li><strong>大小变化（Scale variation）</strong>：物体可视的大小通常是会变化的（不仅是在图片中，在真实世界中大小也是变化的）。</li>
<li><strong>形变（Deformation）</strong>：很多东西的形状并非一成不变，会有很大变化。</li>
<li><strong>遮挡（Occlusion）</strong>：目标物体可能被挡住。有时候只有物体的一小部分（可以小到几个像素）是可见的。</li>
<li><strong>光照条件（Illumination conditions）</strong>：在像素层面上，光照的影响非常大。</li>
<li><strong>背景干扰（Background clutter）</strong>：物体可能混入背景之中，使之难以被辨认。</li>
<li><strong>类内差异（Intra-class variation）</strong>：一类物体的个体之间的外形差异很大，比如椅子。这一类物体有许多不同的对象，每个都有自己的外形。</li>
</ul>
<p>面对以上所有变化及其组合，好的图像分类模型能够在维持分类结论稳定的同时，保持对类间差异足够敏感。</p>
<hr>
<p><img src="https://pic2.zhimg.com/1ee9457872f773d671dd5b225647ef45_b.jpg" alt=""></p>
<hr>
<p><strong>数据驱动方法</strong>：如何写一个图像分类的算法呢？这和写个排序算法可是大不一样。怎么写一个从图像中认出猫的算法？搞不清楚。因此，与其在代码中直接写明各类物体到底看起来是什么样的，倒不如说我们采取的方法和教小孩儿看图识物类似：给计算机很多数据，然后实现学习算法，让计算机学习到每个类的外形。这种方法，就是<strong>_数据驱动方法_</strong>。既然<u>该方法的第一步就是收集已经做好分类标注的图片来作为训练集</u>，那么下面就看看数据库到底长什么样：</p>
<hr>
<p><img src="https://pic1.zhimg.com/bbbfd2e6878d6f5d2a82f8239addbbc0_b.jpg" alt=""></p>
<p>一个有4个视觉分类的训练集。在实际中，我们可能有上千的分类，每个分类都有成千上万的图像。</p>
<hr>
<p><strong>图像分类流程</strong>。在课程视频中已经学习过，<strong>图像分类</strong>就是输入一个元素为像素值的数组，然后给它分配一个分类标签。完整流程如下：</p>
<ul>
<li><strong>输入</strong>：输入是包含N个图像的集合，每个图像的标签是K种分类标签中的一种。这个集合称为<strong>_训练集。_</strong></li>
<li><strong>学习</strong>：这一步的任务是使用训练集来学习每个类到底长什么样。一般该步骤叫做<strong>_训练分类器_</strong>或者<strong>_学习一个模型_</strong>。</li>
<li><strong>评价</strong>：让分类器来预测它未曾见过的图像的分类标签，并以此来评价分类器的质量。我们会把分类器预测的标签和图像真正的分类标签对比。毫无疑问，分类器预测的分类标签和图像真正的分类标签如果一致，那就是好事，这样的情况越多越好。</li>
</ul>
<p><br></p>
<h2 id="Nearest-Neighbor分类器"><a href="#Nearest-Neighbor分类器" class="headerlink" title="Nearest Neighbor分类器"></a>Nearest Neighbor分类器</h2><p>作为课程介绍的第一个方法，我们来实现一个<strong>Nearest Neighbor分类器</strong>。虽然这个分类器和卷积神经网络没有任何关系，实际中也极少使用，但通过实现它，可以让读者对于解决图像分类问题的方法有个基本的认识。</p>
<p><br></p>
<p><strong>图像分类数据集：CIFAR-10。</strong>一个非常流行的图像分类数据集是<a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">CIFAR-10</a>。这个数据集包含了60000张32X32的小图像。每张图像都有10种分类标签中的一种。这60000张图像被分为包含50000张图像的训练集和包含10000张图像的测试集。在下图中你可以看见10个类的10张随机图片。</p>
<hr>
<p><img src="https://pic1.zhimg.com/fff49fd8cec00f77f657a4c4a679b030_b.jpg" alt=""></p>
<p><strong>左边</strong>：从<a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">CIFAR-10</a>数据库来的样本图像。<strong>右边</strong>：第一列是测试图像，然后第一列的每个测试图像右边是使用Nearest Neighbor算法，根据像素差异，从训练集中选出的10张最类似的图片。</p>
<hr>
<p>假设现在我们有CIFAR-10的50000张图片（每种分类5000张）作为训练集，我们希望将余下的10000作为测试集并给他们打上标签。Nearest Neighbor算法将会拿着测试图片和训练集中每一张图片去比较，然后将它认为最相似的那个训练集图片的标签赋给这张测试图片。上面右边的图片就展示了这样的结果。请注意上面10个分类中，只有3个是准确的。比如第8行中，马头被分类为一个红色的跑车，原因在于红色跑车的黑色背景非常强烈，所以这匹马就被错误分类为跑车了。</p>
<p><br></p>
<p>那么具体如何比较两张图片呢？在本例中，就是比较32x32x3的像素块。最简单的方法就是逐个像素比较，最后将差异值全部加起来。换句话说，就是将两张图片先转化为两个向量$I_1$和$I_2$，然后计算他们的<strong>L1距离：</strong><br>$$<br>d_1(I_1,I_2)=\sum_p|I^p_1-I^p_2|<br>$$<br>这里的求和是针对所有的像素。下面是整个比较流程的图例：</p>
<p><br></p>
<p>（自注：每个对应像素之差的结果全部取和——所谓L1距离）</p>
<hr>
<p><img src="https://pic2.zhimg.com/95cfe7d9efb83806299c218e0710a6c5_b.jpg" alt=""></p>
<p>以图片中的一个颜色通道为例来进行说明。两张图片使用L1距离来进行比较。逐个像素求差值，然后将所有差值加起来得到一个数值。如果两张图片一模一样，那么L1距离为0，但是如果两张图片很是不同，那L1值将会非常大。</p>
<hr>
<p>下面，让我们看看如何用代码来实现这个分类器。首先，我们将CIFAR-10的数据加载到内存中，并分成4个数组：训练数据和标签，测试数据和标签。在下面的代码中，<strong>Xtr</strong>（大小是50000x32x32x3）存有训练集中所有的图像，<strong>Ytr</strong>是对应的长度为50000的1维数组，存有图像对应的分类标签（从0到9）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Xtr, Ytr, Xte, Yte = load_CIFAR10(<span class="string">'data/cifar10/'</span>) <span class="comment"># a magic function we provide</span></span><br><span class="line"><span class="comment"># flatten out all images to be one-dimensional</span></span><br><span class="line">Xtr_rows = Xtr.reshape(Xtr.shape[<span class="number">0</span>], <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>) <span class="comment"># Xtr_rows becomes 50000 x 3072</span></span><br><span class="line">Xte_rows = Xte.reshape(Xte.shape[<span class="number">0</span>], <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>) <span class="comment"># Xte_rows becomes 10000 x 3072</span></span><br></pre></td></tr></table></figure>
<p>（自注：训练集中有50000张图片，每个图片reshape成为了一条一维的数组，共3072个元素。）</p>
<p><br></p>
<p>现在我们得到所有的图像数据，并且把他们拉长成为行向量了。接下来展示如何训练并评价一个分类器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nn = NearestNeighbor() <span class="comment"># create a Nearest Neighbor classifier class</span></span><br><span class="line">nn.train(Xtr_rows, Ytr) <span class="comment"># train the classifier on the training images and labels</span></span><br><span class="line">Yte_predict = nn.predict(Xte_rows) <span class="comment"># predict labels on the test images</span></span><br><span class="line"><span class="comment"># and now print the classification accuracy, which is the average number</span></span><br><span class="line"><span class="comment"># of examples that are correctly predicted (i.e. label matches)</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'accuracy: %f'</span> % ( np.mean(Yte_predict == Yte) )  <span class="comment"># python2.x</span></span><br></pre></td></tr></table></figure>
<p>作为评价标准，我们常常使用<strong>准确率</strong>，它描述了我们预测正确的得分。请注意以后我们实现的所有分类器都需要有这个API：<strong>train(X, y)</strong>函数。该函数使用训练集的数据和标签来进行训练。从其内部来看，类应该实现一些关于标签和标签如何被预测的模型。这里还有个<strong>predict(X)</strong>函数，它的作用是预测输入的新数据的分类标签。现在还没介绍分类器的实现，下面就是<u>使用L1距离的Nearest Neighbor分类器</u>的实现套路：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NearestNeighbor</span><span class="params">(object)</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">    	<span class="string">""" </span></span><br><span class="line"><span class="string">    	这个地方的训练其实就是把所有的已有图片读取进来 -_-||</span></span><br><span class="line"><span class="string">    	"""</span></span><br><span class="line">    	<span class="string">""" X is N x D where each row is an example. Y is 1-dimension of size N """</span></span><br><span class="line">    	<span class="comment"># the nearest neighbor classifier simply remembers all the training data</span></span><br><span class="line">    	self.Xtr = X</span><br><span class="line">    	self.ytr = y</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line"> 		<span class="string">""" </span></span><br><span class="line"><span class="string">    	所谓的预测过程其实就是扫描所有训练集中的图片，计算距离，取最小的距离对应图片的类目</span></span><br><span class="line"><span class="string">    	"""</span></span><br><span class="line">    	<span class="string">""" X is N x D where each row is an example we wish to predict label for """</span></span><br><span class="line">    	num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 这里要保证维度一致！</span></span><br><span class="line">    	<span class="comment"># lets make sure that the output type matches the input type</span></span><br><span class="line">    	Ypred = np.zeros(num_test, dtype = self.ytr.dtype) <span class="comment"># 一维元素都是0的array</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 把训练集扫一遍 -_-||</span></span><br><span class="line">		<span class="comment"># loop over all test rows</span></span><br><span class="line">    	<span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):  </span><br><span class="line">            <span class="comment"># 注意这里的xrange仅适用于python2.x，range适用于python3.x</span></span><br><span class="line">      		<span class="comment"># find the nearest training image to the i'th test image</span></span><br><span class="line">      		<span class="comment"># using the L1 distance (sum of absolute value differences)</span></span><br><span class="line">      		<span class="comment"># 对训练集中每一张图片都与指定的一张测试图片，在对应元素位置上做差，</span></span><br><span class="line">            <span class="comment"># 然后分别以每张图片为单位求和。</span></span><br><span class="line">      		distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = <span class="number">1</span>)	<span class="comment"># 一个5000元素的list</span></span><br><span class="line">      		<span class="comment"># 取最小distance图片的下标：</span></span><br><span class="line">      		min_index = np.argmin(distances) <span class="comment"># get the index with smallest distance</span></span><br><span class="line">      		Ypred[i] = self.ytr[min_index] <span class="comment"># predict the label of the nearest example</span></span><br><span class="line"></span><br><span class="line">    	<span class="keyword">return</span> Ypred</span><br></pre></td></tr></table></figure>
<p>如果你用这段代码跑CIFAR-10，你会发现准确率能达到<strong>38.6%</strong>。这比随机猜测的10%要好，但是比人类识别的水平（<a href="http://link.zhihu.com/?target=http%3A//karpathy.github.io/2011/04/27/manually-classifying-cifar10/" target="_blank" rel="noopener">据研究推测是94%</a>）和卷积神经网络能达到的95%还是差多了。点击查看基于CIFAR-10数据的<a href="http://link.zhihu.com/?target=http%3A//www.kaggle.com/c/cifar-10/leaderboard" target="_blank" rel="noopener">Kaggle算法竞赛排行榜</a>。</p>
<p><br></p>
<p><strong>距离选择</strong>：计算向量间的距离有很多种方法，另一个常用的方法是<strong>L2距离</strong>，从几何学的角度，可以理解为它在计算两个向量间的欧式距离。L2距离的公式如下：<br>$$<br>d_2(I_1,I_2)=\sqrt{\sum_p(I^P_1-I^p_2)^2}<br>$$<br>换句话说，我们依旧是在计算像素间的差值，只是先求其平方，然后把这些平方全部加起来，最后对这个和开方。在Numpy中，我们只需要替换上面代码中的1行代码就行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>注意在这里使用了<strong>np.sqrt</strong>，但是在实际中可能不用。因为求平方根函数是一个_单调函数_，它对不同距离的绝对值求平方根虽然改变了数值大小，但依然保持了不同距离大小的顺序。所以用不用它，都能够对像素差异的大小进行正确比较。如果你在CIFAR-10上面跑这个模型，正确率是<strong>35.4%</strong>，比刚才低了一点。</p>
<p><br></p>
<p><strong>L1和L2比较</strong>。比较这两个度量方式是挺有意思的。在面对两个向量之间的差异时，L2比L1更加不能容忍这些差异。也就是说，相对于1个巨大的差异，L2距离更倾向于接受多个中等程度的差异。L1和L2都是在<a href="http://link.zhihu.com/?target=http%3A//planetmath.org/vectorpnorm" target="_blank" rel="noopener">p-norm</a>常用的特殊形式。</p>
<p><br></p>
<p>（自注：更多的距离准则可以参见<a href="http://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.spatial.distance.pdist.html" target="_blank" rel="noopener">scipy相关计算页面</a>.）</p>
<p><br></p>
<h2 id="k-Nearest-Neighbor分类器"><a href="#k-Nearest-Neighbor分类器" class="headerlink" title="k-Nearest Neighbor分类器"></a>k-Nearest Neighbor分类器</h2><p>你可能注意到了，为什么只用最相似的1张图片的标签来作为测试图像的标签呢？这不是很奇怪吗！是的，使用<strong>k-Nearest Neighbor分类器</strong>就能做得更好。它的思想很简单：与其只找最相近的那1个图片的标签，我们找最相似的k个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。所以当k=1的时候，k-Nearest Neighbor分类器就是Nearest Neighbor分类器。从直观感受上就可以看到，更高的k值可以让分类的效果更平滑，使得分类器对于异常值更有抵抗力。</p>
<hr>
<p><img src="https://pic3.zhimg.com/51aef845faa10195e33bdd4657592f86_b.jpg" alt=""></p>
<p>上面示例展示了Nearest Neighbor分类器和5-Nearest Neighbor分类器的区别。例子使用了2维的点来表示，分成3类（红、蓝和绿）。不同颜色区域代表的是使用L2距离的分类器的<strong>决策边界</strong>。白色的区域是分类模糊的例子（即图像与两个以上的分类标签绑定）。需要注意的是，在NN分类器中，异常的数据点（比如：在蓝色区域中的绿点）制造出一个不正确预测的孤岛。5-NN分类器将这些不规则都平滑了，使得它针对测试数据的<strong>泛化（generalization）</strong>能力更好（例子中未展示）。注意，5-NN中也存在一些灰色区域，这些区域是因为近邻标签的最高票数相同导致的（比如：2个邻居是红色，2个邻居是蓝色，还有1个是绿色）。</p>
<hr>
<p>在实际中，大多使用k-NN分类器。但是k值如何确定呢？接下来就讨论这个问题。</p>
<p><br></p>
<h2 id="用于超参数调优的验证集"><a href="#用于超参数调优的验证集" class="headerlink" title="用于超参数调优的验证集"></a>用于超参数调优的验证集</h2><p>k-NN分类器需要设定k值，那么选择哪个k值最合适的呢？我们可以选择不同的距离函数，比如L1范数和L2范数等，那么选哪个好？还有不少选择我们甚至连考虑都没有考虑到（比如：点积）。所有这些选择，被称为<strong>超参数（hyperparameter）</strong>。在基于数据进行学习的机器学习算法设计中，超参数是很常见的。一般说来，这些超参数具体怎么设置或取值并不是显而易见的。</p>
<p><br></p>
<p>你可能会建议尝试不同的值，看哪个值表现最好就选哪个。好主意！我们就是这么做的，但这样做的时候要非常细心。<u>特别注意：</u><strong>决不能使用测试集来进行调优</strong>。当你在设计机器学习算法的时候，应该把测试集看做非常珍贵的资源，不到最后一步，绝不使用它。如果你使用测试集来调优，而且算法看起来效果不错，那么真正的危险在于：算法实际部署后，性能可能会远低于预期。这种情况，称之为算法对测试集<strong>过拟合</strong>。从另一个角度来说，如果使用测试集来调优，实际上就是把测试集当做训练集，由测试集训练出来的算法再跑测试集，自然性能看起来会很好。这其实是过于乐观了，实际部署起来效果就会差很多。所以，最终测试的时候再使用测试集，可以很好地近似度量你所设计的分类器的泛化性能（在接下来的课程中会有很多关于泛化性能的讨论）。</p>
<blockquote>
<p>测试数据集只使用一次，即在训练完成后评价最终的模型时使用。</p>
</blockquote>
<p>好在我们有不用测试集调优的方法。其思路是：从训练集中取出一部分数据用来调优，我们称之为<strong>验证集（validation set）</strong>。以CIFAR-10为例，我们可以用49000个图像作为训练集，用1000个图像作为验证集。验证集其实就是作为假的测试集来调优。下面就是代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假定已经有Xtr_rows, Ytr, Xte_rows, Yte了，其中Xtr_rows为50000*3072 矩阵</span></span><br><span class="line"><span class="comment"># assume we have Xtr_rows, Ytr, Xte_rows, Yte as before</span></span><br><span class="line"><span class="comment"># recall Xtr_rows is 50,000 x 3072 matrix</span></span><br><span class="line">Xval_rows = Xtr_rows[:<span class="number">1000</span>, :] <span class="comment"># take first 1000 for validation 构建前1000个图为交叉验证集</span></span><br><span class="line">Yval = Ytr[:<span class="number">1000</span>]</span><br><span class="line">Xtr_rows = Xtr_rows[<span class="number">1000</span>:, :] <span class="comment"># keep last 49,000 for train 保留其余49000个图为训练集</span></span><br><span class="line">Ytr = Ytr[<span class="number">1000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置一些k值，用于试验</span></span><br><span class="line"><span class="comment"># find hyperparameters that work best on the validation set</span></span><br><span class="line">validation_accuracies = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化对象</span></span><br><span class="line">    <span class="comment"># use a particular value of k and evaluation on validation data</span></span><br><span class="line">    nn = NearestNeighbor()</span><br><span class="line">	nn.train(Xtr_rows, Ytr)</span><br><span class="line">    <span class="comment"># 修改一下predict函数，接受 k 作为参数</span></span><br><span class="line">  	<span class="comment"># here we assume a modified NearestNeighbor class that can take a k as input</span></span><br><span class="line">  	Yval_predict = nn.predict(Xval_rows, k = k)</span><br><span class="line">  	acc = np.mean(Yval_predict == Yval)</span><br><span class="line">  	<span class="keyword">print</span> <span class="string">'accuracy: %f'</span> % (acc,)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出结果</span></span><br><span class="line">  	<span class="comment"># keep track of what works on the validation set</span></span><br><span class="line">  	validation_accuracies.append((k, acc)) <span class="comment"># 元组形式append在列表里</span></span><br></pre></td></tr></table></figure>
<p>程序结束后，我们会作图分析出哪个k值表现最好，然后用这个k值来跑真正的测试集，并作出对算法的评价。</p>
<blockquote>
<p>把训练集分成训练集和验证集。使用验证集来对所有超参数调优。最后只在测试集上跑一次并报告结果。</p>
</blockquote>
<p><strong>交叉验证</strong>。有时候，训练集数量较小（因此验证集的数量更小），人们会使用一种被称为<strong>交叉验证</strong>的方法，这种方法更加复杂些。还是用刚才的例子，如果是交叉验证集，我们就不是取1000个图像，而是将训练集平均分成5份，其中4份用来训练，1份用来验证。然后我们循环着取其中4份来训练，其中1份来验证，最后取所有5次验证结果的平均值作为算法验证结果。</p>
<hr>
<p><img src="https://pic1.zhimg.com/6a3ceec60cc0a379b4939c37ee3e89e8_b.png" alt=""></p>
<p>这就是5份交叉验证对k值调优的例子。针对每个k值，得到5个准确率结果，取其平均值，然后对不同k值的平均表现画线连接。本例中，当k=7的时算法表现最好（对应图中的准确率峰值）。如果我们将训练集分成更多份数，直线一般会更加平滑（噪音更少）。</p>
<hr>
<p><strong>实际应用</strong>。在实际情况下，人们不是很喜欢用交叉验证，主要是因为它会耗费较多的计算资源。一般直接把训练集按照50%-90%的比例分成训练集和验证集。但这也是根据具体情况来定的：如果超参数数量多，你可能就想用更大的验证集，而验证集的数量不够，那么最好还是用交叉验证吧。至于分成几份比较好，一般都是分成3、5和10份。</p>
<hr>
<p><img src="https://pic1.zhimg.com/cc88207c6c3c5e91df8b6367368f6450_b.jpg" alt=""></p>
<p>常用的数据分割模式。给出训练集和测试集后，训练集一般会被均分。这里是分成5份。前面4份用来训练，黄色那份用作验证集调优。如果采取交叉验证，那就各份轮流作为验证集。最后模型训练完毕，超参数都定好了，让模型跑一次（而且只跑一次）测试集，以此测试结果评价算法。</p>
<hr>
<p><br></p>
<h2 id="Nearest-Neighbor分类器的优劣"><a href="#Nearest-Neighbor分类器的优劣" class="headerlink" title="Nearest Neighbor分类器的优劣"></a>Nearest Neighbor分类器的优劣</h2><p>现在对Nearest Neighbor分类器的优缺点进行思考。首先，Nearest Neighbor分类器<u>易于理解</u>，<u>实现简单</u>。其次，<u>算法的训练不需要花时间</u>，因为其训练过程只是将训练集数据存储起来。然而<u>测试要花费大量时间计算</u>，因为每个测试图像需要和所有存储的训练图像进行比较，这显然是一个缺点。在实际应用中，我们关注测试效率远远高于训练效率。其实，我们后续要学习的卷积神经网络在这个权衡上走到了另一个极端：虽然训练花费很多时间，但是一旦训练完成，对新的测试数据进行分类非常快。这样的模式就符合实际使用需求。</p>
<p><br></p>
<p>Nearest Neighbor分类器的计算复杂度研究是一个活跃的研究领域，若干<strong>Approximate Nearest Neighbor </strong>(ANN)算法和库的使用可以提升Nearest Neighbor分类器在数据上的计算速度（比如：<a href="http://link.zhihu.com/?target=http%3A//www.cs.ubc.ca/research/flann/" target="_blank" rel="noopener">FLANN</a>）。这些算法可以在准确率和时空复杂度之间进行权衡，并通常依赖一个预处理/索引过程，这个过程中一般包含kd树的创建和k-means算法的运用。</p>
<p><br></p>
<p>Nearest Neighbor分类器在某些特定情况（比如数据维度较低）下，可能是不错的选择。但是在实际的图像分类工作中，很少使用。因为图像都是高维度数据（他们通常包含很多像素），而高维度向量之间的距离通常是反直觉的。下面的图片展示了基于像素的相似和基于感官的相似是有很大不同的：</p>
<hr>
<p><img src="https://pic3.zhimg.com/fd42d369eebdc5d81c89593ec1082e32_b.png" alt=""></p>
<p>在高维度数据上，基于像素的的距离和感官上的非常不同。上图中，右边3张图片和左边第1张原始图片的L2距离是一样的。很显然，基于像素比较的相似和感官上以及语义上的相似是不同的。</p>
<hr>
<p>这里还有个视觉化证据，可以证明使用像素差异来比较图像是不够的。这是一个叫做<a href="http://link.zhihu.com/?target=http%3A//lvdmaaten.github.io/tsne/" target="_blank" rel="noopener">t-SNE</a>的可视化技术，它将CIFAR-10中的图片按照二维方式排布，这样能很好展示图片之间的像素差异值。在这张图片中，排列相邻的图片L2距离就小。</p>
<hr>
<p><img src="https://pic1.zhimg.com/0f4980edb8710eaba0f3e661b1cbb830_b.jpg" alt=""></p>
<p>上图使用t-SNE的可视化技术将CIFAR-10的图片进行了二维排列。排列相近的图片L2距离小。可以看出，图片的排列是被背景主导而不是图片语义内容本身主导。</p>
<hr>
<p>具体说来，这些图片的排布更像是一种颜色分布函数，或者说是基于背景的，而不是图片的语义主体。比如，狗的图片可能和青蛙的图片非常接近，这是因为两张图片都是白色背景。从理想效果上来说，我们肯定是希望同类的图片能够聚集在一起，而不被背景或其他不相关因素干扰。为了达到这个目的，我们不能止步于原始像素比较，得继续前进。</p>
<p><br></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>简要说来：</p>
<ul>
<li>介绍了<strong>图像分类</strong>问题。在该问题中，给出一个由被标注了分类标签的图像组成的集合，要求算法能预测没有标签的图像的分类标签，并根据算法预测准确率进行评价。</li>
<li>介绍了一个简单的图像分类器：<strong>最近邻分类器(Nearest Neighbor classifier)</strong>。分类器中存在不同的超参数(比如k值或距离类型的选取)，要想选取好的超参数不是一件轻而易举的事。</li>
<li>选取超参数的正确方法是：将原始训练集分为<strong>训练集</strong>和<strong>验证集</strong>，我们在验证集上尝试不同的超参数，最后保留表现最好那个。</li>
<li>如果训练数据量不够，使用<strong>交叉验证</strong>方法，它能帮助我们在选取最优超参数的时候减少噪音。</li>
<li>一旦找到最优的超参数，就让算法以该参数在测试集跑且只跑一次，并根据测试结果评价算法。</li>
<li>最近邻分类器能够在CIFAR-10上得到将近40%的准确率。该算法简单易实现，但需要存储所有训练数据，并且在测试的时候过于耗费计算能力。</li>
<li>最后，我们知道了仅仅使用L1和L2范数来进行像素比较是不够的，图像更多的是按照背景和颜色被分类，而不是语义主体分身。</li>
</ul>
<p>在接下来的课程中，我们将专注于解决这些问题和挑战，并最终能够得到超过90%准确率的解决方案。该方案能够在完成学习就丢掉训练集，并在一毫秒之内就完成一张图片的分类。</p>
<p><br></p>
<h2 id="小结：实际应用k-NN"><a href="#小结：实际应用k-NN" class="headerlink" title="小结：实际应用k-NN"></a>小结：实际应用k-NN</h2><p>如果你希望将k-NN分类器用到实处（最好别用到图像上，若是仅仅作为练手还可以接受），那么可以按照以下流程：</p>
<ol>
<li>预处理你的数据：对你数据中的特征进行<u>归一化（normalize）</u>，让其具有<u>零平均值（zero mean）和单位方差（unit variance）</u>。在后面的小节我们会讨论这些细节。本小节不讨论，是因为图像中的像素都是同质的，不会表现出较大的差异分布，也就不需要标准化处理了。</li>
<li>如果数据是高维数据，考虑使用降维方法，比如PCA(<a href="http://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Principal_component_analysis" target="_blank" rel="noopener">wiki ref</a>, <a href="http://link.zhihu.com/?target=http%3A//cs229.stanford.edu/notes/cs229-notes10.pdf" target="_blank" rel="noopener">CS229ref</a>, <a href="http://link.zhihu.com/?target=http%3A//www.bigdataexaminer.com/understanding-dimensionality-reduction-principal-component-analysis-and-singular-value-decomposition/" target="_blank" rel="noopener">blog ref</a>)或<a href="http://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/random_projection.html" target="_blank" rel="noopener">随机投影</a>。</li>
<li>将数据随机分入训练集和验证集。按照一般规律，70%-90% 数据作为训练集。这个比例根据算法中有多少超参数，以及这些超参数对于算法的预期影响来决定。<u>如果需要预测的超参数很多，那么就应该使用更大的验证集来有效地估计它们。</u>如果担心验证集数量不够，那么就尝试交叉验证方法。<u>如果计算资源足够，使用交叉验证总是更加安全的</u>（份数越多，效果越好，也更耗费计算资源）。</li>
<li>在验证集上调优，<u>尝试足够多的k值</u>，<u>尝试L1和L2两种范数计算方式</u>。</li>
<li>如果分类器跑得太慢，尝试使用Approximate Nearest Neighbor库（比如<a href="http://link.zhihu.com/?target=http%3A//www.cs.ubc.ca/research/flann/" target="_blank" rel="noopener">FLANN</a>）来加速这个过程，其代价是降低一些准确率。</li>
<li>对最优的超参数做记录。记录最优参数后，是否应该让使用最优参数的算法在完整的训练集上运行并再次训练呢？因为如果把验证集重新放回到训练集中（自然训练集的数据量就又变大了），有可能最优参数又会有所变化。在实践中，<strong>不要这样做</strong>。千万不要在最终的分类器中使用验证集数据，这样做会破坏对于最优参数的估计。<strong>直接使用测试集来测试用最优参数设置好的最优模型</strong>，得到测试集数据的分类准确率，并以此作为你的kNN分类器在该数据上的性能表现。</li>
</ol>
<p><br></p>
<h2 id="拓展阅读"><a href="#拓展阅读" class="headerlink" title="拓展阅读"></a>拓展阅读</h2><p>下面是一些你可能感兴趣的拓展阅读链接：</p>
<ul>
<li><a href="http://link.zhihu.com/?target=http%3A//homes.cs.washington.edu/%257Epedrod/papers/cacm12.pdf" target="_blank" rel="noopener">A Few Useful Things to Know about Machine Learning</a>，文中第6节与本节相关，但是整篇文章都强烈推荐。</li>
<li><a href="http://link.zhihu.com/?target=http%3A//people.csail.mit.edu/torralba/shortCourseRLOC/index.html" target="_blank" rel="noopener">Recognizing and Learning Object Categories</a>，ICCV 2005上的一节关于物体分类的课程。</li>
</ul>
<p><br></p>
<h2 id="作业：k-Nearest-Neighbor-kNN"><a href="#作业：k-Nearest-Neighbor-kNN" class="headerlink" title="作业：k-Nearest Neighbor (kNN)"></a>作业：k-Nearest Neighbor (kNN)</h2><p>该assignment作业的<a href="http://cs231n.github.io/assignments2017/assignment1/" target="_blank" rel="noopener">官方说明地址</a>(Q1: k-Nearest Neighbor classifier) </p>
<p><br></p>
<p>内含数据集下载办法，环境版本支持信息。</p>
<ul>
<li>首先，初始化一些代码：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> cs231n.data_utils <span class="keyword">import</span> load_CIFAR10</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is a bit of magic to make matplotlib figures appear inline in the notebook</span></span><br><span class="line"><span class="comment"># rather than in a new window.</span></span><br><span class="line"><span class="comment">#这里有一个小技巧可以让matplotlib画的图出现在notebook页面上，而不是新建一个画图窗口</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment"># set default size of plots 设置默认的绘图窗口大小</span></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">10.0</span>, <span class="number">8.0</span>) </span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span> <span class="comment"># 差值方式</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span> <span class="comment"># 灰度空间</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Some more magic so that the notebook will reload external python modules;</span></span><br><span class="line"><span class="comment"># see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython</span></span><br><span class="line"><span class="comment">#另一个小技巧，下面的语句会让notebook自动加载最新版本的外部python模块</span></span><br><span class="line"><span class="comment">#详情见 http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython</span></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>There is a trick: when you <strong>forget all</strong> of the meaning of autoreload when using <code>ipython</code>, just try:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> autoreload</span><br><span class="line">?autoreload</span><br></pre></td></tr></table></figure>
<ul>
<li>加载数据源：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load the raw CIFAR-10 data.</span></span><br><span class="line"><span class="comment"># 这里加载数据源的方法在data_utils.py中，会将data_batch_!到5的数据作为训练集，test_batch作为测试集</span></span><br><span class="line">cifar10_dir = <span class="string">'cs231n/datasets/cifar-10-batches-py'</span></span><br><span class="line">X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># As a sanity check, we print out the size o f the training and test data.</span></span><br><span class="line"><span class="comment"># 检查一下数据，打印训练集及其标签和测试集及其标签的大小 </span></span><br><span class="line">print(<span class="string">'Training data shape: '</span>, X_train.shape)</span><br><span class="line">print(<span class="string">'Training labels shape: '</span>, y_train.shape)</span><br><span class="line">print(<span class="string">'Test data shape: '</span>, X_test.shape)</span><br><span class="line">print(<span class="string">'Test labels shape: '</span>, y_test.shape)</span><br><span class="line"><span class="comment"># 返回值是：</span></span><br><span class="line"><span class="comment"># Training data shape:  (50000, 32, 32, 3)</span></span><br><span class="line"><span class="comment"># Training labels shape:  (50000,)</span></span><br><span class="line"><span class="comment"># Test data shape:  (10000, 32, 32, 3)</span></span><br><span class="line"><span class="comment"># Test labels shape:  (10000,)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>来看看数据集都长什么样：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Visualize some examples from the dataset.</span></span><br><span class="line"><span class="comment"># We show a few examples of training images from each class.</span></span><br><span class="line"><span class="comment"># 这里我们将训练集中每一个类别的样本随机挑出几个进行展示</span></span><br><span class="line">classes = [<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>, <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>]</span><br><span class="line">num_classes = len(classes)  <span class="comment"># 类别数目</span></span><br><span class="line">samples_per_class = <span class="number">7</span>    <span class="comment"># 每个类别采样个数</span></span><br><span class="line"><span class="comment"># 对列表的元素位置和元素进行循环：</span></span><br><span class="line"><span class="keyword">for</span> y, cls <span class="keyword">in</span> enumerate(classes):  <span class="comment"># 这里对应生成的是 (y, cls) = (0, plane), (1, car), (2, bird), ..., (9, truck)</span></span><br><span class="line">    <span class="comment"># 在循环每一个类别时，挑出测试集y中该类别图片的索引位置（index）：</span></span><br><span class="line">    idxs = np.flatnonzero(y_train == y)   </span><br><span class="line">    <span class="comment"># np.flatnonzero函数：返回扁平化后矩阵中非零元素(或真值)的位置索引（index）</span></span><br><span class="line">    <span class="comment"># 随机从idxs中选取num_classes个数的内容，并将选取结果生成新的array中返回</span></span><br><span class="line">    idxs = np.random.choice(idxs, samples_per_class, replace=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">for</span> i, idx <span class="keyword">in</span> enumerate(idxs):  <span class="comment"># 同上，i表示循环idx的序列号，idx是idxs的元素之一</span></span><br><span class="line">        plt_idx = i * num_classes + y + <span class="number">1</span>  <span class="comment"># 在子图中所占位置的计算，竖着一列一列画的。</span></span><br><span class="line">        plt.subplot(samples_per_class, num_classes, plt_idx)  <span class="comment"># 说明要画的子图的编号</span></span><br><span class="line">        plt.imshow(X_train[idx].astype(<span class="string">'uint8'</span>))  <span class="comment"># show出image</span></span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            plt.title(cls)   <span class="comment"># 每当画第一行图像时，写上标题，也就是类别名</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># REF：http://www.cnblogs.com/lijiajun/p/5479523.html</span></span><br></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/2301760-77211119464a6c5d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<ul>
<li>数据准备</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Subsample the data for more efficient code execution in this exercise</span></span><br><span class="line"><span class="comment"># 为了更高效地运行我们的代码，这里取出一个子集进行后面的练习（取训练集前50000个，测试集前500个）</span></span><br><span class="line">num_training = <span class="number">5000</span></span><br><span class="line">mask = list(range(num_training))</span><br><span class="line">X_train = X_train[mask]</span><br><span class="line">y_train = y_train[mask]</span><br><span class="line"></span><br><span class="line">num_test = <span class="number">500</span></span><br><span class="line">mask = list(range(num_test))</span><br><span class="line">X_test = X_test[mask]</span><br><span class="line">y_test = y_test[mask]</span><br></pre></td></tr></table></figure>
<p>(下面这一步可以省)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reshape the image data into rows</span></span><br><span class="line"><span class="comment"># 将图像数据转置成二维的</span></span><br><span class="line">X_train = np.reshape(X_train, (X_train.shape[<span class="number">0</span>], <span class="number">-1</span>))  </span><br><span class="line">X_test = np.reshape(X_test, (X_test.shape[<span class="number">0</span>], <span class="number">-1</span>))</span><br><span class="line"><span class="comment"># 这里的-1表示，X_train.shape[0]作为新array的行数后，剩余可自动推断出的列数。</span></span><br><span class="line">print(X_train.shape, X_test.shape)</span><br><span class="line"><span class="comment"># 打印结果：</span></span><br><span class="line"><span class="comment"># (5000, 3072) (500, 3072)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>训练模型</li>
</ul>
<p>We would now like to classify the test data with the kNN classifier. Recall that we can break down this process into two steps: </p>
<p><br></p>
<p>现在我们可以使用kNN分类器对测试样本进行分类了。我们可以将预测过程分为以下两步：</p>
<ol>
<li><p>First we must compute the distances between all test examples and all train examples. </p>
<p>首先，我们需要计算测试样本和所有训练样本的距离。</p>
</li>
<li><p>Given these distances, for each test example we find the k nearest examples and have them vote for the label</p>
<p>得到距离矩阵后，找出离测试样本最近的k个训练样本，选择出现次数最多的类别作为测试样本的类别</p>
</li>
</ol>
<p>Lets begin with computing the distance matrix between all training and test examples. For example, if there are <strong>Ntr</strong> training examples and <strong>Nte</strong> test examples, this stage should result in a <strong>Nte x Ntr</strong> matrix where each element (i,j) is the distance between the i-th test and j-th train example.<br>首先，计算距离矩阵。如果训练样本有<strong>Ntr</strong>个，测试样本有<strong>Nte</strong>个，则距离矩阵应该是个<strong>Nte x Ntr</strong>大小的矩阵，其中元素[i,j]表示第i个测试样本到第j个训练样本的距离。</p>
<p><br></p>
<p>First, open <code>cs231n/classifiers/k_nearest_neighbor.py</code> and implement the function <code>compute_distances_two_loops</code> that uses a (very inefficient) double loop over all pairs of (test, train) examples and computes the distance matrix one element at a time.</p>
<p><br></p>
<p>下面，打开<code>cs231n/classifiers/k_nearest_neighbor.py</code>，并补全<code>compute_distances_two_loops</code>方法，它使用了一个两层循环的方式（非常低效）计算测试样本与训练样本的距离.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cs231n.classifiers <span class="keyword">import</span> KNearestNeighbor</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a kNN classifier instance. </span></span><br><span class="line"><span class="comment"># Remember that training a kNN classifier is a noop: </span></span><br><span class="line"><span class="comment"># the Classifier simply remembers the data and does no further processing </span></span><br><span class="line">classifier = KNearestNeighbor()</span><br><span class="line">classifier.train(X_train, y_train)</span><br></pre></td></tr></table></figure>
<ul>
<li>模型测试</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Open cs231n/classifiers/k_nearest_neighbor.py and implement</span></span><br><span class="line"><span class="comment"># compute_distances_two_loops.</span></span><br><span class="line"><span class="comment"># 打开cs231n/classifiers/k_nearest_neighbor.py，补全compute_distances_two_loops方法</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test your implementation:</span></span><br><span class="line"><span class="comment"># 测试一下</span></span><br><span class="line">dists = classifier.compute_distances_two_loops(X_test)</span><br><span class="line">print(dists.shape)</span><br><span class="line"><span class="comment"># 打印结果：</span></span><br><span class="line"><span class="comment"># (500, 5000)	其dists[i,j]表示的是第i个测试图片与第j个训练图片之间的L2距离</span></span><br></pre></td></tr></table></figure>
<ul>
<li>距离矩阵可视化</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We can visualize the distance matrix: each row is a single test example and</span></span><br><span class="line"><span class="comment"># its distances to training examples</span></span><br><span class="line"><span class="comment"># 我们可以将距离矩阵进行可视化：其中每一行表示一个测试样本与所有训练样本的距离</span></span><br><span class="line">plt.imshow(dists, interpolation=<span class="string">'none'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20170304162108675?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDQyOTk4OQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p><strong>Inline Question #1:</strong> Notice the structured patterns in the distance matrix, where some rows or columns are visible brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.) </p>
<p><br></p>
<p><strong>问个小问题 #1:</strong> 图中可以明显看出，行列之间有颜色深浅之分。（其中深色表示距离值小，而浅色表示距离值大）</p>
<ol>
<li><p>What in the data is the cause behind the distinctly bright rows?</p>
<p>什么原因导致图中某些行的颜色明显偏浅？</p>
</li>
<li><p>What causes the columns?</p>
<p>为什么某些列的颜色明显偏浅？</p>
</li>
</ol>
<p><strong>Your Answer</strong>: _fill this in._ </p>
<p><br></p>
<p><strong>你的答案</strong>: _写在这里。_</p>
<ol>
<li>测试样本与训练集中的样本差异较大，该测试样本可能是其他类别的图片，或者是一个异常图片</li>
<li>所有测试样本与该列表示的训练样本L2距离都较大</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Now implement the function predict_labels and run the code below:</span></span><br><span class="line"><span class="comment"># We use k = 1 (which is Nearest Neighbor).</span></span><br><span class="line"><span class="comment"># 实现predict_labels方法，然后运行下列代码</span></span><br><span class="line"><span class="comment"># 这里我们将k设置为1</span></span><br><span class="line"></span><br><span class="line">y_test_pred = classifier.predict_labels(dists, k=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute and print the fraction of correctly predicted examples</span></span><br><span class="line"><span class="comment"># 计算并打印准确率</span></span><br><span class="line">num_correct = np.sum(y_test_pred == y_test)</span><br><span class="line">accuracy = float(num_correct) / num_test</span><br><span class="line">print(<span class="string">'Got %d / %d correct =&gt; accuracy: %f'</span> % (num_correct, num_test, accuracy))</span><br><span class="line"><span class="comment"># 打印结果：</span></span><br><span class="line"><span class="comment"># Got 137 / 500 correct =&gt; accuracy: 0.274000</span></span><br></pre></td></tr></table></figure>
<p>You should expect to see approximately <code>27%</code> accuracy. Now lets try out a larger <code>k</code>, say <code>k = 5</code>: </p>
<p>结果应该约为27%。现在，我们将k调大一点试试，令k = 5：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_test_pred = classifier.predict_labels(dists, k=<span class="number">5</span>)</span><br><span class="line">num_correct = np.sum(y_test_pred == y_test)</span><br><span class="line">accuracy = float(num_correct) / num_test</span><br><span class="line">print(<span class="string">'Got %d / %d correct =&gt; accuracy: %f'</span> % (num_correct, num_test, accuracy))</span><br><span class="line"><span class="comment"># 打印结果：</span></span><br><span class="line"><span class="comment"># Got 139 / 500 correct =&gt; accuracy: 0.278000</span></span><br></pre></td></tr></table></figure>
<p>You should expect to see a slightly better performance than with <code>k = 1</code>. </p>
<p>结果应该略好于k=1时的情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Now lets speed up distance matrix computation by using partial vectorization</span></span><br><span class="line"><span class="comment"># with one loop. Implement the function compute_distances_one_loop and run the</span></span><br><span class="line"><span class="comment"># code below:</span></span><br><span class="line"><span class="comment"># 现在我们将距离计算的效率提升一下，使用单层循环结构的计算方法。实现</span></span><br><span class="line"><span class="comment"># compute_distances_one_loop方法，并运行下列代码</span></span><br><span class="line"></span><br><span class="line">dists_one = classifier.compute_distances_one_loop(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># To ensure that our vectorized implementation is correct, we make sure that it</span></span><br><span class="line"><span class="comment"># agrees with the naive implementation. There are many ways to decide whether</span></span><br><span class="line"><span class="comment"># two matrices are similar; one of the simplest is the Frobenius norm. In case</span></span><br><span class="line"><span class="comment"># you haven't seen it before, the Frobenius norm of two matrices is the square</span></span><br><span class="line"><span class="comment"># root of the squared sum of differences of all elements; in other words, reshape</span></span><br><span class="line"><span class="comment"># the matrices into vectors and compute the Euclidean distance between them.</span></span><br><span class="line"><span class="comment"># 为了保证矢量化的代码运行正确，我们将运行结果与前面的方法的结果进行对比。对比两个</span></span><br><span class="line"><span class="comment"># 矩阵是否相等的方法有很多，比较简单的一种是使用Frobenius范数。Frobenius范数表示的</span></span><br><span class="line"><span class="comment"># 是两个矩阵所有元素的插值的平方和的根。或者说是将两个矩阵reshape成矢量后，它们之</span></span><br><span class="line"><span class="comment"># 间的欧氏距离</span></span><br><span class="line">difference = np.linalg.norm(dists - dists_one, ord=<span class="string">'fro'</span>)</span><br><span class="line">print(<span class="string">'Difference was: %f'</span> % (difference, ))</span><br><span class="line"><span class="keyword">if</span> difference &lt; <span class="number">0.001</span>:</span><br><span class="line">    print(<span class="string">'Good! The distance matrices are the same'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'Uh-oh! The distance matrices are different'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 打印结果：</span></span><br><span class="line"><span class="comment"># Difference was: 0.000000</span></span><br><span class="line"><span class="comment"># Good! The distance matrices are the same</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Now implement the fully vectorized version inside compute_distances_no_loops</span></span><br><span class="line"><span class="comment"># and run the code</span></span><br><span class="line"><span class="comment"># 完成完全矢量化方式运行的compute_distances_no_loops方法，并运行下列代码</span></span><br><span class="line">dists_two = classifier.compute_distances_no_loops(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check that the distance matrix agrees with the one we computed before:</span></span><br><span class="line"><span class="comment"># 将结果与之前的计算结果进行对比</span></span><br><span class="line">difference = np.linalg.norm(dists - dists_two, ord=<span class="string">'fro'</span>)</span><br><span class="line">print(<span class="string">'Difference was: %f'</span> % (difference, ))</span><br><span class="line"><span class="keyword">if</span> difference &lt; <span class="number">0.001</span>:</span><br><span class="line">    print(<span class="string">'Good! The distance matrices are the same'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'Uh-oh! The distance matrices are different'</span>)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 打印结果：</span></span><br><span class="line"><span class="comment"># Difference was: 0.000000</span></span><br><span class="line"><span class="comment"># Good! The distance matrices are the same</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's compare how fast the implementations are</span></span><br><span class="line"><span class="comment"># 下面我们对比一下各方法的执行速度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_function</span><span class="params">(f, *args)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Call a function f with args and return the time (in seconds) that it took to execute.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line">    tic = time.time()</span><br><span class="line">    f(*args)</span><br><span class="line">    toc = time.time()</span><br><span class="line">    <span class="keyword">return</span> toc - tic</span><br><span class="line"></span><br><span class="line">two_loop_time = time_function(classifier.compute_distances_two_loops, X_test)</span><br><span class="line">print(<span class="string">'Two loop version took %f seconds'</span> % two_loop_time)</span><br><span class="line"></span><br><span class="line">one_loop_time = time_function(classifier.compute_distances_one_loop, X_test)</span><br><span class="line">print(<span class="string">'One loop version took %f seconds'</span> % one_loop_time)</span><br><span class="line"></span><br><span class="line">no_loop_time = time_function(classifier.compute_distances_no_loops, X_test)</span><br><span class="line">print(<span class="string">'No loop version took %f seconds'</span> % no_loop_time)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果：</span></span><br><span class="line"><span class="comment"># Two loop version took 57.122203 seconds</span></span><br><span class="line"><span class="comment"># One loop version took 71.512672 seconds</span></span><br><span class="line"><span class="comment"># No loop version took 0.510630 seconds</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># you should see significantly faster performance with the fully vectorized implementation</span></span><br><span class="line"><span class="comment"># 你应该可以看到，完全矢量化的代码运行效率有明显的提高</span></span><br></pre></td></tr></table></figure>
<h3 id="Cross-validation"><a href="#Cross-validation" class="headerlink" title="Cross-validation"></a>Cross-validation</h3><p>We have implemented the k-Nearest Neighbor classifier but we set the value k = 5 arbitrarily. We will now determine the best value of this hyperparameter with cross-validation.</p>
<h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>之前我们已经完成了k-Nearest分类器的编写，但是对于k值的选择很随意。下面我们将使用交叉验证的方法选择最优的超参数k。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">num_folds = <span class="number">5</span></span><br><span class="line">k_choices = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">X_train_folds = []</span><br><span class="line">y_train_folds = []</span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Split up the training data into folds. After splitting, X_train_folds and    #</span></span><br><span class="line"><span class="comment"># y_train_folds should each be lists of length num_folds, where                #</span></span><br><span class="line"><span class="comment"># y_train_folds[i] is the label vector for the points in X_train_folds[i].     #</span></span><br><span class="line"><span class="comment"># Hint: Look up the numpy array_split function.                                #</span></span><br><span class="line"><span class="comment"># 任务:                                                                        #</span></span><br><span class="line"><span class="comment"># 将训练数据切分成不同的折。切分之后,训练样本和对应的样本标签被包含在数组                 #</span></span><br><span class="line"><span class="comment"># X_train_folds和y_train_folds之中，数组长度是折数num_folds。其中                  #</span></span><br><span class="line"><span class="comment"># y_train_folds[i]是一个矢量，表示矢量X_train_folds[i]中所有样本的标签              #</span></span><br><span class="line"><span class="comment"># 提示: 可以尝试使用numpy的array_split方法。                                      #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line">X_train_folds = np.array_split(X_train, num_folds)	</span><br><span class="line">y_train_folds = np.array_split(y_train, num_folds)</span><br><span class="line"><span class="comment"># 以num_folds的折数从前向后分割，最后不够num_folds数目的为最后一折</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                                 END OF YOUR CODE                             #</span></span><br><span class="line"><span class="comment">#                                 结束                                          #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A dictionary holding the accuracies for different values of k that we find</span></span><br><span class="line"><span class="comment"># when running cross-validation. After running cross-validation,</span></span><br><span class="line"><span class="comment"># k_to_accuracies[k] should be a list of length num_folds giving the different</span></span><br><span class="line"><span class="comment"># accuracy values that we found when using that value of k.</span></span><br><span class="line"><span class="comment"># 我们将不同k值下的准确率保存在一个字典中。交叉验证之后，k_to_accuracies[k]保存了</span></span><br><span class="line"><span class="comment"># k值下，一个长度为折数的准确率矢量</span></span><br><span class="line"></span><br><span class="line">k_to_accuracies = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">    k_to_accuracies.setdefault(k, [])</span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Perform k-fold cross validation to find the best value of k. For each        #</span></span><br><span class="line"><span class="comment"># possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #</span></span><br><span class="line"><span class="comment"># where in each case you use all but one of the folds as training data and the #</span></span><br><span class="line"><span class="comment"># last fold as a validation set. Store the accuracies for all fold and all     #</span></span><br><span class="line"><span class="comment"># values of k in the k_to_accuracies dictionary.                               #</span></span><br><span class="line"><span class="comment"># 任务:                                                                         #</span></span><br><span class="line"><span class="comment"># 通过k折的交叉验证找到最佳k值。对于每一个k值，执行kNN算法num_folds次，每一次            #</span></span><br><span class="line"><span class="comment"># 执行中，选择一折为训练集，最后一折为验证集。将不同k值在不同折上的运算结果保              #</span></span><br><span class="line"><span class="comment"># 存在k_to_accuracies字典中。                                                    #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_folds):</span><br><span class="line">    classifier = KNearestNeighbor()</span><br><span class="line">    X_train_ = np.vstack(X_train_folds[:i] + X_train_folds[i+<span class="number">1</span>:])</span><br><span class="line">    y_train_ = np.hstack(y_train_folds[:i] + y_train_folds[i+<span class="number">1</span>:])</span><br><span class="line">    classifier.train(X_train_, y_train_)</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">        y_pred_ = classifier.predict(X_train_folds[i], k=k)</span><br><span class="line">        accuracy = np.mean(y_pred_ == y_train_folds[i])</span><br><span class="line">        k_to_accuracies[k].append(accuracy)</span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                                 END OF YOUR CODE                             #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the computed accuracies</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> sorted(k_to_accuracies):</span><br><span class="line">    <span class="keyword">for</span> accuracy <span class="keyword">in</span> k_to_accuracies[k]:</span><br><span class="line">        print(<span class="string">'k = %d, accuracy = %f'</span> % (k, accuracy))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot the raw observations</span></span><br><span class="line"><span class="comment"># 画个图会更直观一点</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">    accuracies = k_to_accuracies[k]</span><br><span class="line">    plt.scatter([k] * len(accuracies), accuracies)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the trend line with error bars that correspond to standard deviation</span></span><br><span class="line"><span class="comment"># 画出在不同k值下，误差均值和标准差</span></span><br><span class="line">accuracies_mean = np.array([np.mean(v) <span class="keyword">for</span> k,v <span class="keyword">in</span> sorted(k_to_accuracies.items())])</span><br><span class="line">accuracies_std = np.array([np.std(v) <span class="keyword">for</span> k,v <span class="keyword">in</span> sorted(k_to_accuracies.items())])</span><br><span class="line">plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)</span><br><span class="line">plt.title(<span class="string">'Cross-validation on k'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'k'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Cross-validation accuracy'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://images2015.cnblogs.com/blog/1131087/201703/1131087-20170322095846736-165706060.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Based on the cross-validation results above, choose the best value for k,   </span></span><br><span class="line"><span class="comment"># retrain the classifier using all the training data, and test it on the test</span></span><br><span class="line"><span class="comment"># data. You should be able to get above 28% accuracy on the test data.</span></span><br><span class="line"><span class="comment"># 根据上面交叉验证的结果，选择最优的k，然后在全量数据上进行试验，你将得到约28%的准确率</span></span><br><span class="line">best_k = <span class="number">6</span></span><br><span class="line"></span><br><span class="line">classifier = KNearestNeighbor()</span><br><span class="line">classifier.train(X_train, y_train)</span><br><span class="line">y_test_pred = classifier.predict(X_test, k=best_k)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute and display the accuracy</span></span><br><span class="line"><span class="comment"># 计算并展示准确率</span></span><br><span class="line">num_correct = np.sum(y_test_pred == y_test)</span><br><span class="line">accuracy = float(num_correct) / num_test</span><br><span class="line">print(<span class="string">'Got %d / %d correct =&gt; accuracy: %f'</span> % (num_correct, num_test, accuracy))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果：</span></span><br><span class="line"><span class="comment"># Got 141 / 500 correct =&gt; accuracy: 0.282000</span></span><br></pre></td></tr></table></figure>
<h3 id="k-nearest-neighbor-py"><a href="#k-nearest-neighbor-py" class="headerlink" title="k_nearest_neighbor.py"></a>k_nearest_neighbor.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#from past.builtins import xrange</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNearestNeighbor</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">""" a kNN classifier with L2 distance """</span></span><br><span class="line">	<span class="string">""" 使用L2距离的kNN分类器 """</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line">	</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Train the classifier. For k-nearest neighbors this is just </span></span><br><span class="line"><span class="string">    memorizing the training data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_train, D) containing the training data</span></span><br><span class="line"><span class="string">      consisting of num_train samples each of dimension D.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing the training labels, where</span></span><br><span class="line"><span class="string">         y[i] is the label for X[i].</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    训练分类器。对于KNN分类器来说，训练就是将训练数据保存</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    -X: 训练数据集，一个(训练样本数量，维度)大小的numpy数组</span></span><br><span class="line"><span class="string">    -y: 训练样本标签，一个(训练样本数量，1)大小的numpy数组，其中y[i]表示样本X[i]的类别标签</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    	self.X_train = X</span><br><span class="line">    	self.y_train = y</span><br><span class="line">    </span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X, k=<span class="number">1</span>, num_loops=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Predict labels for test data using this classifier.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_test, D) containing test data consisting</span></span><br><span class="line"><span class="string">         of num_test samples each of dimension D.</span></span><br><span class="line"><span class="string">    - k: The number of nearest neighbors that vote for the predicted labels.</span></span><br><span class="line"><span class="string">    - num_loops: Determines which implementation to use to compute distances</span></span><br><span class="line"><span class="string">      between training points and testing points.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (num_test,) containing predicted labels for the</span></span><br><span class="line"><span class="string">      test data, where y[i] is the predicted label for the test point X[i].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    该方法对输入数据进行类别预测</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    - X: 测试数据集，一个(测试样本数量，维度)大小的numpy数组</span></span><br><span class="line"><span class="string">    - k: 最近邻数量</span></span><br><span class="line"><span class="string">    - num_loops: 计算测试样本和训练样本距离的方法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    - y: 类别预测结果，一个(测试样本数量，1)大小的numpy数组，其中y[i]是样本X[i]的预测结果</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    	<span class="keyword">if</span> num_loops == <span class="number">0</span>:</span><br><span class="line">      		dists = self.compute_distances_no_loops(X)</span><br><span class="line">	    <span class="keyword">elif</span> num_loops == <span class="number">1</span>:</span><br><span class="line">      		dists = self.compute_distances_one_loop(X)</span><br><span class="line">	    <span class="keyword">elif</span> num_loops == <span class="number">2</span>:</span><br><span class="line">            dists = self.compute_distances_two_loops(X)</span><br><span class="line">	    <span class="keyword">else</span>:</span><br><span class="line">			<span class="keyword">raise</span> ValueError(<span class="string">'Invalid value %d for num_loops'</span> % num_loops)</span><br><span class="line"></span><br><span class="line">    	<span class="keyword">return</span> self.predict_labels(dists, k=k)</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">compute_distances_two_loops</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">    in self.X_train using a nested loop over both the training data and the </span></span><br><span class="line"><span class="string">    test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_test, D) containing test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span></span><br><span class="line"><span class="string">      is the Euclidean distance between the ith test point and the jth training</span></span><br><span class="line"><span class="string">      point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    通过一个两层的嵌套循环，遍历测试样本点，并求其到全部训练样本点的距离</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    输入:</span></span><br><span class="line"><span class="string">    - X: 测试数据集，一个(测试样本数量, 维度)大小的numpy数组</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    - dists: 一个(测试样本数量, 训练样本数量) 大小的numpy数组，其中dists[i, j]</span></span><br><span class="line"><span class="string">      表示测试样本i到训练样本j的欧式距离</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">		num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    	num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    	dists = np.zeros((num_test, num_train))</span><br><span class="line">    	<span class="keyword">for</span> i <span class="keyword">in</span> range(num_test):</span><br><span class="line">      		<span class="keyword">for</span> j <span class="keyword">in</span> range(num_train):</span><br><span class="line">        		<span class="comment">#####################################################################</span></span><br><span class="line">		        <span class="comment"># <span class="doctag">TODO:</span>                                                             #</span></span><br><span class="line">		        <span class="comment"># Compute the l2 distance between the ith test point and the jth    #</span></span><br><span class="line">		        <span class="comment"># training point, and store the result in dists[i, j]. You should   #</span></span><br><span class="line">		        <span class="comment"># not use a loop over dimension.                                    #</span></span><br><span class="line">		        <span class="comment"># 任务:                                                             #</span></span><br><span class="line">		        <span class="comment"># 计算第i个测试点到第j个训练样本点的L2距离，并保存到dists[i, j]中，         #</span></span><br><span class="line">		        <span class="comment"># 注意不要在维度上使用for循环                                           #</span></span><br><span class="line">		        <span class="comment">#####################################################################</span></span><br><span class="line">	        	dists[i, j] = np.sqrt(np.sum(np.square(self.X_train[j,:] - X[i,:])))</span><br><span class="line">                <span class="comment"># 这里的np.square()是对其中每个元素平方操作</span></span><br><span class="line">		        <span class="comment">#####################################################################</span></span><br><span class="line">		        <span class="comment">#                       END OF YOUR CODE                            #</span></span><br><span class="line">		        <span class="comment">#                       任务结束                                     #</span></span><br><span class="line">		        <span class="comment">#####################################################################</span></span><br><span class="line">    	<span class="keyword">return</span> dists</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">compute_distances_one_loop</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">    in self.X_train using a single loop over the test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input / Output: Same as compute_distances_two_loops</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    通过一个单层的嵌套循环，遍历测试样本点，并求其到全部训练样本点的距离</span></span><br><span class="line"><span class="string">    输入/输出：和compute_distances_two_loops方法相同</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    	num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    	num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    	dists = np.zeros((num_test, num_train))</span><br><span class="line">    	<span class="keyword">for</span> i <span class="keyword">in</span> range(num_test):</span><br><span class="line">      		<span class="comment">#######################################################################</span></span><br><span class="line">      		<span class="comment"># <span class="doctag">TODO:</span>                                                               #</span></span><br><span class="line">      		<span class="comment"># Compute the l2 distance between the ith test point and all training #</span></span><br><span class="line">      		<span class="comment"># points, and store the result in dists[i, :].                        #</span></span><br><span class="line">      		<span class="comment"># 任务:                                                               #</span></span><br><span class="line">      		<span class="comment"># 计算第i个测试样本点到所有训练样本点的L2距离，并保存到dists[i, :]中          #</span></span><br><span class="line">      		<span class="comment">#######################################################################</span></span><br><span class="line">			dists[i,:] = np.sqrt(np.sum(np.square(self.X_train - X[i,:]),axis = <span class="number">1</span>))</span><br><span class="line">      		<span class="comment">#######################################################################</span></span><br><span class="line">      		<span class="comment">#                         END OF YOUR CODE                            #</span></span><br><span class="line">      		<span class="comment">#                         任务结束                                     #</span></span><br><span class="line">      		<span class="comment">#######################################################################</span></span><br><span class="line">	<span class="keyword">return</span> dists</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">compute_distances_no_loops</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">    in self.X_train using no explicit loops.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input / Output: Same as compute_distances_two_loops</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    不通过循环方式，遍历测试样本点，并求其到全部训练样本点的距离</span></span><br><span class="line"><span class="string">    输入/输出：和compute_distances_two_loops方法相同</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    	num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    	num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    	dists = np.zeros((num_test, num_train)) </span><br><span class="line">    	<span class="comment">#########################################################################</span></span><br><span class="line">    	<span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">    	<span class="comment"># Compute the l2 distance between all test points and all training      #</span></span><br><span class="line">    	<span class="comment"># points without using any explicit loops, and store the result in      #</span></span><br><span class="line">    	<span class="comment"># dists.                                                                #</span></span><br><span class="line">    	<span class="comment">#                                                                       #</span></span><br><span class="line">    	<span class="comment"># You should implement this function using only basic array operations; #</span></span><br><span class="line">    	<span class="comment"># in particular you should not use functions from scipy.                #</span></span><br><span class="line">    	<span class="comment">#                                                                       #</span></span><br><span class="line">    	<span class="comment"># HINT: Try to formulate the l2 distance using matrix multiplication    #</span></span><br><span class="line">    	<span class="comment">#       and two broadcast sums.                                         #</span></span><br><span class="line">    	<span class="comment"># 任务:                                                                 #</span></span><br><span class="line">    	<span class="comment"># 计算测试样本点和训练样本点之间的L2距离，并且不使用for循环，最后将结果           #</span></span><br><span class="line">    	<span class="comment"># 保存到dists中                                                          #</span></span><br><span class="line">    	<span class="comment">#                                                                       #</span></span><br><span class="line">    	<span class="comment"># 请使用基本的数组操作完成该方法；另外，不要使用scipy中的方法                    #</span></span><br><span class="line">    	<span class="comment">#                                                                       #</span></span><br><span class="line">    	<span class="comment"># 提示: 可以使用矩阵乘法和两次广播加法                                        #</span></span><br><span class="line">    	<span class="comment">#########################################################################</span></span><br><span class="line">		<span class="comment"># 这里对完全平方公式做了展开，以方便我们实现矢量化，(a-b)^2=a^2+b^2-2ab</span></span><br><span class="line">        dists = np.sqrt(<span class="number">-2</span>*np.dot(X, self.X_train.T) <span class="comment"># 一个500*5000的矩阵</span></span><br><span class="line">                        + np.sum(np.square(self.X_train), axis=<span class="number">1</span>) <span class="comment"># 一个1*5000的矩阵</span></span><br><span class="line">                        + np.transpose([np.sum(np.square(X), axis=<span class="number">1</span>)]))	<span class="comment"># 一个500*1的矩阵</span></span><br><span class="line">        <span class="comment"># 留意self.X_train的shape是(5000,3072),X的shape是(500,3072)</span></span><br><span class="line">        <span class="comment"># np.dot()是矩阵乘法, np.sqrt()和np.square()都是对矩阵每个元素的操作</span></span><br><span class="line">        <span class="comment"># np.sum(array_like, axis=1)是对矩阵每行分别求和，即每个图片像素求和</span></span><br><span class="line">        <span class="comment"># 注意：后两个矩阵的加法可以张成一个500*5000的矩阵！</span></span><br><span class="line">    	<span class="comment">#########################################################################</span></span><br><span class="line">    	<span class="comment">#                         END OF YOUR CODE                              #</span></span><br><span class="line">    	<span class="comment">#                         任务结束                                       #</span></span><br><span class="line">    	<span class="comment">#########################################################################</span></span><br><span class="line">	<span class="keyword">return</span> dists</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">predict_labels</span><span class="params">(self, dists, k=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Given a matrix of distances between test points and training points,</span></span><br><span class="line"><span class="string">    predict a label for each test point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span></span><br><span class="line"><span class="string">      gives the distance betwen the ith test point and the jth training point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (num_test,) containing predicted labels for the</span></span><br><span class="line"><span class="string">      test data, where y[i] is the predicted label for the test point X[i].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    通过距离矩阵，预测每一个测试样本的类别</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    - dists: 一个(测试样本数量，训练样本数量)大小的numpy数组，其中dists[i, j]表示</span></span><br><span class="line"><span class="string">      第i个测试样本到第j个训练样本的距离</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    - y: 一个(测试样本数量，)大小的numpy数组，其中y[i]表示测试样本X[i]的预测结果</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">		num_test = dists.shape[<span class="number">0</span>]</span><br><span class="line">    	y_pred = np.zeros(num_test)</span><br><span class="line">    	<span class="keyword">for</span> i <span class="keyword">in</span> range(num_test):</span><br><span class="line">      		<span class="comment"># A list of length k storing the labels of the k nearest neighbors to</span></span><br><span class="line">      		<span class="comment"># the ith test point.</span></span><br><span class="line">      		<span class="comment"># 一个长度为k的list数组，其中保存着第i个测试样本的k个最近邻的类别标签</span></span><br><span class="line">      		closest_y = []</span><br><span class="line">      		<span class="comment">#########################################################################</span></span><br><span class="line">      		<span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">      		<span class="comment"># Use the distance matrix to find the k nearest neighbors of the ith    #</span></span><br><span class="line">      		<span class="comment"># testing point, and use self.y_train to find the labels of these       #</span></span><br><span class="line">      		<span class="comment"># neighbors. Store these labels in closest_y.                           #</span></span><br><span class="line">      		<span class="comment"># Hint: Look up the function numpy.argsort.                             #</span></span><br><span class="line">      		<span class="comment"># 任务:                                                                 #</span></span><br><span class="line">      		<span class="comment"># 通过距离矩阵找到第i个测试样本的k个最近邻,然后在self.y_train中找到这些         #</span></span><br><span class="line">      		<span class="comment"># 最近邻对应的类别标签，并将这些类别标签保存到closest_y中。                    #</span></span><br><span class="line">      		<span class="comment"># 提示: 可以尝试使用numpy.argsort方法                                      #</span></span><br><span class="line">      		<span class="comment">#########################################################################</span></span><br><span class="line">      		<span class="comment"># 在dists距离矩阵中，第i个测试图片(第i行)下的列(训练图片)，按值从小到大排列，</span></span><br><span class="line">            <span class="comment"># 取出前k个的索引(原列数，亦训练集的索引)，最后根据该索引在测试集中切片出相应的标签值</span></span><br><span class="line">            closest_y = self.y_train[np.argsort(dists[i])[:k]]</span><br><span class="line">            <span class="comment"># np.argsort()函数返回的是数组值从小到大的索引值</span></span><br><span class="line">      		<span class="comment">#########################################################################</span></span><br><span class="line">      		<span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">      		<span class="comment"># Now that you have found the labels of the k nearest neighbors, you    #</span></span><br><span class="line">      		<span class="comment"># need to find the most common label in the list closest_y of labels.   #</span></span><br><span class="line">      		<span class="comment"># Store this label in y_pred[i]. Break ties by choosing the smaller     #</span></span><br><span class="line">      		<span class="comment"># label.                                                                #</span></span><br><span class="line">      		<span class="comment"># 任务:                                                                 #</span></span><br><span class="line">      		<span class="comment"># 现在你已经找到了k个最近邻对应的标签, 下面就需要找到其中最普遍的那个             #</span></span><br><span class="line">      		<span class="comment"># 类别标签，然后保存到y_pred[i]中。如果有票数相同的类别，则选择编号小            #</span></span><br><span class="line">      		<span class="comment"># 的类别                                                                #</span></span><br><span class="line">      		<span class="comment">#########################################################################</span></span><br><span class="line">      		y_pred[i] = np.argmax(np.bincount(closest_y))</span><br><span class="line">            <span class="comment"># np.bincount()是凡是与索引值相同的值，计数+1，计数结果返回为新的索引值里的计数值</span></span><br><span class="line">            <span class="comment"># np.argmax() 返回的是数值最大的索引值，在这个例子中即是标签</span></span><br><span class="line">      		<span class="comment">#########################################################################</span></span><br><span class="line">      		<span class="comment">#                           END OF YOUR CODE                            # </span></span><br><span class="line">      		<span class="comment">#########################################################################</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure></section>
    <!-- Tags START -->
    
      <div class="tags">
        <span>Tags:</span>
        
  <a href="/tags#漫游CS231n" >
    <span class="tag-code">漫游CS231n</span>
  </a>

      </div>
    
    <!-- Tags END -->
    <!-- NAV START -->
    
  <div class="nav-container">
    <!-- reverse left and right to put prev and next in a more logic postition -->
    
      <a class="nav-left" href="/2018/02/NG_DeepLearning1/">
        <span class="nav-arrow">← </span>
        
          Andrew Ng 的 DeepLearning.ai 课程学习：神经网络和深度学习
        
      </a>
    
    
      <a class="nav-right" href="/2018/02/CS231n_MXNet2/">
        
          CS231n 与 MXNet 实现：线性分类
        
        <span class="nav-arrow"> →</span>
      </a>
    
  </div>

    <!-- NAV END -->
    <!-- 打赏 START -->
    
      <div class="money-like">
        <div class="reward-btn">
          赏
          <span class="money-code">
            <span class="alipay-code">
              <div class="code-image"></div>
              <b>使用支付宝打赏</b>
            </span>
            <span class="wechat-code">
              <div class="code-image"></div>
              <b>使用微信打赏</b>
            </span>
          </span>
        </div>
        <p class="notice">若你觉得我的文章对你有帮助，欢迎点击上方按钮对我打赏</p>
      </div>
    
    <!-- 打赏 END -->
    <!-- 二维码 START -->
    
      <div class="qrcode">
        <canvas id="share-qrcode"></canvas>
        <p class="notice">扫描二维码，分享此文章</p>
      </div>
    
    <!-- 二维码 END -->
    
      <!-- Gitment START -->
      <div id="comments"></div>
      <!-- Gitment END -->
    
  </article>
  <!-- Article END -->
  <!-- Catalog START -->
  
    <aside class="catalog-container">
  <div class="toc-main">
    <strong class="toc-title">Catalog</strong>
    
      <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#图像分类"><span class="toc-nav-text">图像分类</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Nearest-Neighbor分类器"><span class="toc-nav-text">Nearest Neighbor分类器</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#k-Nearest-Neighbor分类器"><span class="toc-nav-text">k-Nearest Neighbor分类器</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#用于超参数调优的验证集"><span class="toc-nav-text">用于超参数调优的验证集</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Nearest-Neighbor分类器的优劣"><span class="toc-nav-text">Nearest Neighbor分类器的优劣</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#小结"><span class="toc-nav-text">小结</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#小结：实际应用k-NN"><span class="toc-nav-text">小结：实际应用k-NN</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#拓展阅读"><span class="toc-nav-text">拓展阅读</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#作业：k-Nearest-Neighbor-kNN"><span class="toc-nav-text">作业：k-Nearest Neighbor (kNN)</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Cross-validation"><span class="toc-nav-text">Cross-validation</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#交叉验证"><span class="toc-nav-text">交叉验证</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#k-nearest-neighbor-py"><span class="toc-nav-text">k_nearest_neighbor.py</span></a></li></ol></li></ol>
    
  </div>
</aside>
  
  <!-- Catalog END -->
</main>

<script>
  (function () {
    var url = 'http://iphysresearch.github.io/2018/02/CS231n_MXNet1/';
    var banner = 'https://i.loli.net/2018/02/15/5a855eaf63ccd.png'
    if (banner !== '' && banner !== 'undefined' && banner !== 'null') {
      $('#article-banner').css({
        'background-image': 'url(' + banner + ')'
      })
    } else {
      $('#article-banner').geopattern(url)
    }
    $('.header').removeClass('fixed-header')

     // error image
    $(".markdown-content img").on('error', function() {
      $(this).attr('src', 'http://file.muyutech.com/error-img.png')
      $(this).css({
        'cursor': 'default'
      })
    })

    // zoom image
    $(".markdown-content img").on('click', function() {
      var src = $(this).attr('src')
      if (src !== 'http://file.muyutech.com/error-img.png') {
        var imageW = $(this).width()
        var imageH = $(this).height()
        
        var zoom = ($(window).width() * 0.95 / imageW).toFixed(2)
        zoom = zoom < 1 ? 1 : zoom
        zoom = zoom > 2 ? 2 : zoom
        var transY = (($(window).height() - imageH) / 2).toFixed(2)

        $('body').append('<div class="image-view-wrap"><div class="image-view-inner"><img src="'+ src +'" /></div></div>')
        $('.image-view-wrap').addClass('wrap-active')
        $('.image-view-wrap img').css({
          'width': `${imageW}`,
          'transform': `translate3d(0, ${transY}px, 0) scale3d(${zoom}, ${zoom}, 1)`
        })
        $('html').css('overflow', 'hidden')

        $('.image-view-wrap').on('click', function() {
          $(this).remove()
          $('html').attr('style', '')
        })
      }
    })

    // qrcode
    var qr = new QRious({
      element: document.getElementById('share-qrcode'),
      value: document.location.href
    });

    // gitment
    var gitmentConfig = "iphysresearch";
    if (gitmentConfig !== 'undefined') {
      var gitment = new Gitment({
        id: "CS231n 与 MXNet 实现：图像分类",
        owner: "iphysresearch",
        repo: "iphysresearch.github.io",
        oauth: {
          client_id: "6b978dc207dc30e58ec8",
          client_secret: "2bc56895d0221e8c27ab87b072f8f18523231e22"
        },
        theme: {
          render(state, instance) {
            const container = document.createElement('div')
            container.lang = "en-US"
            container.className = 'gitment-container gitment-root-container'
            container.appendChild(instance.renderHeader(state, instance))
            container.appendChild(instance.renderEditor(state, instance))
            container.appendChild(instance.renderComments(state, instance))
            container.appendChild(instance.renderFooter(state, instance))
            return container;
          }
        }
      })
      gitment.render(document.getElementById('comments'))
    }
  })();
</script>

    <div class="scroll-top">
  <span class="arrow-icon"></span>
</div>
    <footer class="app-footer">
  <p class="copyright">
    &copy; 2018 | Proudly powered by <a href="https://hexo.io" target="_blank">Hexo</a>
    <br>
    Theme by <a href="https://github.com/yanm1ng">yanm1ng</a>
  </p>
</footer>

<script>
  function async(u, c) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }
</script>
<script>
  async("//cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
    FastClick.attach(document.body);
  })
</script>

<script>
  var hasLine = 'true';
  async("//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js", function(){
    $('figure pre').each(function(i, block) {
      var figure = $(this).parents('figure');
      if (hasLine === 'false') {
        figure.find('.gutter').hide();
      }
      var lang = figure.attr('class').split(' ')[1] || 'code';
      var codeHtml = $(this).html();
      var codeTag = document.createElement('code');
      codeTag.className = lang;
      codeTag.innerHTML = codeHtml;
      $(this).attr('class', '').empty().html(codeTag);
      figure.attr('data-lang', lang.toUpperCase());
      hljs.highlightBlock(block);
    });
  })
</script>
<!-- Baidu Tongji -->

<script src="/js/script.js"></script>
  </body>
</html>