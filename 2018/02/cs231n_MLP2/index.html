<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
  <meta name="description" content="Machine Learning, Deep Learning, Physics">
  <meta name="keyword" content="hexo-theme, vuejs">
  
    <link rel="shortcut icon" href="/css/images/logo.png">
  
  <title>
    
      一段关于神经网络的故事：所谓的前向传播 | Teaching is Learning
    
  </title>
  <link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.css" rel="stylesheet">
  <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/tomorrow.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/plugins/gitment.css">
  <script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
  <script src="//cdn.bootcss.com/geopattern/1.2.3/js/geopattern.min.js"></script>
  <script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>
  <script src="/js/qrious.js"></script>
<script src="/js/gitment.js"></script>
  
  
    <!-- MathJax support START -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- MathJax support END -->
  


</head>
<div class="wechat-share">
  <img src="/css/images/logo.png" />
</div>

  <body>
    <header class="header fixed-header">
  <div class="header-container">
    <a class="home-link" href="/">
      <div class="logo"></div>
      <span>Teaching is Learning</span>
    </a>
    <ul class="right-list">
      
        <li class="list-item">
          
            <a href="/" class="item-link">Home</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/tags/" class="item-link">Tags</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/archives/" class="item-link">Archives</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/project/" class="item-link">Projects</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/about/" class="item-link">About</a>
          
        </li>
      
    </ul>
    <div class="menu">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </div>
    <div class="menu-mask">
      <ul class="menu-list">
        
          <li class="menu-item">
            
              <a href="/" class="menu-link">Home</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/tags/" class="menu-link">Tags</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/archives/" class="menu-link">Archives</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/project/" class="menu-link">Projects</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/about/" class="menu-link">About</a>
            
          </li>
        
      </ul>
    </div>
  </div>
</header>

    <div id="article-banner">
  <h2>一段关于神经网络的故事：所谓的前向传播</h2>
  <p class="post-date">2018-02-04</p>
  <div class="arrow-down">
    <a href="javascript:;"></a>
  </div>
</div>
<main class="app-body flex-box">
  <!-- Article START -->
  <article class="post-article">
    <section class="markdown-content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><ul>
<li>Contents：<ol>
<li>一个神经元的本事<ul>
<li>一张图片 v.s 一个神经元</li>
<li>多张图片 v.s 一个神经元</li>
</ul>
</li>
<li>强大的层状神经元<ul>
<li>一张图片 v.s 多个神经元</li>
<li>多张图片 v.s 多个神经元</li>
</ul>
</li>
<li>不废话了，看代码！</li>
<li>前向传播手绘图</li>
</ol>
</li>
</ul>
<a id="more"></a>
<blockquote>
<p>这篇系列长文的灵感来源于自己参与【大数据文摘&amp;稀牛学院】的一个 CS231n 作业资料整理与讲解 case，详情可见：大数据文摘<a href="https://study.163.com/provider/10146755/index.htm" target="_blank" rel="noopener">网易云课堂专栏</a>，<a href="http://blog.csdn.net/bigdatadigest/article/details/79286510" target="_blank" rel="noopener">CSDN专栏</a> 和 <a href="https://github.com/theBigDataDigest/Stanford-CS231n-assignments-in-Chinese/tree/master/assignment2/Q1-Q3" target="_blank" rel="noopener">GitHub专栏</a>。 其中，我负责的是 CS231n 的 Assignment2 中的Q1-Q3部分，也正是此故事系列的主要内容，探讨的是基于全连接网络下各种常用技术的 Python 代码实现。</p>
</blockquote>
<p><br> </p>
<p>（接上文：<a href="https://iphysresearch.github.io/2018/02/cs231n_MLP1/">一段关于神经网络的故事：引言</a>）</p>
<p><br></p>
<h1 id="所谓的前向传播"><a href="#所谓的前向传播" class="headerlink" title="所谓的前向传播"></a>所谓的前向传播</h1><p><br></p>
<h2 id="一个神经元的本事"><a href="#一个神经元的本事" class="headerlink" title="一个神经元的本事"></a>一个神经元的本事</h2><p>我们先仅<strong>前向传播</strong>而言，来谈谈<strong>一个神经元</strong>究竟是做了什么事情。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/2301760-08e7b1c0cdfe2263.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><br></p>
<p><strong>前向传播</strong>，顾名思义，这名字起的也是神乎其神的，说白了就是将样本图片的数据信息，沿着箭头正向传给一个带参数的神经网络层中咀嚼一番，然后再吐出来一堆数据再喂给后面的一层吃罢了(如此而已，居然就叫做了前向/正向传播了，让人忍不住吐槽一番)。那么，对于一个<strong>全连接层(fully-connected layer)</strong> <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="全连接层中的神经元与其前后两层的神经元是完全成对连接的，但是在同一个全连接层内的神经元之间没有连接。">[1]</span></a></sup>的前向传播来说，所谓的“带参数的神经网络层”一般就是指对输入数据源(此后用”数据源”这个词来表示输入层所有输入样本图片数据总体)先进行一个矩阵乘法，然后加上偏置，得到数字再运用激活函数”修饰”出去，最后再反复迭代这个过程罢了（后文都默认使用此线性模型）。</p>
<p><br></p>
<p>是不是晕了？别着急，我们进一步嚼碎了来看看一个神经元(处于第一隐藏层)究竟是如何处理输入层传来的<strong>一张样本图片</strong>(带有猫咪标签)的？</p>
<p><br></p>
<h3 id="一张图片-v-s-一个神经元"><a href="#一张图片-v-s-一个神经元" class="headerlink" title="一张图片 v.s 一个神经元"></a>一张图片 v.s 一个神经元</h3><p><img src="http://images2015.cnblogs.com/blog/405927/201603/405927-20160327155604964-1011783613.png" alt=""></p>
<p>上面提到过，输入数据源是一张尺寸为$32\times 32$的RGB彩色图像，我们假定输入数据$x_i$的个数是$D$的话（即$i$是有$D$个），那这个$D=32\times 32\times 3=3072$。为了普遍意义，下文继续用大写字母$D$来表示一张图片作为数据源的维数个数（如果该神经元位于隐藏层，则大写字母$D$表示本隐藏层神经元的神经元个数，下一节还会提到）。</p>
<p><br></p>
<p>显然，一张图片中的$D$个数据$x_i$包含了判断该图片是一支猫的所有特征信息，那么我们就需要”充分利用”这些信息来给这张样本图片”打个分”，来评价一下这张图像究竟有多像猫。</p>
<p><br></p>
<p>不能空口套白狼，一张美图说明问题：</p>
<hr>
<p><img src="https://i.loli.net/2018/02/10/5a7e646283836.png" alt=""></p>
<p><strong>左图</strong>不用看，这个一般是用来装X用的，并不是真的要严格类比。虽然最初的神经网络算法确实是受生物神经系统的启发，但是现在早已与之分道扬镳，成为一个工程问题。关键我们是要看<strong>右图</strong>的数学模型(严格地说，这就是传说中的<strong>感知器perceptron</strong>)。</p>
<hr>
<p>如右图中的数学模型所示，我们为每一个喂进来的数据$x_i$都对应的”许配”一个”权重”参数$w_i$，再加上一个偏置$b$，然后一股脑的把他们都加起来得到一个数(scalar)：<br>$$<br>\sum_iw_ix_i+b = w_0x_0+w_1x<em>1+\cdots+w</em>{D-1}x_{D-1}+b\<br>$$<br>上面的代数表达式看上去很繁杂，不容易推广，所以我们把它改写成<br>$$<br>\underbrace{\begin{bmatrix}\sum_iw_ix<em>i+b<br>\end{bmatrix}}</em>{1\times 1}=<br>\underbrace{\begin{bmatrix}<br>\cdots &amp; x<em>i &amp; \cdots<br>\end{bmatrix}}</em>{1\times D}<br>\cdot<br>\underbrace{\begin{bmatrix}<br>\vdots  \  w<em>i   \<br>\vdots<br>\end{bmatrix}}</em>{D\times 1}<br>+<br>\underbrace{\begin{bmatrix}<br> b<br>\end{bmatrix}}_{1\times 1}<br>$$<br>上面等式左侧这样算出的一个数字，表示为对于输入进来的$D$个数据$x_i$，在当前选定的参数$(w_i,b)$下，这个<strong>神经元能够正确评价其所对应的”猫咪”标签的程度</strong>。所以，这个得分越高，越能说明其对应的在某种$(w_i,b)$这$D+1$个参数的评价下，该神经元正确判断的能力越好，准确率越高。</p>
<p><br></p>
<p>换句话说，相当于是有一个神经元坐在某选秀的评委席里，戴着一款度数为$(w_i,b)$雷朋眼镜，给某一位台上模仿猫咪的样本图片$x_i$打了一个分(评价分数)。显然，得分的高低是不仅依赖于台上的主角$x_i$的表现，还严重依赖于神经元评委戴着的有色眼镜(参数$w_i,b$)。当然，我们已经假定评委的智商(线性模型)是合乎统一要求的。</p>
<p><br></p>
<p>现如今，参加选秀的人可谓趋之若鹜，一个神经元评委该如何同时的批量化打分，提高效率嗯？</p>
<p><br></p>
<h3 id="多张图片-v-s-一个神经元"><a href="#多张图片-v-s-一个神经元" class="headerlink" title="多张图片 v.s 一个神经元"></a>多张图片 v.s 一个神经元</h3><p>也就是说，一个神经元面对$N$张图片该如何给每一张图片打分的问题。这就是矩阵表达式的优势了，我们只需要很自然地把上述矩阵表达式纵向延展下即可，如下所示：</p>
<p>$$<br>\underbrace{\begin{bmatrix}<br> \sum_iw_ix<em>i+b  \<br> \vdots<br>\end{bmatrix}}</em>{N\times1 }<br>=<br>\underbrace{\begin{bmatrix}<br>\cdots &amp; x<em>i &amp; \cdots \<br>&amp; \vdots &amp;<br>\end{bmatrix}}</em>{N\times D}<br>\cdot<br>\underbrace{\begin{bmatrix}<br>\vdots  \<br>  w<em>i   \<br>\vdots<br>\end{bmatrix}}</em>{D\times 1}<br>+<br>\underbrace{\begin{bmatrix}<br> b \<br> \vdots\<br> \vdots<br>\end{bmatrix}}_{N\times 1}<br>$$<br>上面矩阵表达式中，等号左侧的得分矩阵中每一行运算都是独立并行的，并且其每一行分别代表$N$张样本图片的数据经过一个神经元后的得分数值。到此，我们就明白了一个神经元是如何面对一个shape为(N, D)的输入样本图片数据矩阵，并给出得分的。</p>
<p><br></p>
<p>然而，关于一个神经元的故事还没完。。。。</p>
<p><br></p>
<p>你可能注意到了，上面例子中的美图中有个函数f，我们把图放大仔细看清楚：</p>
<p><img src="http://cs231n.github.io/assets/nn1/neuron_model.jpeg" alt=""></p>
<p>在神经元对每张图片算得的“得分”送给下一个神经元之前都要经过一个函数f的考验。这就暗示我们，选秀节目的导演对神经元评委给出的得分还并不满意，为了(未来模型训练的)快捷方便，导演要求对每一个得分需要做进一步的“激活”处理（即上图中的函数$f$），于是这个叫<strong>激活函数(activation)</strong>的家伙会对结果做进一步的处理，比如大家这些年都在用的ReLU函数上来就是临门一脚，要求把得分小于零的都阉割掉！一律给0分（都得负分的了还选什么秀啊？给0分滚蛋）：<br>$$<br>f(x)=\max(0,x)<br>$$</p>
<p>所以，总结下来，一个神经元干的活就是如下所示的公式：<br>$$<br>\text{out}=f(\sum_iw_ix_i+b)=\max(0,\sum_iw_ix_i+b)<br>$$<br>如果这个数学看得让人心烦意乱，不要怕，一层神经元的故事之后就是万众期待的Python代码实现了，相信看后会让你不禁感慨：“小样！不过如此嘛～”</p>
<p><br></p>
<p><strong>小备注：</strong></p>
<p><br></p>
<p>这里最后再多一句嘴：一个神经元在前向传播中输出的只是一个数字，另外，神经网络的训练过程，训练的是上述提到的模型参数$(w_i,b)$。</p>
<p><br></p>
<p>再多一句嘴，通常我们在整个神经网络结构中只使用一种激活函数。并且值得你注意的是，全连接层的最后一层就是<strong>输出层</strong>，除了这个最后一层，其它的全连接层神经元都要包含激活函数。</p>
<p><br></p>
<p>最最后说再一句，由于神经网络的激活函数都是<u>非线性的</u>，所以可以说神经网络是一个<strong>非线性</strong>的分类器。</p>
<p><img src="https://i.loli.net/2018/02/10/5a7eb4767888c.png" alt=""></p>
<p><br></p>
<h2 id="强大的层状神经元"><a href="#强大的层状神经元" class="headerlink" title="强大的层状神经元"></a>强大的层状神经元</h2><p>在正式开始谈一层神经元之前，我们继续来探讨下神经元面对一张图片还可以做什么？</p>
<p><br></p>
<h3 id="一张图片-v-s-多个神经元"><a href="#一张图片-v-s-多个神经元" class="headerlink" title="一张图片 v.s 多个神经元"></a>一张图片 v.s 多个神经元</h3><p>对于一张标签是猫咪的样本图片，我们光能评价有多么的像猫咪还不能满足，我们还需要神经元评价一下其他9个标签才行，然后才好比较评判得出最终结论。于是，光用$(w_i,b)$ 这$D+1$个参数就不够用了，应该要有$10\times(D+1)$个参数才行。还是用那个恶搞的例子说明的话，就是说一个神经元评委可不够用哦，要10个戴着不同有色眼镜的神经元评委分头各自去考察10个不同标签，这样就可以对每个样本图片给出10个对应不同类别的得分。</p>
<p><br></p>
<p>所以，我们可以在最初的矩阵表达式 $\sum_iw_ix_i+b$ 的基础上横向延展成如下矩阵表达式：</p>
<p>$$<br>\underbrace{\begin{bmatrix}<br> \sum_iw_ix<em>i+b &amp;  \cdots &amp;<br>\end{bmatrix}}</em>{1\times 10}<br>=<br>\underbrace{\begin{bmatrix}<br>\cdots &amp; x<em>i &amp; \cdots<br>\end{bmatrix}}</em>{1\times D}<br>\cdot<br>\underbrace{\begin{bmatrix}<br>\vdots &amp; &amp;  \<br>  w<em>i &amp; \cdots &amp; \cdots \<br>\vdots &amp;  &amp;<br>\end{bmatrix}}</em>{D\times 10}<br>+<br>\underbrace{\begin{bmatrix}<br> b &amp;\cdots &amp;<br>\end{bmatrix}}_{\underset{\text{Broadcasting}}{1\times 10}}<br>$$<br>忘了说，在上面表达式里，大括号下面的数字表示的是当前矩阵的【行数$\times$列数】。同样地，这里运用了矩阵乘法和向量化表示会大大的提高运算效率。在如此简单的矩阵运算之后，等号左边所得到一行数组(行向量)就表达了在参数矩阵w和向量b的描述下，该一张样本图片分别对应10个不同类标标签的得分。别忘了，我们最终的目标就是学习如何训练这里的参数w和b，并且我们希望训练好后的某样本图片在正确目标标签下所对应的得分是最高的。</p>
<p><br></p>
<p>为了便于直观理解，给你一个简单数值栗子尝尝鲜（和上面的矩阵公式有点区别，但并不影响理解，都转置一下就一模一样了）：</p>
<hr>
<p><img src="http://upload-images.jianshu.io/upload_images/2301760-339645fdcb2c2cdb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到我们拿了样本猫咪图片中的四个像素作为一个神经元的输入数据源，不过上图只查看了3个标签(猫／狗／船)，且把输入数据$x_i$改成用列向量表达罢了，这并不影响我们理解，无非是我们的矩阵公式改为$w^T x + b^T$表达 。接下来看图说话，可以看到在如图初始化矩阵$W$和$b$的情况下，算出的得分对应于猫咪的分数居然是最低的<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="0.2\*56 -0.5\*231 +0.1\*24 +2.0\*2 +1.1 = -96.8">[2]</span></a></sup>，这说明$W$和$b$的值没有训练好啊！</p>
<p><br></p>
<p>那么究竟该如何训练出合适的参数呢？难道每次都要肉眼观察每个标签算出的得分再同时对比参数选的究竟好不好？再难道每次都要自己手调参数矩阵的每个值来观察得分效果么？当然不会这么傻啦，到时候一个叫<strong>损失函数</strong>的概念就登场了，且继续听故事先～</p>
<hr>
<p>回到上面经过横向延展后的矩阵表达式：</p>
<p>$$<br>\underbrace{\begin{bmatrix}<br> \sum_iw_ix<em>i+b &amp;  \cdots &amp;<br>\end{bmatrix}}</em>{1\times 10}<br>=<br>\underbrace{\begin{bmatrix}<br>\cdots &amp; x<em>i &amp; \cdots<br>\end{bmatrix}}</em>{1\times D}<br>\cdot<br>\underbrace{\begin{bmatrix}<br>\vdots &amp; &amp;  \<br>  w<em>i &amp; \cdots &amp; \cdots \<br>\vdots &amp;  &amp;<br>\end{bmatrix}}</em>{D\times 10}<br>+<br>\underbrace{\begin{bmatrix}<br> b &amp;\cdots &amp;<br>\end{bmatrix}}_{\underset{\text{Broadcasting}}{1\times 10}}<br>$$<br>可以看到对于一个$D$维图片$x_i$，都和w矩阵的每一列(10个神经元评委)有过亲密接触，独立并行的进行过矩阵乘法，回想一下，这不和一个神经元面对一张样本图片的公式异曲同工了么？只不过换成了10个神经元并行面对一张样本图片。所以，上述矩阵表达式就相当于一张图片的数据从输入层流到含有10个神经元的层状结构（无激活函数）。（严格地说，这其实是<strong>无隐层神经网络</strong>，或者叫<strong>单层感知器</strong>，因为输入层的下一层就是输出层，直接输出了10个标签的打分结果了，如下面的示意图）</p>
<p><img src="http://ovj0qranm.bkt.clouddn.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cinputs%E5%92%8Coutputs.jpg" alt=""></p>
<p><br></p>
<h3 id="多张图片-v-s-多个神经元"><a href="#多张图片-v-s-多个神经元" class="headerlink" title="多张图片 v.s 多个神经元"></a>多张图片 v.s 多个神经元</h3><p>接下来，推广到一般的隐藏层神经元们是如何干活的就易如反掌了！只要充分利用矩阵乘法，就可以很清楚了。小结如下：</p>
<hr>
<p><img src="http://simtalk.cn/img/Neural-Network/DNN.jpg" alt=""><br>$$<br>\underbrace{ \begin{bmatrix}<br> &amp;  &amp; \<br> &amp; \max(0,\hat{x}<em>i) &amp; \<br> &amp;  &amp;<br>\end{bmatrix}}</em>{N\times M}<br>\overleftarrow{=}<br>\underbrace{\begin{bmatrix}<br> &amp;  &amp; \<br> &amp; \hat{x}<em>i &amp; \<br> &amp;  &amp;<br>\end{bmatrix}}</em>{N\times M}<br>\overleftarrow{=}<br>\underbrace{\begin{bmatrix}<br> &amp;  &amp; \<br> &amp; x<em>i &amp; \<br> &amp;  &amp;<br>\end{bmatrix}}</em>{N\times H}<br>\cdot<br>\underbrace{\begin{bmatrix}<br> &amp;  &amp; \<br> &amp; w<em>{ij} &amp; \<br> &amp;  &amp;<br>\end{bmatrix}}</em>{H\times M}<br>+<br>\underbrace{\begin{bmatrix}<br> &amp;  &amp; \<br> &amp;  b<em>i &amp; \<br> &amp;  &amp;<br>\end{bmatrix}}</em>{\underset{\text{Broadcasting}}{N\times M}}<br>$$<br>从右往左看公式(等号类似赋值操作)。上述矩阵表达式表示的是某一隐藏层的H个神经元们在面对上一层输入数据$x_i$是如何传给下一层的M个神经元的。N表示数据样本的个数，H表示本隐藏层神经元的个数，M表示下一隐藏层神经元的个数(或位于下一输出层的样本图片标签种类数)。（最后矩阵b里的Broadcasting含义见后文哈～不要急）</p>
<p><br></p>
<p>每一个隐藏层得出得分$\hat{x}_i$后，都还需要经过激活函数$f(\hat{x}_i)=\max(0,\hat{x}_i)$的处理，要留意的是最后的输出层并不需要激活函数，给出得分后即可交给损失函数(后文会提到)。</p>
<hr>
<p><strong>小备注：</strong></p>
<p><br></p>
<p>值得注意的是：流入每一层神经元们的输入数据矩阵x和流出每一层神经元们的输出数据矩阵的行数没有变化，都是N，即样本图片个数。每一层神经元们的参数矩阵w和b都是不同的，那么你就能想象到对于一个”很大很深”神经网络而言，需要训练学习的参数个数可是相当多的哦～而且参数矩阵w的维数也很有特点，其行数H和列数M，分别对应于当前隐藏层神经元个数和下一层神经元的个数。如此一来，就把每一层神经元一层套一层连接起来了。</p>
<p><br></p>
<h2 id="不废话了，看代码！"><a href="#不废话了，看代码！" class="headerlink" title="不废话了，看代码！"></a>不废话了，看代码！</h2><p>正如本文开头那句话，</p>
<blockquote>
<p>“…everything became much clearer when I started writing code.”</p>
</blockquote>
<p>再强大的算法，无论矫情得怎么解释，也都不如直接飞代码来得更清晰，更直接。</p>
<p><br></p>
<p>那么任意某一层神经元们在前向传播中，究竟做了什么呢？前面总算把数据如何运动的故事说清楚了，现在开始直接用代码说明一切，第一步，我们定义面对输入数据某一层神经元们给出“得分”的函数 <code>affine_forward(x, w, b) = (out, cache)</code> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_forward</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k) 样本</span></span><br><span class="line"><span class="string">    - w: A numpy array of weights, of shape (D, M) 权重</span></span><br><span class="line"><span class="string">    - b: A numpy array of biases, of shape (M,) 偏置</span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: output, of shape (N, M)</span></span><br><span class="line"><span class="string">    - cache: (x, w, b)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    out = <span class="keyword">None</span>            <span class="comment"># 初始化</span></span><br><span class="line">    reshaped_x = np.reshape(x, (x.shape[<span class="number">0</span>],<span class="number">-1</span>))  <span class="comment"># 确保x是一个规整的矩阵</span></span><br><span class="line">    out = reshape_x.dot(w) +b                    <span class="comment"># out = w x +b</span></span><br><span class="line">    cache = (x, w, b)							</span><br><span class="line">    <span class="comment"># cache将该函数的输入值缓冲储存起来，以备后面计算梯度时使用</span></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<p><strong>代码详解：</strong></p>
<ul>
<li>首先，需要对输入数据$x$进行<strong>矩阵化</strong>，这是因为如果这代表的是第一层神经元前向传播，那么对于我们的图片数据集CIFAR-10，其每张图片输入进来的数据$x$的shape是$(N,32,32,3)$，是一个4维的array，所以需要将其reshape成$(N,3072)$的2维矩阵，其中每行是由一串3072个数字所代表的一个图片样本。<code>np.reshape(x, (x.shape[0],-1))</code> 中的 <code>-1</code> 是指剩余可填充的维度，所以这段代码意思就是保证reshape后的矩阵行数是$N$，剩余的维度信息都规则的排场一行即可。</li>
<li>输出的 <code>cache</code> 变量就是把该函数的输入值$(x,w,b)$存为元组(tuple)再输出出去以备用，它当然不会流到下一层神经元，但其在后面会讲到的反向传播算法中利用到，由此可见我们是有多么的老谋深算啊！</li>
<li>接下来是重点： <code>out = reshape_x.dot(w) +b</code> 这句代码就表达了某一层神经元中，每个神经元可以并行的独立完成上两节提到的线性感知器模型，对每个图像给出自己的评价分数，与代码对应一致的内涵可见如下矩阵表达式：</li>
</ul>
<p>$$<br>\underbrace{\begin{bmatrix}<br> &amp;  &amp; \<br> &amp; \text{out} &amp; \<br> &amp;  &amp;<br>\end{bmatrix}}_{N\times M}<br>=<br>\underbrace{\begin{bmatrix}<br> &amp;  &amp; \<br> &amp; \text{reshape<em>x} &amp; \<br> &amp;  &amp;<br>\end{bmatrix}}</em>{N\times D}<br>\cdot<br>\underbrace{\begin{bmatrix}<br> &amp;  &amp; \<br> &amp; \text{w} &amp; \<br> &amp;  &amp;<br>\end{bmatrix}}<em>{D\times M}<br>+<br>\underbrace{\begin{bmatrix}<br> &amp;  &amp; \<br> &amp; \text{b} &amp; \<br> &amp;  &amp;<br>\end{bmatrix}}</em>{\underset{\text{Broadcasting}}{N\times M}}<br>$$</p>
<p>这里你要清楚的是 <code>reshape_x</code> 中每一行和 <code>w</code> 中的每一列的运算对应于一个神经元面对一张图片的运算过程。矩阵乘法没啥可说的，只要把输入矩阵和要输出的矩阵的维数掰扯清楚了，就会很简单。这里参数w和b都作为函数的输入参数参与运算的，可见你想让神经网络运作起来，你是需要先初始化所有的神经网络参数的，那么究竟如何初始化呢？这还是门小学问，我们暂且假定是随机填了些的参数进来。虽然输入进来的参数 b 的shape是 <code>(M,)</code>，但在numpy中，两个array的”+”相加，是完全等价于<code>np.add()</code>函数(详情可help该函数)，这里体现了numpy的<strong>Broadcasting机制</strong>：(详情可查看 <a href="http://blog.csdn.net/hongxingabc/article/details/53149655" target="_blank" rel="noopener">Python库numpy中的Broadcasting机制解析</a>)</p>
<blockquote>
<p>简单的说，对两个阵进行操作时，NumPy逐元素地比较他们的形状。只有两种情况下Numpy会认为两个矩阵内的两个对应维度是兼容的：1. 它们相等； 2. 其中一个是1维。举个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; A      (<span class="number">4</span>d array):  <span class="number">8</span> x <span class="number">1</span> x <span class="number">6</span> x <span class="number">1</span></span><br><span class="line">&gt; B      (<span class="number">3</span>d array):      <span class="number">7</span> x <span class="number">1</span> x <span class="number">5</span></span><br><span class="line">&gt; Result (<span class="number">4</span>d array):  <span class="number">8</span> x <span class="number">7</span> x <span class="number">6</span> x <span class="number">5</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
<p>当任何一个维度是1，那么另一个不为1的维度将被用作最终结果的维度。也就是说，尺寸为1的维度将延展或“逐个复制”到与另一个维度匹配。<br><br><br>所以，<strong>代码中的偏置b，其shape为(M,)，其实它表明是一个$1\times M$的行向量，面对另一个$N\times M$的矩阵，b 便遇强则强的在弱势维度上(纵向)被延展成了一个$N\times M$的矩阵</strong>：<br>$$<br>\Big\downarrow\left.\begin{bmatrix}<br>\cdots &amp;b &amp; \cdots \<br>\cdots  &amp;  b&amp; \cdots \<br>\cdots  &amp;  b&amp;  \cdots<br>\end{bmatrix}\right} N行<br>$$</p>
</blockquote>
<p>虽然，我们说清楚了 <code>affine_forward(x,w,b)</code> 函数的故事，但要注意的是，在前向传播中一层神经元要干的活还没完哦～ 在隐藏层中得到的得分结果还需要ReLU激活函数“刺激”一下才算结束。</p>
<p><br></p>
<p>于是我们再定义 <code>relu_forward(x) = (out, cache)</code> 函数来完成这一步，其Python代码就更简单了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_forward</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the forward pass for a layer of rectified linear units (ReLUs).</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Inputs, of any shape</span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output, of the same shape as x</span></span><br><span class="line"><span class="string">    - cache: x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    out = np.maximum(<span class="number">0</span>, x)   <span class="comment"># 取x中每个元素和0做比较</span></span><br><span class="line">    cache = x				 <span class="comment"># 缓冲输入进来的x矩阵</span></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<p><strong>代码详解：</strong></p>
<p><br></p>
<p>我们可以注意到，<code>np.maximum()</code> 函数中的接受的两个参数一样用到了刚刚详解过的Broadcasting机制：前一个参数是只有一个维度的数值0，被延展成了一个和矩阵x同样shape的矩阵，然后在对应元素上比大小(相当于矩阵x的所有元素中把比0小的元素都替换成0)，取较大元素填在新的同shape形的矩阵out中。</p>
<p><br></p>
<p>那么，终于到最后了。一个隐藏层层神经元们在前向传播中究竟做了什么呢？那就是下面定义的 <code>affine_relu_forward(x, w, b) = (out, cache)</code> 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_relu_forward</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Convenience layer that perorms an affine transform followed by a ReLU</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input to the affine layer</span></span><br><span class="line"><span class="string">    - w, b: Weights for the affine layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output from the ReLU</span></span><br><span class="line"><span class="string">    - cache: Object to give to the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    a, fc_cache = affine_forward(x, w, b) <span class="comment"># 线性模型</span></span><br><span class="line">    out, relu_cache = relu_forward(a)	<span class="comment"># 激活函数</span></span><br><span class="line">    cache = (fc_cache, relu_cache)  <span class="comment"># 缓冲的是元组：(x, w, b, (a))</span></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<p>这里还是要留个心眼，对于输出层的神经元们来说，他们只需要用 <code>affine_forward(x, w, b)</code> 函数给出得分即可，无需再被”激活”。</p>
<p><br></p>
<h2 id="前向传播手绘图"><a href="#前向传播手绘图" class="headerlink" title="前向传播手绘图"></a>前向传播手绘图</h2><p><strong>小结一下：</strong></p>
<p><br></p>
<p>我们手绘一张图来说清楚，一隐藏层神经元们在前向传播的 <code>affine_relu_forward()</code> 函数中，数据变量在模块代码中究竟是如何流动的：</p>
<p><img src="https://i.loli.net/2018/02/10/5a7eb94ccec10.png" alt=""></p>
<p><br></p>
<p>（接下文：<a href="https://iphysresearch.github.io/2018/02/cs231n_MLP3/">一段关于神经网络的故事：传说中的反向传播</a>）</p>
<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">全连接层中的神经元与其前后两层的神经元是完全成对连接的，但是在同一个全连接层内的神经元之间没有连接。<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">0.2<em>56 -0.5</em>231 +0.1<em>24 +2.0</em>2 +1.1 = -96.8<a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div></section>
    <!-- Tags START -->
    
      <div class="tags">
        <span>Tags:</span>
        
  <a href="/tags#漫游CS231n" >
    <span class="tag-code">漫游CS231n</span>
  </a>

      </div>
    
    <!-- Tags END -->
    <!-- NAV START -->
    
  <div class="nav-container">
    <!-- reverse left and right to put prev and next in a more logic postition -->
    
      <a class="nav-left" href="/2018/02/cs231n_MLP1/">
        <span class="nav-arrow">← </span>
        
          一段关于神经网络的故事：引言
        
      </a>
    
    
      <a class="nav-right" href="/2018/02/cs231n_MLP3/">
        
          一段关于神经网络的故事：传说中的反向传播
        
        <span class="nav-arrow"> →</span>
      </a>
    
  </div>

    <!-- NAV END -->
    <!-- 打赏 START -->
    
      <div class="money-like">
        <div class="reward-btn">
          赏
          <span class="money-code">
            <span class="alipay-code">
              <div class="code-image"></div>
              <b>使用支付宝打赏</b>
            </span>
            <span class="wechat-code">
              <div class="code-image"></div>
              <b>使用微信打赏</b>
            </span>
          </span>
        </div>
        <p class="notice">若你觉得我的文章对你有帮助，欢迎点击上方按钮对我打赏</p>
      </div>
    
    <!-- 打赏 END -->
    <!-- 二维码 START -->
    
      <div class="qrcode">
        <canvas id="share-qrcode"></canvas>
        <p class="notice">扫描二维码，分享此文章</p>
      </div>
    
    <!-- 二维码 END -->
    
      <!-- Gitment START -->
      <div id="comments"></div>
      <!-- Gitment END -->
    
  </article>
  <!-- Article END -->
  <!-- Catalog START -->
  
    <aside class="catalog-container">
  <div class="toc-main">
    <strong class="toc-title">Catalog</strong>
    
      <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#所谓的前向传播"><span class="toc-nav-text">所谓的前向传播</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#一个神经元的本事"><span class="toc-nav-text">一个神经元的本事</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#一张图片-v-s-一个神经元"><span class="toc-nav-text">一张图片 v.s 一个神经元</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#多张图片-v-s-一个神经元"><span class="toc-nav-text">多张图片 v.s 一个神经元</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#强大的层状神经元"><span class="toc-nav-text">强大的层状神经元</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#一张图片-v-s-多个神经元"><span class="toc-nav-text">一张图片 v.s 多个神经元</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#多张图片-v-s-多个神经元"><span class="toc-nav-text">多张图片 v.s 多个神经元</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#不废话了，看代码！"><span class="toc-nav-text">不废话了，看代码！</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#前向传播手绘图"><span class="toc-nav-text">前向传播手绘图</span></a></li></ol></li></ol>
    
  </div>
</aside>
  
  <!-- Catalog END -->
</main>

<script>
  (function () {
    var url = 'http://iphysresearch.github.io/2018/02/cs231n_MLP2/';
    var banner = 'https://i.loli.net/2018/02/03/5a74d126a2aa9.png'
    if (banner !== '' && banner !== 'undefined' && banner !== 'null') {
      $('#article-banner').css({
        'background-image': 'url(' + banner + ')'
      })
    } else {
      $('#article-banner').geopattern(url)
    }
    $('.header').removeClass('fixed-header')

     // error image
    $(".markdown-content img").on('error', function() {
      $(this).attr('src', 'http://file.muyutech.com/error-img.png')
      $(this).css({
        'cursor': 'default'
      })
    })

    // zoom image
    $(".markdown-content img").on('click', function() {
      var src = $(this).attr('src')
      if (src !== 'http://file.muyutech.com/error-img.png') {
        var imageW = $(this).width()
        var imageH = $(this).height()
        
        var zoom = ($(window).width() * 0.95 / imageW).toFixed(2)
        zoom = zoom < 1 ? 1 : zoom
        zoom = zoom > 2 ? 2 : zoom
        var transY = (($(window).height() - imageH) / 2).toFixed(2)

        $('body').append('<div class="image-view-wrap"><div class="image-view-inner"><img src="'+ src +'" /></div></div>')
        $('.image-view-wrap').addClass('wrap-active')
        $('.image-view-wrap img').css({
          'width': `${imageW}`,
          'transform': `translate3d(0, ${transY}px, 0) scale3d(${zoom}, ${zoom}, 1)`
        })
        $('html').css('overflow', 'hidden')

        $('.image-view-wrap').on('click', function() {
          $(this).remove()
          $('html').attr('style', '')
        })
      }
    })

    // qrcode
    var qr = new QRious({
      element: document.getElementById('share-qrcode'),
      value: document.location.href
    });

    // gitment
    var gitmentConfig = "iphysresearch";
    if (gitmentConfig !== 'undefined') {
      var gitment = new Gitment({
        id: "一段关于神经网络的故事：所谓的前向传播",
        owner: "iphysresearch",
        repo: "iphysresearch.github.io",
        oauth: {
          client_id: "6b978dc207dc30e58ec8",
          client_secret: "2bc56895d0221e8c27ab87b072f8f18523231e22"
        },
        theme: {
          render(state, instance) {
            const container = document.createElement('div')
            container.lang = "en-US"
            container.className = 'gitment-container gitment-root-container'
            container.appendChild(instance.renderHeader(state, instance))
            container.appendChild(instance.renderEditor(state, instance))
            container.appendChild(instance.renderComments(state, instance))
            container.appendChild(instance.renderFooter(state, instance))
            return container;
          }
        }
      })
      gitment.render(document.getElementById('comments'))
    }
  })();
</script>

    <div class="scroll-top">
  <span class="arrow-icon"></span>
</div>
    <footer class="app-footer">
  <p class="copyright">
    &copy; 2018 | Proudly powered by <a href="https://hexo.io" target="_blank">Hexo</a>
    <br>
    Theme by <a href="https://github.com/yanm1ng">yanm1ng</a>
  </p>
</footer>

<script>
  function async(u, c) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }
</script>
<script>
  async("//cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
    FastClick.attach(document.body);
  })
</script>

<script>
  var hasLine = 'true';
  async("//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js", function(){
    $('figure pre').each(function(i, block) {
      var figure = $(this).parents('figure');
      if (hasLine === 'false') {
        figure.find('.gutter').hide();
      }
      var lang = figure.attr('class').split(' ')[1] || 'code';
      var codeHtml = $(this).html();
      var codeTag = document.createElement('code');
      codeTag.className = lang;
      codeTag.innerHTML = codeHtml;
      $(this).attr('class', '').empty().html(codeTag);
      figure.attr('data-lang', lang.toUpperCase());
      hljs.highlightBlock(block);
    });
  })
</script>
<!-- Baidu Tongji -->

<script src="/js/script.js"></script>
  </body>
</html>