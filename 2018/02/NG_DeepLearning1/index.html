<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
  <meta name="description" content="Machine Learning, Deep Learning, Physics">
  <meta name="keyword" content="hexo-theme, vuejs">
  
    <link rel="shortcut icon" href="/css/images/logo.png">
  
  <title>
    
      品读 A. Ng 的 DeepLearning.ai 之“神经网络和深度学习” | Teaching is Learning
    
  </title>
  <link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.css" rel="stylesheet">
  <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/tomorrow.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/plugins/gitment.css">
  <script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
  <script src="//cdn.bootcss.com/geopattern/1.2.3/js/geopattern.min.js"></script>
  <script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>
  <script src="/js/qrious.js"></script>
<script src="/js/gitment.js"></script>
  
  
    <!-- MathJax support START -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- MathJax support END -->
  


</head>
<div class="wechat-share">
  <img src="/css/images/logo.png" />
</div>

  <body>
    <header class="header fixed-header">
  <div class="header-container">
    <a class="home-link" href="/">
      <div class="logo"></div>
      <span>Teaching is Learning</span>
    </a>
    <ul class="right-list">
      
        <li class="list-item">
          
            <a href="/" class="item-link">Home</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/tags/" class="item-link">Tags</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/archives/" class="item-link">Archives</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/project/" class="item-link">Projects</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/about/" class="item-link">About</a>
          
        </li>
      
    </ul>
    <div class="menu">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </div>
    <div class="menu-mask">
      <ul class="menu-list">
        
          <li class="menu-item">
            
              <a href="/" class="menu-link">Home</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/tags/" class="menu-link">Tags</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/archives/" class="menu-link">Archives</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/project/" class="menu-link">Projects</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/about/" class="menu-link">About</a>
            
          </li>
        
      </ul>
    </div>
  </div>
</header>

    <div id="article-banner">
  <h2>品读 A. Ng 的 DeepLearning.ai 之“神经网络和深度学习”</h2>
  <p class="post-date">2018-02-15</p>
  <div class="arrow-down">
    <a href="javascript:;"></a>
  </div>
</div>
<main class="app-body flex-box">
  <!-- Article START -->
  <article class="post-article">
    <section class="markdown-content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>此文是吴恩达 DeepLearning.ai 课程第一课——神经网络和深度学习部分的自己讲给自己的讲稿。</p>
<p><br></p>
<p>内含自己的学习历程和从其他信息渠道获取的归纳与总结，并不会细致的罗列所有内容，仅摘取对个人有一定价值的信息。</p>
<p><br></p>
<p>待续中。。。。</p>
<a id="more"></a>
<hr>
<p><br></p>
<p>黄兄组织整理了Ng的 <a href="http://www.ai-start.com/dl2017/" target="_blank" rel="noopener">深度学习（DeepLearning.ai）学习笔记</a> 是最全面的中文文本讲义资料。我这里将全面借鉴和引用其中内容和信息，不过此文背后更大程度上记录的是我个人的学习历程和从其他信息渠道获取的归纳与总结，以自述的方式“教自己”。</p>
<p><br></p>
<p>相关官方资源：<a href="http://mooc.study.163.com/learn/2001281002?tid=2001392029#/learn/content" target="_blank" rel="noopener">网易云微专业</a>、<a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="noopener">Coursera</a></p>
<p><br></p>
<h1 id="1-深度学习引言"><a href="#1-深度学习引言" class="headerlink" title="1. 深度学习引言"></a>1. 深度学习引言</h1><p><br></p>
<h2 id="1-1-1-2-欢迎-amp-什么是神经网络？"><a href="#1-1-1-2-欢迎-amp-什么是神经网络？" class="headerlink" title="1.1-1.2 欢迎 &amp; 什么是神经网络？"></a>1.1-1.2 欢迎 &amp; 什么是神经网络？</h2><p>Ng 对 AI 的形象理解：</p>
<blockquote>
<p>我认为AI是最新的电力，大约在一百年前，我们社会的电气化改变了每个主要行业，从交通运输行业到制造业、医疗保健、通讯等方面，我认为如今我们见到了<strong>AI</strong>明显的令人惊讶的能量，带来了同样巨大的转变。显然，AI的各个分支中，发展的最为迅速的就是深度学习。</p>
</blockquote>
<p>这个理解基本等价于是把 AI 技术的崛起等价于了第四次工业革命，不过究竟是否真的是一场给人类带来巨大变革的革命，还需要拭目以待。</p>
<p><br></p>
<p>总共有五门课：</p>
<ul>
<li><strong>神经网络和深度学习</strong>：学习神经网络的基础；用深度神经网络辨认猫的项目；</li>
<li><strong>改善深层神经网络：超参数调试、正则化以及优化</strong>：学习严密地构建神经网络，如何真正让它表现良好；</li>
<li><strong>结构化机器学习项目</strong>：让端对端的深度学习系统工作的更好；</li>
<li><strong>卷积神经网络</strong>：它经常被用于图像领域；</li>
<li><strong>序列模型</strong>：构建自然语言、音频以及其他序列数据的模型；</li>
</ul>
<p>本文对应的是第一门课，即所谓“神经网络和深度学习”。源于 Ng 在著名的 Coursera 上的一门网络公开课，现今国内的网易云课堂已经将课程完整引入，可以免费观看到全部视频课程和讲义，不过并不包括编程作业和解答。</p>
<p><br></p>
<p>Ng 对于什么是神经网络这个问题的引入可谓非常浅显。用一个房价预测的回归例子不仅引入了一个神经元的行为（突出强调数据输入与输出），还顺便把 <strong>ReLU</strong> 非线性函数也顺带安利了一下。</p>
<p><img src="https://i.loli.net/2018/02/16/5a86e16695305.png" alt=""></p>
<p><strong>ReLU</strong> 激活函数如今可谓是相当常用且非常强大的。它的全称是<strong>Rectified Linear Unit</strong>。rectify（修正）可以理解成 $\max(0,x)$。</p>
<p><br></p>
<p>接着将单个神经元拓展成多个神经元的结构，继续用房价作为例子，不过是更多的特征作为”输入层”。</p>
<p><img src="https://i.loli.net/2018/02/16/5a86e2446d721.png" alt=""></p>
<p>在图上每一个画的小圆圈都可以是<strong>ReLU</strong>的一部分，也就是指修正线性单元，或者其它稍微非线性的函数。下面的图，用一个标准且典型的单隐层神经网络来揭开DDL的庐山真面目：</p>
<p><img src="https://i.loli.net/2018/02/16/5a86e31d589fa.png" alt=""></p>
<p>这是一个全连接的神经网络，中间隐层中每一个神经元都与全部四个特征相连接，训练好后的三个节点神经元将会有各自的内禀含义，比如第一个节点训练好后发现它赋予了 $x_1,x_2$ 更大的权重，那么这个节点可能就代表着“家庭人口”这个含义。最终输出层给出一个标量值，来表示在当前特征参数下的房价预测值。</p>
<p><br></p>
<p>总之，神经网络非常擅长计算从$x$到$y$的精准映射函数。那么训练是如何保证给出的房价贴近于真实房价呢？这就是监督学习最擅长的事情了，要知道，神经网络在监督学习的环境下也是非常的有效和强大的。</p>
<p><br></p>
<h2 id="1-3-神经网络的监督学习"><a href="#1-3-神经网络的监督学习" class="headerlink" title="1.3 神经网络的监督学习"></a>1.3 神经网络的监督学习</h2><p>监督学习说白了就是给你一本带答案的习题集，你知道自己学的知识究竟对不对，肯定要看一眼答案，然后回过头来做比对。相对应的，木有课后答案的习题集就是非监督学习了。Ng 在课程中有言：</p>
<ul>
<li>事实表明，到目前几乎所有由神经网络创造的经济价值，本质上都离不开一种叫做<strong>监督学习</strong>的机器学习类别。尤其是广告商！人家能够预测你会点击哪个广告，都是基于用户的信息和过去线上的操作行为，这些就相当于是”答案“，进而机器学会了后，再对新题目给出的新答案就相当于是对用户对广告的点击行为预测了。</li>
</ul>
<p>Ng 将结构化数据和非结构化数据专门讨论了一番。这其实挺重要的，因为对于我们数据的类型进行分类后可以很好的为我们模型的构建和理解带来参考。</p>
<p><img src="https://i.loli.net/2018/02/16/5a86edbd4e607.png" alt=""></p>
<p>通常来说，传统的机器学习仅适用于结构化数据，对非结构化数据表现非常差。Ng 指出：</p>
<ul>
<li>神经网络算法对于<strong>结构化</strong>和<strong>非结构化数据</strong>都有用处。</li>
</ul>
<p>要我来说，神经网络算法不仅几乎完美的解决了非结构化数据的模型构建，还非常完美的实现了结构化数据的自动化特征工程技术。</p>
<p><br></p>
<h2 id="1-4-1-6-为什么深度学习会兴起？-amp-关于本课程-amp-课程资源"><a href="#1-4-1-6-为什么深度学习会兴起？-amp-关于本课程-amp-课程资源" class="headerlink" title="1.4-1.6 为什么深度学习会兴起？&amp; 关于本课程 &amp; 课程资源"></a>1.4-1.6 为什么深度学习会兴起？&amp; 关于本课程 &amp; 课程资源</h2><p>关于为啥深度学习火了呢？这是一个很多人都谈的问题，Ng 也没法回避它。 我来一语中的先从学术上说，就是因为2007年 Hinton 等人发的一篇重要文章开始了深度学习领域的篇章，但整个学术界真正开始广泛重视起 DL 是因为2012年一场数据科学竞赛的结果。而 DL 以及带动的整个机器学习/人工智能/数据分析等领域在全世界范围内如此之火，是因为 Google 家里的那只狗（AlphaGo）。</p>
<p><br></p>
<p>Ng 是从技术发展的本质角度来谈的，非常重要的三要素得到了史无前例的提升和进步：</p>
<p><img src="https://i.loli.net/2018/02/16/5a86f43a9963a.png" alt=""></p>
<ul>
<li>Data （大数据啊！深度学习本质上是数据驱动的啊！）</li>
<li>Computation（英伟达啊！黄教主啊！GPU 啊！）</li>
<li>Algorithms（算法肯定进步了啊！不然你我还来学这个干啥？）</li>
</ul>
<p>下面的图记得是 Google 最早给出的吧，毕竟人家有大量的资源做这种测评估计。图中描述的是不同数据量尺度下的传统机器学习和深度学习之间的差异关系。</p>
<p><img src="https://i.loli.net/2018/02/15/5a8506ba4030f.png" alt=""></p>
<p>Ng 指出：</p>
<blockquote>
<p>事实上，如今在神经网络上最可靠的方法来获得更好的性能，往往就是<strong>要么训练一个更大的神经网络，要么投入更多的数据</strong>，但这只能在一定程度上起作用，因为最终你可能会耗尽了数据，或者最终你的网络是如此的大规模而导致将要用太久的时间去训练，但是仅仅提升规模的的确确地让我们在深度学习的世界中摸索了很多时间。</p>
</blockquote>
<p>Ng 确实是学术出身，在讲解这个图的过程中描述的很严谨，强调了该图的描述其实有很多定义和理解是含糊的，大趋势确实是如图所示，不过其中很多细节还不敢一概而论。</p>
<h1 id="2-神经网络的编程基础"><a href="#2-神经网络的编程基础" class="headerlink" title="2. 神经网络的编程基础"></a>2. 神经网络的编程基础</h1><p>建设中。。。。</p>
<h3 id="2-1-二分类"><a href="#2-1-二分类" class="headerlink" title="2.1 二分类"></a>2.1 二分类</h3><p>这周我们将学习神经网络的基础知识，其中需要注意的是，当实现一个神经网络的时候，我们需要知道一些非常重要的技术和技巧。例如有一个包含个样本的训练集，你很可能习惯于用一个<strong>for</strong>循环来遍历训练集中的每个样本，但是当实现一个神经网络的时候，我们通常不直接使用<strong>for</strong>循环来遍历整个训练集，所以在这周的课程中你将学会如何处理训练集。</p>
<p>另外在神经网络的计算中，通常先有一个叫做前向暂停(<strong>forward pause</strong>)或叫做前向传播(<strong>foward propagation</strong>)的步骤，接着有一个叫做反向暂停(<strong>backward pause</strong>) 或叫做反向传播<strong>(backward propagation</strong>)的步骤。所以这周我也会向你介绍为什么神经网络的训练过程可以分为前向传播和反向传播两个独立的部分。</p>
<p>在课程中我将使用逻辑回归(l<strong>ogistic regression</strong>)来传达这些想法，以使大家能够更加容易地理解这些概念。即使你之前了解过逻辑回归，我认为这里还是有些新的、有趣的东西等着你去发现和了解，所以现在开始进入正题。</p>
<p>逻辑回归是一个用于二分类(<strong>binary classification</strong>)的算法。首先我们从一个问题开始说起，这里有一个二分类问题的例子，假如你有一张图片作为输入，比如这只猫，如果识别这张图片为猫，则输出标签1作为结果；如果识别出不是猫，那么输出标签0作为结果。现在我们可以用字母 来 表示输出的结果标签，如下图所示：</p>
<p><img src="http://www.ai-start.com/dl2017/images/269118812ea785aee00f6ffc11b5c882.png" alt="img"></p>
<p>我们来看看一张图片在计算机中是如何表示的，为了保存一张图片，需要保存三个矩阵，它们分别对应图片中的红、绿、蓝三种颜色通道，如果你的图片大小为64x64像素，那么你就有三个规模为64x64的矩阵，分别对应图片中红、绿、蓝三种像素的强度值。为了便于表示，这里我画了三个很小的矩阵，注意它们的规模为5x4 而不是64x64，如下图所示：</p>
<p><img src="http://www.ai-start.com/dl2017/images/1e664a86fa2014d5212bcb88f1c419cf.png" alt="img"></p>
<p>为了把这些像素值放到一个特征向量中，我们需要把这些像素值提取出来，然后放入一个特征向量。为了把这些像素值转换为特征向量 ，我们需要像下面这样定义一个特征向量  来表示这张图片，我们把所有的像素都取出来，例如255、231等等，直到取完所有的红色像素，接着最后是255、134、…、255、134等等，直到得到一个特征向量，把图片中所有的红、绿、蓝像素值都列出来。如果图片的大小为64x64像素，那么向量  的总维度，将是64乘以64乘以3，这是三个像素矩阵中像素的总量。在这个例子中结果为12,288。现在我们用，来表示输入特征向量的维度，有时候为了简洁，我会直接用小写的来表示输入特征向量的维度。所以在二分类问题中，我们的目标就是习得一个分类器，它以图片的特征向量作为输入，然后预测输出结果为1还是0，也就是预测图片中是否有猫：</p>
<p><img src="http://www.ai-start.com/dl2017/images/e173fd42de5f1953deb617623d5087e8.png" alt="img"></p>
<p>接下来我们说明一些在余下课程中，需要用到的一些符号。</p>
<p><strong>符号定义</strong> ：</p>
<p>：表示一个维数据，为输入数据，维度为； </p>
<p>：表示输出结果，取值为；</p>
<p>：表示第组数据，可能是训练数据，也可能是测试数据，此处默认为训练数据； </p>
<p>：表示所有的训练数据集的输入值，放在一个 的矩阵中，其中表示样本数目; </p>
<p>：对应表示所有训练数据集的输出值，维度为。</p>
<p>用一对来表示一个单独的样本，代表维的特征向量， 表示标签(输出结果)只能为0或1。 而训练集将由个训练样本组成，其中表示第一个样本的输入和输出，表示第二个样本的输入和输出，直到最后一个样本，然后所有的这些一起表示整个训练集。有时候为了强调这是训练样本的个数，会写作，当涉及到测试集的时候，我们会使用来表示测试集的样本数，所以这是测试集的样本数：</p>
<p><img src="http://www.ai-start.com/dl2017/images/12f602ed40ba90540112ae0fee77fadf.png" alt="img"></p>
<p>最后为了能把训练集表示得更紧凑一点，我们会定义一个矩阵用大写的表示，它由输入向量、等组成，如下图放在矩阵的列中，所以现在我们把作为第一列放在矩阵中，作为第二列，放到第列，然后我们就得到了训练集矩阵。所以这个矩阵有列，是训练集的样本数量，然后这个矩阵的高度记为，注意有时候可能因为其他某些原因，矩阵会由训练样本按照行堆叠起来而不是列，如下图所示：的转置直到的转置，但是在实现神经网络的时候，使用左边的这种形式，会让整个实现的过程变得更加简单：</p>
<p><img src="http://www.ai-start.com/dl2017/images/1661e545ce5fd2c27b15444d5b69ec78.png" alt="img"></p>
<p>现在来简单温习一下:是一个规模为乘以的矩阵，当你用<strong>Python</strong>实现的时候，你会看到<code>X.shape</code>，这是一条<strong>Python</strong>命令，用于显示矩阵的规模，即<code>X.shape</code>等于，是一个规模为乘以的矩阵。所以综上所述，这就是如何将训练样本（输入向量的集合）表示为一个矩阵。</p>
<p>那么输出标签呢？同样的道理，为了能更加容易地实现一个神经网络，将标签放在列中将会使得后续计算非常方便，所以我们定义大写的等于，所以在这里是一个规模为1乘以的矩阵，同样地使用<strong>Python</strong>将表示为<code>Y.shape</code>等于，表示这是一个规模为1乘以的矩阵。</p>
<p><img src="http://www.ai-start.com/dl2017/images/55345ba411053da11ff843bbb3406369.png" alt="img"></p>
<p>当你在后面的课程中实现神经网络的时候，你会发现，一个好的符号约定能够将不同训练样本的数据很好地组织起来。而我所说的数据不仅包括  或者  还包括之后你会看到的其他的量。将不同的训练样本的数据提取出来，然后就像刚刚我们对  或者  所做的那样，将他们堆叠在矩阵的列中，形成我们之后会在逻辑回归和神经网络上要用到的符号表示。如果有时候你忘了这些符号的意思，比如什么是 ，或者什么是 ，或者忘了其他一些东西，我们也会在课程的网站上放上符号说明，然后你可以快速地查阅每个具体的符号代表什么意思，好了，我们接着到下一个视频，在下个视频中，我们将以逻辑回归作为开始。 备注：附录里也写了符号说明。</p>
<h3 id="2-2-逻辑回归-Logistic-Regression"><a href="#2-2-逻辑回归-Logistic-Regression" class="headerlink" title="2.2 逻辑回归(Logistic Regression)"></a>2.2 逻辑回归(Logistic Regression)</h3><p>在这个视频中，我们会重温逻辑回归学习算法，该算法适用于二分类问题，本节将主要介绍逻辑回归的<strong>Hypothesis Function</strong>（假设函数）。</p>
<p>对于二元分类问题来讲，给定一个输入特征向量，它可能对应一张图片，你想识别这张图片识别看它是否是一只猫或者不是一只猫的图片，你想要一个算法能够输出预测，你只能称之为，也就是你对实际值  的估计。更正式地来说，你想让  表示  等于1的一种可能性或者是机会，前提条件是给定了输入特征。换句话来说，如果是我们在上个视频看到的图片，你想让  来告诉你这是一只猫的图片的机率有多大。在之前的视频中所说的，是一个维的向量（相当于有个特征的特征向量）。我们用来表示逻辑回归的参数，这也是一个维向量（因为实际上是特征权重，维度与特征向量相同），参数里面还有，这是一个实数（表示偏差）。所以给出输入以及参数和之后，我们怎样产生输出预测值，一件你可以尝试却不可行的事是让。</p>
<p><img src="http://www.ai-start.com/dl2017/images/dfb5731c30b81eced917450d31e860a3.png" alt="img"></p>
<p>这时候我们得到的是一个关于输入的线性函数，实际上这是你在做线性回归时所用到的，但是这对于二元分类问题来讲不是一个非常好的算法，因为你想让表示实际值等于1的机率的话， 应该在0到1之间。这是一个需要解决的问题，因为可能比1要大得多，或者甚至为一个负值。对于你想要的在0和1之间的概率来说它是没有意义的，因此在逻辑回归中，我们的输出应该是等于由上面得到的线性函数式子作为自变量的<strong>sigmoid</strong>函数中，公式如上图最下面所示，将线性函数转换为非线性函数。</p>
<p>下图是<strong>sigmoid</strong>函数的图像，如果我把水平轴作为轴，那么关于的<strong>sigmoid</strong>函数是这样的，它是平滑地从0走向1，让我在这里标记纵轴，这是0，曲线与纵轴相交的截距是0.5，这就是关于的<strong>sigmoid</strong>函数的图像。我们通常都使用来表示的值。</p>
<p><img src="http://www.ai-start.com/dl2017/images/7e304debcca5945a3443d56bcbdd2964.png" alt="img"></p>
<p>关于<strong>sigmoid</strong>函数的公式是这样的，,在这里是一个实数，这里要说明一些要注意的事情，如果非常大那么将会接近于0，关于的<strong>sigmoid</strong>函数将会近似等于1除以1加上某个非常接近于0的项，因为 的指数如果是个绝对值很大的负数的话，这项将会接近于0，所以如果很大的话那么关于的<strong>sigmoid</strong>函数会非常接近1。相反地，如果非常小或者说是一个绝对值很大的负数，那么关于这项会变成一个很大的数，你可以认为这是1除以1加上一个非常非常大的数，所以这个就接近于0。实际上你看到当变成一个绝对值很大的负数，关于的<strong>sigmoid</strong>函数就会非常接近于0，因此当你实现逻辑回归时，你的工作就是去让机器学习参数以及这样才使得成为对这一情况的概率的一个很好的估计。</p>
<p><img src="http://www.ai-start.com/dl2017/images/f5049dc7ce815b495fbbdf71f23fc66c.png" alt="img"></p>
<p>在继续进行下一步之前，介绍一种符号惯例，可以让参数和参数分开。在符号上要注意的一点是当我们对神经网络进行编程时经常会让参数和参数分开，在这里参数对应的是一种偏置。在之前的机器学习课程里，你可能已经见过处理这个问题时的其他符号表示。比如在某些例子里，你定义一个额外的特征称之为，并且使它等于1，那么现在就是一个加1维的变量，然后你定义的<strong>sigmoid</strong>函数。在这个备选的符号惯例里，你有一个参数向量，这样就充当了，这是一个实数，而剩下的 直到充当了，结果就是当你实现你的神经网络时，有一个比较简单的方法是保持和分开。但是在这节课里我们不会使用任何这类符号惯例，所以不用去担心。 现在你已经知道逻辑回归模型是什么样子了，下一步要做的是训练参数和参数，你需要定义一个代价函数，让我们在下节课里对其进行解释。</p>
<h3 id="2-3-逻辑回归的代价函数（Logistic-Regression-Cost-Function）"><a href="#2-3-逻辑回归的代价函数（Logistic-Regression-Cost-Function）" class="headerlink" title="2.3 逻辑回归的代价函数（Logistic Regression Cost Function）"></a>2.3 逻辑回归的代价函数（Logistic Regression Cost Function）</h3><p>在上个视频中，我们讲了逻辑回归模型，这个视频里，我们讲逻辑回归的代价函数（也翻译作成本函数）。</p>
<p><strong>为什么需要代价函数：</strong></p>
<p>为了训练逻辑回归模型的参数参数和参数我们，需要一个代价函数，通过训练代价函数来得到参数和参数。先看一下逻辑回归的输出函数：</p>
<p><img src="http://www.ai-start.com/dl2017/images/4c9a27b071ce9162dbbcdad3393061d2.png" alt="1"></p>
<p>为了让模型通过学习调整参数，你需要给予一个样本的训练集，这会让你在训练集上找到参数和参数,，来得到你的输出。</p>
<p>对训练集的预测值，我们将它写成，我们更希望它会接近于训练集中的值，为了对上面的公式更详细的介绍，我们需要说明上面的定义是对一个训练样本来说的，这种形式也使用于每个训练样本，我们使用这些带有圆括号的上标来区分索引和样本，训练样本所对应的预测值是,是用训练样本的然后通过<strong>sigmoid</strong>函数来得到，也可以把定义为,我们将使用这个符号注解，上标来指明数据表示或者或者或者其他数据的第个训练样本，这就是上标的含义。</p>
<p><strong>损失函数：</strong></p>
<p>损失函数又叫做误差函数，用来衡量算法的运行情况，<strong>Loss function:.</strong></p>
<p>我们通过这个称为的损失函数，来衡量预测输出值和实际值有多接近。一般我们用预测值和实际值的平方差或者它们平方差的一半，但是通常在逻辑回归中我们不这么做，因为当我们在学习逻辑回归参数的时候，会发现我们的优化目标不是凸优化，只能找到多个局部最优值，梯度下降法很可能找不到全局最优值，虽然平方差是一个不错的损失函数，但是我们在逻辑回归模型中会定义另外一个损失函数。</p>
<p>我们在逻辑回归中用到的损失函数是：</p>
<p>为什么要用这个函数作为逻辑损失函数？当我们使用平方误差作为损失函数的时候，你会想要让这个误差尽可能地小，对于这个逻辑回归损失函数，我们也想让它尽可能地小，为了更好地理解这个损失函数怎么起作用，我们举两个例子：</p>
<p>当时损失函数，如果想要损失函数尽可能得小，那么就要尽可能大，因为<strong>sigmoid</strong>函数取值，所以会无限接近于1。</p>
<p>当时损失函数，如果想要损失函数尽可能得小，那么就要尽可能小，因为<strong>sigmoid</strong>函数取值，所以会无限接近于0。</p>
<p><strong>在这门课中有很多的函数效果和现在这个类似，就是如果等于1，我们就尽可能让变大，如果等于0，我们就尽可能让  变小。</strong> 损失函数是在单个训练样本中定义的，它衡量的是算法在单个训练样本中表现如何，为了衡量算法在全部训练样本上的表现如何，我们需要定义一个算法的代价函数，算法的代价函数是对个样本的损失函数求和然后除以:  损失函数只适用于像这样的单个训练样本，而代价函数是参数的总代价，所以在训练逻辑回归模型时候，我们需要找到合适的和，来让代价函数  的总代价降到最低。 根据我们对逻辑回归算法的推导及对单个样本的损失函数的推导和针对算法所选用参数的总代价函数的推导，结果表明逻辑回归可以看做是一个非常小的神经网络，在下一个视频中，我们会看到神经网络会做什么。</p>
<h3 id="2-4-梯度下降法（Gradient-Descent）"><a href="#2-4-梯度下降法（Gradient-Descent）" class="headerlink" title="2.4 梯度下降法（Gradient Descent）"></a>2.4 梯度下降法（Gradient Descent）</h3><p><strong>梯度下降法可以做什么？</strong></p>
<p>在你测试集上，通过最小化代价函数（成本函数）来训练的参数和， </p>
<p><img src="http://www.ai-start.com/dl2017/images/cbd5ff8c461fcb5a699c4ec4789687b3.jpg" alt="img"></p>
<p>如图，在第二行给出和之前一样的逻辑回归算法的代价函数（成本函数）</p>
<p><strong>梯度下降法的形象化说明</strong></p>
<p><img src="http://www.ai-start.com/dl2017/images/a3c81d2c8629d674141def47dc02f312.jpg" alt="img"></p>
<p>在这个图中，横轴表示你的空间参数和，在实践中，可以是更高的维度，但是为了更好地绘图，我们定义和，都是单一实数，代价函数（成本函数）是在水平轴和上的曲面，因此曲面的高度就是在某一点的函数值。我们所做的就是找到使得代价函数（成本函数）函数值是最小值，对应的参数和。</p>
<p><img src="http://www.ai-start.com/dl2017/images/236774be30d12524a2002c3c484d22d5.jpg" alt="img"></p>
<p>如图，代价函数（成本函数）是一个凸函数(<strong>convex function</strong>)，像一个大碗一样。</p>
<p><img src="http://www.ai-start.com/dl2017/images/af11ecd5d72c85f777592f8660678ce6.jpg" alt="img"></p>
<p>如图，这就与刚才的图有些相反，因为它是非凸的并且有很多不同的局部最小值。由于逻辑回归的代价函数（成本函数）特性，我们必须定义代价函数（成本函数）为凸函数。 <strong>初始化和，</strong></p>
<p><img src="http://www.ai-start.com/dl2017/images/1b79cca8e1902f0ee24b4eb966755ddd.jpg" alt="img"></p>
<p>可以用如图那个小红点来初始化参数和，也可以采用随机初始化的方法，对于逻辑回归几乎所有的初始化方法都有效，因为函数是凸函数，无论在哪里初始化，应该达到同一点或大致相同的点。</p>
<p><img src="http://www.ai-start.com/dl2017/images/0ad6c298d0ac25ca9b26546bb06d462c.jpg" alt="img"></p>
<p>我们以如图的小红点的坐标来初始化参数和。</p>
<p><strong>2. 朝最陡的下坡方向走一步，不断地迭代</strong></p>
<p><img src="http://www.ai-start.com/dl2017/images/bb909b874b2865e66eaf9a5d18cc00e5.jpg" alt="img"></p>
<p>我们朝最陡的下坡方向走一步，如图，走到了如图中第二个小红点处。</p>
<p><img src="http://www.ai-start.com/dl2017/images/c5eda5608fd2f4d846559ed8e89ed33c.jpg" alt="img"></p>
<p>我们可能停在这里也有可能继续朝最陡的下坡方向再走一步，如图，经过两次迭代走到第三个小红点处。</p>
<p><strong>3.直到走到全局最优解或者接近全局最优解的地方</strong></p>
<p>通过以上的三个步骤我们可以找到全局最优解，也就是代价函数（成本函数）这个凸函数的最小值点。</p>
<p><strong>梯度下降法的细节化说明（仅有一个参数）</strong></p>
<p><img src="http://www.ai-start.com/dl2017/images/5300d40870ec58cb0b8162747b9559b9.jpg" alt="img"></p>
<p>假定代价函数（成本函数） 只有一个参数，即用一维曲线代替多维曲线，这样可以更好画出图像。</p>
<p><img src="http://www.ai-start.com/dl2017/images/6cdef1989a113fc1caaaaf6ebaaa3549.jpg" alt="img"></p>
<p><img src="http://www.ai-start.com/dl2017/images/60cc674531ac72b2d75b0c447db95e96.jpg" alt="img"></p>
<p>迭代就是不断重复做如图的公式:</p>
<p>表示更新参数,</p>
<p> 表示学习率（<strong>learning rate</strong>），用来控制步长（<strong>step</strong>），即向下走一步的长度 就是函数对 求导（<strong>derivative</strong>），在代码中我们会使用表示这个结果</p>
<p><img src="http://www.ai-start.com/dl2017/images/27be50001e7a91bd2abaaeaf7aba7cd4.jpg" alt="img"></p>
<p>对于导数更加形象化的理解就是斜率（<strong>slope</strong>），如图该点的导数就是这个点相切于 的小三角形的高除宽。假设我们以如图点为初始化点，该点处的斜率的符号是正的，即，所以接下来会向左走一步。</p>
<p><img src="http://www.ai-start.com/dl2017/images/4fb3b91114ecb2cd81ec9f3662434d81.jpg" alt="img"></p>
<p>整个梯度下降法的迭代过程就是不断地向左走，直至逼近最小值点。</p>
<p><img src="http://www.ai-start.com/dl2017/images/579fb3957063480420c6a7d294503e97.jpg" alt="img"></p>
<p>假设我们以如图点为初始化点，该点处的斜率的符号是负的，即，所以接下来会向右走一步。</p>
<p><img src="http://www.ai-start.com/dl2017/images/21541fc771ad8895c18d292dd4734fe7.jpg" alt="img"></p>
<p>整个梯度下降法的迭代过程就是不断地向右走，即朝着最小值点方向走。</p>
<p><strong>梯度下降法的细节化说明（两个参数）</strong></p>
<p>逻辑回归的代价函数（成本函数）是含有两个参数的。</p>
<p><img src="http://www.ai-start.com/dl2017/images/593eb7e2835b4f3c3aa8185cfa76155c.png" alt="img">  表示求偏导符号，可以读作round，  就是函数 对 求偏导，在代码中我们会使用 表示这个结果，  就是函数对 求偏导，在代码中我们会使用 表示这个结果， 小写字母 用在求导数（<strong>derivative</strong>），即函数只有一个参数， 偏导数符号 用在求偏导（<strong>partial derivative</strong>），即函数含有两个以上的参数。</p>
<h3 id="2-5-导数（Derivatives）"><a href="#2-5-导数（Derivatives）" class="headerlink" title="2.5 导数（Derivatives）"></a>2.5 导数（Derivatives）</h3><p>这个视频我主要是想帮你获得对微积分和导数直观的理解。或许你认为自从大学毕以后你再也没有接触微积分。这取决于你什么时候毕业，也许有一段时间了，如果你顾虑这点，请不要担心。为了高效应用神经网络和深度学习，你并不需要非常深入理解微积分。因此如果你观看这个视频或者以后的视频时心想：“哇哦，这些知识、这些运算对我来说很复杂。”我给你的建议是：坚持学习视频，最好下课后做作业，成功的完成编程作业，然后你就可以使用深度学习了。在第四周之后的学习中，你会看到定义的很多种类的函数，通过微积分他们能够帮助你把所有的知识结合起来，其中有的叫做前向函数和反向函数，因此你不需要了解所有你使用的那些微积分中的函数。所以你不用担心他们，除此之外在对深度学习的尝试中，这周我们要进一步深入了解微积分的细节。所有你只需要直观地认识微积分，用来构建和成功的应用这些算法。最后，如果你是精通微积分的那一小部分人群，你对微积分非常熟悉，你可以跳过这部分视频。其他同学让我们开始深入学习导数。</p>
<p><img src="http://www.ai-start.com/dl2017/images/efe674e73c693a0c5439b68652ed2ce1.png" alt="img"></p>
<p>一个函数，它是一条直线。下面我们来简单理解下导数。让我们看看函数中几个点，假定，那么是的3倍等于6，也就是说如果，那么函数。假定稍微改变一点点的值，只增加一点，变为2.001，这时将向右做微小的移动。0.001的差别实在是太小了，不能在图中显示出来，我们把它右移一点，现在等于的3倍是6.003，画在图里，比例不太符合。请看绿色高亮部分的这个小三角形，如果向右移动0.001，那么增加0.003，的值增加3倍于右移的，因此我们说函数在，.是这个导数的斜率，或者说，当时，斜率是3。导数这个概念意味着斜率，导数听起来是一个很可怕、很令人惊恐的词，但是斜率以一种很友好的方式来描述导数这个概念。所以提到导数，我们把它当作函数的斜率就好了。更正式的斜率定义为在上图这个绿色的小三角形中，高除以宽。即斜率等于0.003除以0.001，等于3。或者说导数等于3，这表示当你将右移0.001，的值增加3倍水平方向的量。</p>
<p>现在让我们从不同的角度理解这个函数。 假设 ，此时。 把右移一个很小的幅度，增加到5.001，。 即在 时，斜率是3，这就是表示，当微小改变变量的值， 。一个等价的导数表达式可以这样写 ，不管你是否将放在上面或者放在右边都没有关系。 在这个视频中，我讲解导数讨论的情况是我们将偏移0.001，如果你想知道导数的数学定义，导数是你右移很小的值（不是0.001，而是一个非常非常小的值）。通常导数的定义是你右移(可度量的值)一个无限小的值，增加3倍（增加了一个非常非常小的值）。也就是这个三角形右边的高度。</p>
<p><img src="http://www.ai-start.com/dl2017/images/beee332de6c608239d35a7a4f466594c.png" alt="img"></p>
<p>那就是导数的正式定义。但是为了直观的认识，我们将探讨右移 这个值，即使0.001并不是无穷小的可测数据。导数的一个特性是：这个函数任何地方的斜率总是等于3，不管或 ，这个函数的斜率总等于3，也就是说不管的值如何变化，如果你增加0.001，的值就增加3倍。这个函数在所有地方的斜率都相等。一种证明方式是无论你将小三角形画在哪里，它的高除以宽总是3。</p>
<p>我希望带给你一种感觉：什么是斜率？什么是导函数？对于一条直线，在例子中函数的斜率，在任何地方都是3。在下一个视频让我们看一个更复杂的例子，这个例子中函数在不同点的斜率是可变的。</p>
<h3 id="2-6-更多的导数例子（More-Derivative-Examples）"><a href="#2-6-更多的导数例子（More-Derivative-Examples）" class="headerlink" title="2.6 更多的导数例子（More Derivative Examples）"></a>2.6 更多的导数例子（More Derivative Examples）</h3><p>在这个视频中我将给出一个更加复杂的例子，在这个例子中，函数在不同点处的斜率是不一样的，先来举个例子:</p>
<p><img src="http://www.ai-start.com/dl2017/images/c34ba67cca6cb94c79a2e63cc5749c1f.png" alt="img"></p>
<p>我在这里画一个函数，，如果 的话，那么。让我们稍稍往右推进一点点，现在 ，则 (如果你用计算器算的话，这个准确的值应该为4.004001 我只是为了简便起见，省略了后面的部分)，如果你在这儿画，一个小三角形 你就会发现，如果把往右移动0.001，那么将增大四倍，即增大0.004。在微积分中我们把这个三角形斜边的斜率，称为在点 处的导数(即为4)，或者写成微积分的形式，当 的时候，  由此可知，函数，在取不同值的时候，它的斜率是不同的，这和上个视频中的例子是不同的。</p>
<p>这里有种直观的方法可以解释，为什么一个点的斜率，在不同位置会不同如果你在曲线上，的不同位置画一些小小的三角形你就会发现，三角形高和宽的比值，在曲线上不同的地方，它们是不同的。所以当 时，斜率为4；而当时，斜率为10 。如果你翻看微积分的课本，课本会告诉你，函数的斜率（即导数）为。这意味着任意给定一点，如果你稍微将，增大0.001，那么你会看到将增大，即增大的值为点在处斜率或导数，乘以你向右移动的距离。</p>
<p>现在有个小细节需要注意，导数增大的值，不是刚好等于导数公式算出来的值，而只是根据导数算出来的一个估计值。</p>
<p><strong>为了总结这堂课所学的知识，我们再来看看几个例子：</strong></p>
<p><img src="http://www.ai-start.com/dl2017/images/5b78b725478a0b4def09539f54c784be.png" alt="img"></p>
<p>假设 如果你翻看导数公式表，你会发现这个函数的导数，等于。所以这是什么意思呢，同样地举一个例子：我们再次令，所以 ，如果我们又将增大一点点，你会发现， 你可以自己检查一遍，如果我们取8.012，你会发现 ，和8.012很接近，事实上当时，导数值为，即。所以导数公式，表明如果你将向右移动0.001时， 将会向右移动12倍，即0.012。</p>
<p>来看最后一个例子，假设，有些可能会写作，函数 的斜率应该为，所以我们可以解释如下：如果取任何值，比如又取，然后又把向右边移动0.001 那么将增大，如果你借助计算器的话，你会发现当时 ；而时，。所以增大了0.0005，如果你查看导数公式，当的时候，导数值。这表明如果你把 增大0.001，将只会增大0.001的二分之一，即0.0005。如果你画个小三角形你就会发现，如果 轴增加了0.001，那么 轴上的函数，将增大0.001的一半 即0.0005。所以  ，当时这里是 ，就是当时这条线的斜率。这些就是有关，导数的一些知识。</p>
<p><strong>在这个视频中，你只需要记住两点：</strong></p>
<p>第一点，导数就是斜率，而函数的斜率，在不同的点是不同的。在第一个例子中 ，这是一条直线，在任何点它的斜率都是相同的，均为3。但是对于函数 ，或者，它们的斜率是变化的，所以它们的导数或者斜率，在曲线上不同的点处是不同的。</p>
<p>第二点，如果你想知道一个函数的导数，你可参考你的微积分课本或者维基百科，然后你应该就能找到这些函数的导数公式。</p>
<p>最后我希望，你能通过我生动的讲解，掌握这些有关导数和斜率的知识，下一课我们将讲解计算图，以及如何用它来求更加复杂的函数的导数。</p>
<h3 id="2-7-计算图（Computation-Graph）"><a href="#2-7-计算图（Computation-Graph）" class="headerlink" title="2.7 计算图（Computation Graph）"></a>2.7 计算图（Computation Graph）</h3><p>可以说，一个神经网络的计算，都是按照前向或反向传播过程组织的。首先我们计算出一个新的网络的输出（前向过程），紧接着进行一个反向传输操作。后者我们用来计算出对应的梯度或导数。计算图解释了为什么我们用这种方式组织这些计算过程。在这个视频中，我们将举一个例子说明计算图是什么。让我们举一个比逻辑回归更加简单的，或者说不那么正式的神经网络的例子。</p>
<p><img src="http://www.ai-start.com/dl2017/images/5216254e20325aad2dd51975bbc70068.png" alt="img"></p>
<p>我们尝试计算函数，是由三个变量组成的函数，这个函数是 。计算这个函数实际上有三个不同的步骤，首先是计算  乘以 ，我们把它储存在变量中，因此； 然后计算；最后输出，这就是要计算的函数。我们可以把这三步画成如下的计算图，我先在这画三个变量，第一步就是计算，我在这周围放个矩形框，它的输入是，接着第二步，最后一步。 举个例子:  ，就是6， ，就是5+6=11。是3倍的 ，因此。即。如果你把它算出来，实际上得到33就是的值。 当有不同的或者一些特殊的输出变量时，例如本例中的和逻辑回归中你想优化的代价函数，因此计算图用来处理这些计算会很方便。从这个小例子中我们可以看出，通过一个从左向右的过程，你可以计算出的值。为了计算导数，从右到左（红色箭头，和蓝色箭头的过程相反）的过程是用于计算导数最自然的方式。 概括一下：计算图组织计算的形式是用蓝色箭头从左到右的计算，让我们看看下一个视频中如何进行反向红色箭头(也就是从右到左)的导数计算，让我们继续下一个视频的学习。</p>
<h3 id="2-8-计算图的导数计算（Derivatives-with-a-Computation-Graph）"><a href="#2-8-计算图的导数计算（Derivatives-with-a-Computation-Graph）" class="headerlink" title="2.8 计算图的导数计算（Derivatives with a Computation Graph）"></a>2.8 计算图的导数计算（Derivatives with a Computation Graph）</h3><p>在上一个视频中，我们看了一个例子使用流程计算图来计算函数J。现在我们清理一下流程图的描述，看看你如何利用它计算出函数的导数。</p>
<p>下面用到的公式：</p>
<p> ，   ，  </p>
<p>这是一个流程图：</p>
<p><img src="http://www.ai-start.com/dl2017/images/b1c9294420787ec6d7724d64ed9b4a43.png" alt="img"></p>
<p>假设你要计算，那要怎么算呢？好，比如说，我们要把这个值拿过来，改变一下，那么的值会怎么变呢？</p>
<p>所以定义上，现在，所以如果你让v增加一点点，比如到11.001，那么，所以我这里增加了0.001，然后最终结果是J上升到原来的3倍，所以，因为对于任何  的增量都会有3倍增量，而且这类似于我们在上一个视频中的例子，我们有，然后我们推导出，所以这里我们有，所以，这里扮演了的角色，在之前的视频里的例子。</p>
<p>在反向传播算法中的术语，我们看到，如果你想计算最后输出变量的导数，使用你最关心的变量对的导数，那么我们就做完了一步反向传播，在这个流程图中是一个反向步。</p>
<p><img src="http://www.ai-start.com/dl2017/images/44c62688d05844b26599653545d24dd4.png" alt="img"></p>
<p>我们来看另一个例子，是多少呢？换句话说，如果我们提高a的数值，对J的数值有什么影响？</p>
<p>好，我们看看这个例子。变量，我们让它增加到5.001，那么对v的影响就是,之前，现在变成11.001，我们从上面看到现在 就变成33.003了，所以我们看到的是，如果你让增加0.001，增加0.003。那么增加，我是说如果你把这个5换成某个新值，那么的改变量就会传播到流程图的最右，所以最后是33.003。所以J的增量是3乘以的增量，意味着这个导数是3。</p>
<p><img src="http://www.ai-start.com/dl2017/images/d4f37c1db52a999dd68b89564449669f.png" alt="img"></p>
<p>要解释这个计算过程，其中一种方式是：如果你改变了，那么也会改变，通过改变，也会改变，所以值的净变化量，当你提升这个值（0.001），当你把值提高一点点，这就是的变化量（0.003）。</p>
<p><img src="http://www.ai-start.com/dl2017/images/7e9ea7f52cab1a428aa1fb670fbe54e9.png" alt="img"></p>
<p>首先a增加了，也会增加，增加多少呢？这取决于，然后的变化导致也在增加，所以这在微积分里实际上叫链式法则，如果影响到，影响到，那么当你让变大时，J的变化量就是当你改变时，的变化量乘以改变时的变化量，在微积分里这叫链式法则。</p>
<p><img src="http://www.ai-start.com/dl2017/images/eccab9443ace5d97b50ec283a8f85ba8.png" alt="img"></p>
<p>我们从这个计算中看到，如果你让增加0.001，也会变化相同的大小，所以。事实上，如果你代入进去，我们之前算过，，所以这个乘积3×1，实际上就给出了正确答案，。</p>
<p>这张小图表示了如何计算，就是对变量的导数，它可以帮助你计算，所以这是另一步反向传播计算。</p>
<p><img src="http://www.ai-start.com/dl2017/images/960127d6727a198511fe59459bf4a724.png" alt="img"></p>
<p>现在我想介绍一个新的符号约定，当你编程实现反向传播时，通常会有一个最终输出值是你要关心的，最终的输出变量，你真正想要关心或者说优化的。在这种情况下最终的输出变量是J，就是流程图里最后一个符号，所以有很多计算尝试计算输出变量的导数，所以输出变量对某个变量的导数，我们就用命名，所以在很多计算中你需要计算最终输出结果的导数，在这个例子里是，还有各种中间变量，比如、、、、，当你在软件里实现的时候，变量名叫什么？你可以做的一件事是，在<strong>python</strong>中，你可以写一个很长的变量名，比如，但这个变量名有点长，我们就用，但因为你一直对求导，对这个最终输出变量求导。我这里要介绍一个新符号，在程序里，当你编程的时候，在代码里，我们就使用变量名，来表示那个量。</p>
<p><img src="http://www.ai-start.com/dl2017/images/88f92a16ca8cd36f8252c5c6db9c980b.png" alt="img"></p>
<p>好，所以在程序里是表示导数，你关心的最终变量的导数，有时最后是，对代码中各种中间量的导数，所以代码里这个东西，你用表示这个值，所以，你的代码表示就是。</p>
<p><img src="http://www.ai-start.com/dl2017/images/22e98c374e2aaa999f46d27339ce6720.png" alt="img"></p>
<p>好，所以我们通过这个流程图完成部分的后向传播算法。我们在下一张幻灯片看看这个例子剩下的部分。</p>
<p>我们清理出一张新的流程图，我们回顾一下，到目前为止，我们一直在往回传播，并计算，再次，是代码里的变量名，其真正的定义是。我发现，再次，是代码里的变量名，其实代表的值。</p>
<p><img src="http://www.ai-start.com/dl2017/images/74a169313e532a2c953ea01b05d57385.png" alt="img"></p>
<p>大概手算了一下，两条直线怎么计算反向传播。</p>
<p>好，我们继续计算导数，我们看看这个值，那么是多少呢？通过和之前类似的计算，现在我们从出发，如果你令u增加到6.001，那么之前是11，现在变成11.001了， 就从33变成33.003，所以 增量是3倍，所以。对的分析很类似对a的分析，实际上这计算起来就是，有了这个，我们可以算出，，最终算出结果是。</p>
<p><img src="http://www.ai-start.com/dl2017/images/596ea4f3492f8e96ecd560e3899e2700.png" alt="img"></p>
<p>所以我们还有一步反向传播，我们最终计算出，这里的当然了，就是。</p>
<p>现在，我们仔细看看最后一个例子，那么呢？想象一下，如果你改变了的值，你想要然后变化一点，让 值到达最大或最小，那么导数是什么呢？这个函数的斜率，当你稍微改变值之后。事实上，使用微积分链式法则，这可以写成两者的乘积，就是，理由是，如果你改变一点点，所以变化比如说3.001，它影响J的方式是，首先会影响u，它对的影响有多大？好，的定义是，所以时这是6，现在就变成6.002了，对吧，因为在我们的例子中，所以这告诉我们当你让b增加0.001时，就增加两倍。所以，现在我想的增加量已经是的两倍，那么是多少呢？我们已经弄清楚了，这等于3，所以让这两部分相乘，我们发现。</p>
<p>好，这就是第二部分的推导，其中我们想知道  增加0.002，会对 有什么影响。实际上，这告诉我们u增加0.002之后，上升了3倍，那么 应该上升0.006，对吧。这可以从推导出来。</p>
<p>如果你仔细看看这些数学内容，你会发现，如果变成3.001，那么就变成6.002，变成11.002，然后，对吧？这就是如何得到。</p>
<p><img src="http://www.ai-start.com/dl2017/images/c56e483eaebb9425af973bb9849ad2e4.png" alt="img"></p>
<p>为了填进去，如果我们反向走的话，，而db其实是<strong>Python</strong>代码中的变量名，表示。</p>
<p><img src="http://www.ai-start.com/dl2017/images/c933c8d935a95e66bbf6a61adecbb816.png" alt="img"></p>
<p>我不会很详细地介绍最后一个例子，但事实上，如果你计算，这个结果是9。</p>
<p><img src="http://www.ai-start.com/dl2017/images/01340e74ea10fa9ba32fbbc4ccf9a6df.png" alt="img"></p>
<p>我不会详细说明这个例子，在最后一步，我们可以推出。</p>
<p><img src="http://www.ai-start.com/dl2017/images/cd75ffa2793fa4af02bdd869fe962bc1.png" alt="img"></p>
<p>所以这个视频的要点是，对于那个例子，当计算所有这些导数时，最有效率的办法是从右到左计算，跟着这个红色箭头走。特别是当我们第一次计算对的导数时，之后在计算对导数就可以用到。然后对的导数，比如说这个项和这里这个项：</p>
<p><img src="http://www.ai-start.com/dl2017/images/90fd887adacd062cc60be1f553797fab.png" alt="img"></p>
<p>可以帮助计算对的导数，然后对的导数。</p>
<p>所以这是一个计算流程图，就是正向或者说从左到右的计算来计算成本函数J，你可能需要优化的函数，然后反向从右到左计算导数。如果你不熟悉微积分或链式法则，我知道这里有些细节讲的很快，但如果你没有跟上所有细节，也不用怕。在下一个视频中，我会再过一遍。在逻辑回归的背景下过一遍，并给你介绍需要做什么才能编写代码，实现逻辑回归模型中的导数计算。</p>
<h3 id="2-9-逻辑回归中的梯度下降（Logistic-Regression-Gradient-Descent）"><a href="#2-9-逻辑回归中的梯度下降（Logistic-Regression-Gradient-Descent）" class="headerlink" title="2.9 逻辑回归中的梯度下降（Logistic Regression Gradient Descent）"></a>2.9 逻辑回归中的梯度下降（Logistic Regression Gradient Descent）</h3><p>本节我们讨论怎样通过计算偏导数来实现逻辑回归的梯度下降算法。它的关键点是几个重要公式，其作用是用来实现逻辑回归中梯度下降算法。但是在本节视频中，我将使用计算图对梯度下降算法进行计算。我必须要承认的是，使用计算图来计算逻辑回归的梯度下降算法有点大材小用了。但是，我认为以这个例子作为开始来讲解，可以使你更好的理解背后的思想。从而在讨论神经网络时，你可以更深刻而全面地理解神经网络。接下来让我们开始学习逻辑回归的梯度下降算法。</p>
<p>假设样本只有两个特征和，为了计算，我们需要输入参数、 和，除此之外还有特征值和。因此的计算公式为：  回想一下逻辑回归的公式定义如下：  其中 损失函数：  代价函数：  假设现在只考虑单个样本的情况，单个样本的代价函数定义如下：  其中是逻辑回归的输出，是样本的标签值。现在让我们画出表示这个计算的计算图。 这里先复习下梯度下降法，和的修正量可以表达如下：</p>
<p>，</p>
<p><img src="http://www.ai-start.com/dl2017/images/03f5f96177ab15d5ead8298ba50300ac.jpg" alt="2017-08-16 16-05-25"></p>
<p>如图：在这个公式的外侧画上长方形。然后计算：  也就是计算图的下一步。最后计算损失函数。 有了计算图，我就不需要再写出公式了。因此，为了使得逻辑回归中最小化代价函数，我们需要做的仅仅是修改参数和的值。前面我们已经讲解了如何在单个训练样本上计算代价函数的前向步骤。现在让我们来讨论通过反向计算出导数。 因为我们想要计算出的代价函数的导数，首先我们需要反向计算出代价函数关于的导数，在编写代码时，你只需要用 来表示 。 通过微积分得到：  如果你不熟悉微积分，也不必太担心，我们会列出本课程涉及的所有求导公式。那么如果你非常熟悉微积分，我们鼓励你主动推导前面介绍的代价函数的求导公式，使用微积分直接求出关于变量的导数。如果你不太了解微积分，也不用太担心。现在我们已经计算出，也就是最终输出结果的导数。 现在可以再反向一步，在编写Python代码时，你只需要用来表示代价函数关于 的导数，也可以写成，这两种写法都是正确的。  。 因为， 并且， 而 ， 因此将这两项相乘  视频中为了简化推导过程，假设 这个推导的过程就是我之前提到过的链式法则。如果你对微积分熟悉，放心地去推导整个求导过程，如果不熟悉微积分，你只需要知道已经计算好了。</p>
<p>现在进行最后一步反向推导，也就是计算和变化对代价函数的影响，特别地，可以用:    视频中，  表示，  表示， 。 因此，关于单个样本的梯度下降算法，你所需要做的就是如下的事情： 使用公式计算， 使用 计算， 计算，  来计算， 然后: 更新， 更新， 更新。 这就是关于单个样本实例的梯度下降算法中参数更新一次的步骤。 <img src="http://www.ai-start.com/dl2017/images/6403f00e5844c3100f4aa9ff043e2319.jpg" alt="2017-08-16 16-07-49"></p>
<p>现在你已经知道了怎样计算导数，并且实现针对单个训练样本的逻辑回归的梯度下降算法。但是，训练逻辑回归模型不仅仅只有一个训练样本，而是有个训练样本的整个训练集。因此在下一节视频中，我们将这些思想应用到整个训练样本集中，而不仅仅只是单个样本上。</p>
<h3 id="2-10-m-个样本的梯度下降-Gradient-Descent-on-m-Examples"><a href="#2-10-m-个样本的梯度下降-Gradient-Descent-on-m-Examples" class="headerlink" title="2.10  m 个样本的梯度下降(Gradient Descent on m Examples)"></a>2.10  m 个样本的梯度下降(Gradient Descent on m Examples)</h3><p>在之前的视频中,你已经看到如何计算导数，以及应用梯度下降在逻辑回归的一个训练样本上。现在我们想要把它应用在个训练样本上。</p>
<p><img src="http://www.ai-start.com/dl2017/images/bf930b1f68d8e0726dda5393afc83672.png" alt="02-10-Gradient [00_02_18][20170816-103833-9]"></p>
<p>首先，让我们时刻记住有关于损失函数的定义。</p>
<p>当你的算法输出关于样本的，是训练样本的预测值，即：。 所以我们在前面的幻灯中展示的是对于任意单个训练样本，如何计算微分当你只有一个训练样本。因此，和 添上上标表示你求得的相应的值。如果你面对的是我们在之前的幻灯中演示的那种情况，但只使用了一个训练样本。 现在你知道带有求和的全局代价函数，实际上是1到项各个损失的平均。 所以它表明全局代价函数对的微分，对的微分也同样是各项损失对微分的平均。</p>
<p><img src="http://www.ai-start.com/dl2017/images/8b725e51dcffc53a5def49438b70d925.png" alt="img"></p>
<p>但之前我们已经演示了如何计算这项，即之前幻灯中演示的如何对单个训练样本进行计算。所以你真正需要做的是计算这些微分，如我们在之前的训练样本上做的。并且求平均，这会给你全局梯度值，你能够把它直接应用到梯度下降算法中。</p>
<p>所以这里有很多细节，但让我们把这些装进一个具体的算法。同时你需要一起应用的就是逻辑回归和梯度下降。</p>
<p>我们初始化</p>
<p>代码流程：</p>
<pre><code>J=0;dw1=0;dw2=0;db=0;
</code></pre><pre><code>for i = 1 to m
</code></pre><pre><code>    z(i) = wx(i)+b;
</code></pre><pre><code>    a(i) = sigmoid(z(i));
</code></pre><pre><code>    J += -[y(i)log(a(i))+(1-y(i)）log(1-a(i));
</code></pre><pre><code>    dz(i) = a(i)-y(i);
</code></pre><pre><code>    dw1 += x1(i)dz(i);
</code></pre><pre><code>    dw2 += x2(i)dz(i);
</code></pre><pre><code>    db += dz(i);
</code></pre><pre><code>J/= m;
</code></pre><pre><code>dw1/= m;
</code></pre><pre><code>dw2/= m;
</code></pre><pre><code>db/= m;
</code></pre><pre><code>w=w-alpha*dw
</code></pre><pre><code>b=b-alpha*db
</code></pre><p>幻灯片上只应用了一步梯度下降。因此你需要重复以上内容很多次，以应用多次梯度下降。看起来这些细节似乎很复杂，但目前不要担心太多。希望你明白，当你继续尝试并应用这些在编程作业里，所有这些会变的更加清楚。</p>
<p>但这种计算中有两个缺点，也就是说应用此方法在逻辑回归上你需要编写两个<strong>for</strong>循环。第一个<strong>for</strong>循环是一个小循环遍历个训练样本，第二个<strong>fo</strong>r循环是一个遍历所有特征的<strong>for</strong>循环。这个例子中我们只有2个特征，所以等于2并且 等于2。 但如果你有更多特征，你开始编写你的因此，，你有相似的计算从一直下去到。所以看来你需要一个<strong>for</strong>循环遍历所有个特征。</p>
<p>当你应用深度学习算法，你会发现在代码中显式地使用<strong>for</strong>循环使你的算法很低效，同时在深度学习领域会有越来越大的数据集。所以能够应用你的算法且没有显式的<strong>for</strong>循环会是重要的，并且会帮助你适用于更大的数据集。所以这里有一些叫做向量化技术,它可以允许你的代码摆脱这些显式的<strong>for</strong>循环。</p>
<p>我想在先于深度学习的时代，也就是深度学习兴起之前，向量化是很棒的。可以使你有时候加速你的运算，但有时候也未必能够。但是在深度学习时代向量化，摆脱for循环已经变得相当重要。因为我们越来越多地训练非常大的数据集，因此你真的需要你的代码变得非常高效。所以在接下来的几个视频中，我们会谈到向量化，以及如何应用向量化而连一个<strong>for</strong>循环都不使用。所以学习了这些，我希望你有关于如何应用逻辑回归，或是用于逻辑回归的梯度下降，事情会变得更加清晰。当你进行编程练习，但在真正做编程练习之前让我们先谈谈向量化。然后你可以应用全部这些东西，应用一个梯度下降的迭代而不使用任何<strong>for</strong>循环。</p>
<h3 id="2-11-向量化-Vectorization"><a href="#2-11-向量化-Vectorization" class="headerlink" title="2.11 向量化(Vectorization)"></a>2.11 向量化(Vectorization)</h3><p>参考视频: 2.11 向量化</p>
<p>向量化是非常基础的去除代码中<strong>for</strong>循环的艺术，在深度学习安全领域、深度学习实践中，你会经常发现自己训练大数据集，因为深度学习算法处理大数据集效果很棒，所以你的代码运行速度非常重要，否则如果在大数据集上，你的代码可能花费很长时间去运行，你将要等待非常长的时间去得到结果。所以在深度学习领域，运行向量化是一个关键的技巧，让我们举个栗子说明什么是向量化。</p>
<p>在逻辑回归中你需要去计算，、都是列向量。如果你有很多的特征那么就会有一个非常大的向量，所以 , ，所以如果你想使用非向量化方法去计算，你需要用如下方式（python）</p>
<pre><code>z=0
</code></pre><pre><code>
</code></pre><pre><code>for i in range(n_x)
</code></pre><pre><code>
</code></pre><pre><code>z+=w[i]*x[i]
</code></pre><pre><code>
</code></pre><pre><code>z+=b
</code></pre><p>这是一个非向量化的实现，你会发现这真的很慢，作为一个对比，向量化实现将会非常直接计算，代码如下：</p>
<p><code>z=np.dot(w,x)+b</code></p>
<p>这是向量化计算的方法，你将会发现这个非常快</p>
<p><img src="http://www.ai-start.com/dl2017/images/e9c7f9f3694453c07fe6b9fe7cf0c4c8.png" alt="img"></p>
<p>让我们用一个小例子说明一下，在我的我将会写一些代码（以下为教授在他的<strong>Jupyter notebook</strong>上写的<strong>Python</strong>代码，）</p>
<pre><code>
</code></pre><pre><code>import numpy as np #导入numpy库
</code></pre><pre><code>a = np.array([1,2,3,4]) #创建一个数据a
</code></pre><pre><code>print(a)
</code></pre><pre><code># [1 2 3 4]
</code></pre><pre><code>import time #导入时间库
</code></pre><pre><code>a = np.random.rand(1000000)
</code></pre><pre><code>b = np.random.rand(1000000) #通过round随机得到两个一百万维度的数组
</code></pre><pre><code>tic = time.time() #现在测量一下当前时间
</code></pre><pre><code>#向量化的版本
</code></pre><pre><code>c = np.dot(a,b)
</code></pre><pre><code>toc = time.time()
</code></pre><pre><code>print(“Vectorized version:” + str(1000*(toc-tic)) +”ms”) #打印一下向量化的版本的时间
</code></pre><pre><code>
</code></pre><pre><code>#继续增加非向量化的版本
</code></pre><pre><code>c = 0
</code></pre><pre><code>tic = time.time()
</code></pre><pre><code>for i in range(1000000):
</code></pre><pre><code>c += a[i]*b[i]
</code></pre><pre><code>toc = time.time()
</code></pre><pre><code>print(c)
</code></pre><pre><code>print(“For loop:” + str(1000*(toc-tic)) + “ms”)#打印for循环的版本的时间
</code></pre><p>返回值见图。</p>
<p>在两个方法中，向量化和非向量化计算了相同的值，如你所见，向量化版本花费了1.5毫秒，非向量化版本的<strong>for</strong>循环花费了大约几乎500毫秒，非向量化版本多花费了300倍时间。所以在这个例子中，仅仅是向量化你的代码，就会运行300倍快。这意味着如果向量化方法需要花费一分钟去运行的数据，for循环将会花费5个小时去运行。</p>
<p>一句话总结，以上都是再说和for循环相比，向量化可以快速得到结果。</p>
<p>你可能听过很多类似如下的话，“大规模的深度学习使用了<strong>GPU</strong>或者图像处理单元实现”，但是我做的所有的案例都是在<strong>jupyter notebook</strong>上面实现，这里只有<strong>CPU</strong>，<strong>CPU</strong>和<strong>GPU</strong>都有并行化的指令，他们有时候会叫做<strong>SIMD</strong>指令，这个代表了一个单独指令多维数据，这个的基础意义是，如果你使用了<strong>built-in</strong>函数,像<code>np.function</code>或者并不要求你实现循环的函数，它可以让<strong>python</strong>的充分利用并行化计算，这是事实在<strong>GPU</strong>和<strong>CPU</strong>上面计算，<strong>GPU</strong>更加擅长<strong>SIMD</strong>计算，但是<strong>CPU</strong>事实上也不是太差，可能没有<strong>GPU</strong>那么擅长吧。接下来的视频中，你将看到向量化怎么能够加速你的代码，经验法则是，无论什么时候，避免使用明确的<strong>for</strong>循环。</p>
<p>以下代码及运行结果截图：</p>
<p><img src="http://www.ai-start.com/dl2017/images/6ed990de06443ed997090b38671ada1a.png" alt="img"></p>
<p><img src="http://www.ai-start.com/dl2017/images/6af2ed5d358b4f4dd50b67c3887867ef.png" alt="img"></p>
<p><img src="http://www.ai-start.com/dl2017/images/77280fc4a2f1d609b58f23cde47b709c.png" alt="img"></p>
<h3 id="2-12-向量化的更多例子（More-Examples-of-Vectorization）"><a href="#2-12-向量化的更多例子（More-Examples-of-Vectorization）" class="headerlink" title="2.12 向量化的更多例子（More Examples of Vectorization）"></a>2.12 向量化的更多例子（More Examples of Vectorization）</h3><p>从上节视频中，你知道了怎样通过<strong>numpy</strong>内置函数和避开显式的循环(<strong>loop</strong>)的方式进行向量化，从而有效提高代码速度。</p>
<p>经验提醒我，当我们在写神经网络程序时，或者在写逻辑(l<strong>ogistic</strong>)回归，或者其他神经网络模型时，应该避免写循环(<strong>loop</strong>)语句。虽然有时写循环(<strong>loop</strong>)是不可避免的，但是我们可以使用比如<strong>numpy</strong>的内置函数或者其他办法去计算。当你这样使用后，程序效率总是快于循环(<strong>loop)</strong>。</p>
<p>让我们看另外一个例子。如果你想计算向量，这时矩阵乘法定义为，矩阵乘法的定义就是：，这取决于你怎么定义值。同样使用非向量化实现，， 并且通过两层循环，得到 。现在就有了 和  的两层循环，这就是非向量化。向量化方式就可以用，右边这种向量化实现方式，消除了两层循环使得代码运行速度更快。 </p>
<p><img src="http://www.ai-start.com/dl2017/images/a22eb5a73d2d45b342810eee70a4620c.png" alt="3"></p>
<p>下面通过另一个例子继续了解向量化。如果你已经有一个向量，并且想要对向量的每个元素做指数操作，得到向量等于的，的，一直到的次方。这里是非向量化的实现方式，首先你初始化了向量，并且通过循环依次计算每个元素。但事实证明可以通过<strong>python</strong>的<strong>numpy</strong>内置函数，帮助你计算这样的单个函数。所以我会引入<code>import numpy as np</code>，执行  命令。注意到，在之前有循环的代码中，这里仅用了一行代码，向量作为输入，作为输出。你已经知道为什么需要循环，并且通过右边代码实现，效率会明显的快于循环方式。</p>
<p>事实上，<strong>numpy</strong>库有很多向量函数。比如 <code>u=np.log</code>是计算对数函数()、 <code>np.abs()</code> 是计算数据的绝对值、<code>np.maximum()</code> 计算元素中的最大值，你也可以 <code>np.maximum(v,0)</code> 、  代表获得元素  每个值得平方、  获取元素  的逆等等。所以当你想写循环时候，检查<strong>numpy</strong>是否存在类似的内置函数，从而避免使用循环(<strong>loop</strong>)方式。</p>
<p><img src="http://www.ai-start.com/dl2017/images/6a7d76b4a66ef5af71d55b8f980a5ab2.png" alt="1"></p>
<p>那么，将刚才所学到的内容，运用在逻辑回归的梯度下降上，看看我们是否能简化两个计算过程中的某一步。这是我们逻辑回归的求导代码，有两层循环。在这例子我们有个特征值。如果你有超过两个特征时，需要循环  、 、 等等。所以  的实际值是1、2 和 ，就是你想要更新的值。所以我们想要消除第二循环，在这一行，这样我们就不用初始化  ，  都等于0。去掉这些，而是定义  为一个向量，设置 。定义了一个行的一维向量，从而替代循环。我们仅仅使用了一个向量操作  。最后，我们得到  。现在我们通过将两层循环转成一层循环，我们仍然还有这个循环训练样本。</p>
<p><img src="http://www.ai-start.com/dl2017/images/e28b7cda6504e2a8ff0f0b2f1e258a96.png" alt="4"></p>
<p><img src="http://www.ai-start.com/dl2017/images/af298e37ade1883eaeb44c822e279d42.png" alt="2"></p>
<p>希望这个视频给了你一点向量化感觉，减少一层循环使你代码更快，但事实证明我们能做得更好。所以在下个视频，我们将进一步的讲解逻辑回归，你将会看到更好的监督学习结果。在训练中不需要使用任何 <strong>for</strong> 循环，你也可以写出代码去运行整个训练集。到此为止一切都好，让我们看下一个视频。</p>
<h3 id="2-13-向量化逻辑回归-Vectorizing-Logistic-Regression"><a href="#2-13-向量化逻辑回归-Vectorizing-Logistic-Regression" class="headerlink" title="2.13 向量化逻辑回归(Vectorizing Logistic Regression)"></a>2.13 向量化逻辑回归(Vectorizing Logistic Regression)</h3><p>我们已经讨论过向量化是如何显著加速你的代码，在本次视频中我们将讨论如何实现逻辑回归的向量化计算。这样就能处理整个数据集，甚至不会用一个明确的for循环就能实现对于整个数据集梯度下降算法的优化。我对这项技术感到非常激动，并且当我们后面谈到神经网络时同样也不会用到一个明确的 for 循环。</p>
<p>让我们开始吧，首先我们回顾一下逻辑回归的前向传播步骤。所以，如果你有  个训练样本，然后对第一个样本进行预测，你需要这样计算。计算 ，我正在使用这个熟悉的公式  。然后计算激活函数  ，计算第一个样本的预测值  。</p>
<p>然后对第二个样本进行预测，你需要计算  ，  。然后对第三个样本进行预测，你需要计算  ，  ，依次类推。如果你有  个训练样本，你可能需要这样做  次，可以看出，为了完成前向传播步骤，即对我们的  个样本都计算出预测值。有一个办法可以并且不需要任何一个明确的<strong>for</strong>循环。让我们来看一下你该怎样做。</p>
<p>首先，回忆一下我们曾经定义了一个矩阵  作为你的训练输入，(如下图中蓝色  )像这样在不同的列中堆积在一起。这是一个  行  列的矩阵。我现在将它写为<strong>Python numpy</strong>的形式  ，这只是表示  是一个  乘以  的矩阵 。</p>
<p><img src="http://www.ai-start.com/dl2017/images/3a8a0c9ed33cd6c033103e35c26eeeb7.png" alt="img"></p>
<p>现在我首先想做的是告诉你该如何在一个步骤中计算 、  、 等等。实际上，只用了一行代码。所以，我打算先构建一个  的矩阵，实际上它是一个行向量，同时我准备计算 ，  ……一直到  ，所有值都是在同一时间内完成。结果发现它可以表达为  的转置乘以大写矩阵  然后加上向量  ，  。 是一个  的向量或者  的矩阵或者是一个  维的行向量。所以希望你熟悉矩阵乘法，你会发现的  转置乘以  ，  一直到  。所以  转置可以是一个行向量。所以第一项  将计算  的转置乘以 ，  转置乘以 等等。然后我们加上第二项  ，你最终将  加到了每个元素上。所以你最终得到了另一个  的向量，  。</p>
<p> 这是第一个元素， 这是第二个元素，  这是第  个元素。</p>
<p>如果你参照上面的定义，第一个元素恰好是  的定义，第二个元素恰好是  的定义，等等。所以，因为是一次获得的，当你得到你的训练样本，一个一个横向堆积起来，这里我将  定义为大写的  ，你用小写  表示并将它们横向排在一起。所以当你将不同训练样本对应的小写  横向堆积在一起时得到大写变量  并且将小写变量也用相同方法处理，将它们横向堆积起来，你就得到大写变量  。结果发现，为了计算  ，<strong>numpy</strong>命令是。这里在<strong>Python</strong>中有一个巧妙的地方，这里  是一个实数，或者你可以说是一个  矩阵，只是一个普通的实数。但是当你将这个向量加上这个实数时，<strong>Python</strong>自动把这个实数  扩展成一个  的行向量。所以这种情况下的操作似乎有点不可思议，它在<strong>Python</strong>中被称作广播(<strong>brosdcasting</strong>)，目前你不用对此感到顾虑，我们将在下一个视频中进行进一步的讲解。话说回来它只用一行代码，用这一行代码，你可以计算大写的 ，而大写  是一个包含所有小写 到  的  的矩阵。这就是  的内容，关于变量  又是如何呢？</p>
<p>我们接下来要做的就是找到一个同时计算  的方法。就像把小写  堆积起来得到大写  和横向堆积小写  得到大写  一样，堆积小写变量  将形成一个新的变量，我们将它定义为大写 。在编程作业中，你将看到怎样用一个向量在<strong>sigmoid</strong>函数中进行计算。所以<strong>sigmoid</strong>函数中输入大写  作为变量并且非常高效地输出大写 。你将在编程作业中看到它的细节。</p>
<p>总结一下，在这张幻灯片中我们已经看到，不需要<strong>for</strong>循环，利用  个训练样本一次性计算出小写  和小写 ，用一行代码即可完成。</p>
<p><code>Z = np.(w.T,X) + b</code></p>
<p>这一行代码： ，通过恰当地运用一次性计算所有 。这就是在同一时间内你如何完成一个所有  个训练样本的前向传播向量化计算。</p>
<p>概括一下，你刚刚看到如何利用向量化在同一时间内高效地计算所有的激活函数的所有 值。接下来，可以证明，你也可以利用向量化高效地计算反向传播并以此来计算梯度。让我们在下一个视频中看该如何实现。</p>
<h3 id="2-14-向量化-logistic-回归的梯度输出（Vectorizing-Logistic-Regression’s-Gradient）"><a href="#2-14-向量化-logistic-回归的梯度输出（Vectorizing-Logistic-Regression’s-Gradient）" class="headerlink" title="2.14 向量化 logistic 回归的梯度输出（Vectorizing Logistic Regression’s Gradient）"></a>2.14 向量化 logistic 回归的梯度输出（Vectorizing Logistic Regression’s Gradient）</h3><p>注：本节中大写字母代表向量，小写字母代表元素</p>
<p>如何向量化计算的同时，对整个训练集预测结果，这是我们之前已经讨论过的内容。在本次视频中我们将学习如何向量化地计算个训练数据的梯度，本次视频的重点是如何<strong>同时</strong>计算  个数据的梯度，并且实现一个非常高效的逻辑回归算法<strong>(Logistic Regression</strong>)。</p>
<p>之前我们在讲梯度计算的时候，列举过几个例子， ， ……等等一系列类似公式。现在，对 个训练数据做同样的运算，我们可以定义一个新的变量  ，所有的 变量横向排列，因此， 是一个  的矩阵，或者说，一个  维行向量。在之前的幻灯片中，我们已经知道如何计算，即 ,我们需要找到这样的一个行向量  ，由此，我们可以这样计算 ，不难发现第一个元素就是 ，第二个元素就是  ……所以我们现在仅需一行代码，就可以同时完成这所有的计算。</p>
<p>在之前的实现中，我们已经去掉了一个<strong>for</strong>循环，但我们仍有一个遍历训练集的循环，如下所示：</p>
<p>………….</p>
<p>………….</p>
<p>上述（伪）代码就是我们在之前实现中做的，我们已经去掉了一个for循环，但用上述方法计算  仍然需要一个循环遍历训练集，我们现在要做的就是将其向量化！</p>
<p>首先我们来看 ，不难发现  ， 之前的讲解中，我们知道所有的已经组成一个行向量 了，所以在Python中，我们很容易地想到；接下来看，我们先写出它的公式  其中， 是一个行向量。因此展开后  。因此我们可以仅用两行代码进行计算：， 。这样，我们就避免了在训练集上使用for循环。</p>
<p>现在，让我们回顾一下，看看我们之前怎么实现的逻辑回归，可以发现，没有向量化是非常低效的，如下图所示代码：</p>
<p><img src="http://www.ai-start.com/dl2017/images/505663d02e8120e30c3d8405f31a8497.jpg" alt="img"></p>
<p>我们的目标是不使用<strong>for</strong>循环，而是向量，我们可以这么做：</p>
<p>现在我们利用前五个公式完成了前向和后向传播，也实现了对所有训练样本进行预测和求导，再利用后两个公式，梯度下降更新参数。我们的目的是不使用<strong>for</strong>循环，所以我们就通过一次迭代实现一次梯度下降，但如果你希望多次迭代进行梯度下降，那么仍然需要<strong>for</strong>循环，放在最外层。不过我们还是觉得一次迭代就进行一次梯度下降，避免使用任何循环比较舒服一些。</p>
<p>最后，我们得到了一个高度向量化的、非常高效的逻辑回归的梯度下降算法，我们将在下次视频中讨论<strong>Python</strong>中的<strong>Broadcasting</strong>技术。</p>
<h3 id="2-15-Python-中的广播（Broadcasting-in-Python）"><a href="#2-15-Python-中的广播（Broadcasting-in-Python）" class="headerlink" title="2.15 Python 中的广播（Broadcasting in Python）"></a>2.15 Python 中的广播（Broadcasting in Python）</h3><p><img src="http://www.ai-start.com/dl2017/images/685f36c96c86fef53c3abc8fe509949c.png" alt="img"></p>
<p>这是一个不同食物(每100g)中不同营养成分的卡路里含量表格，表格为3行4列，列表示不同的食物种类，从左至右依次为苹果，牛肉，鸡蛋，土豆。行表示不同的营养成分，从上到下依次为碳水化合物，蛋白质，脂肪。</p>
<p>那么，我们现在想要计算不同食物中不同营养成分中的卡路里百分比。</p>
<p>现在计算苹果中的碳水化合物卡路里百分比含量，首先计算苹果（100g）中三种营养成分卡路里总和56+1.2+1.8 = 59，然后用56/59 = 94.9%算出结果。</p>
<p>可以看出苹果中的卡路里大部分来自于碳水化合物，而牛肉则不同。</p>
<p>对于其他食物，计算方法类似。首先，按列求和，计算每种食物中（100g）三种营养成分总和，然后分别用不用营养成分的卡路里数量除以总和，计算百分比。</p>
<p>那么，能否不使用for循环完成这样的一个计算过程呢？</p>
<p>假设上图的表格是一个4行3列的矩阵，记为 ，接下来我们要使用<strong>Python</strong>的<strong>numpy</strong>库完成这样的计算。我们打算使用两行代码完成，第一行代码对每一列进行求和，第二行代码分别计算每种食物每种营养成分的百分比。</p>
<p>在<strong>jupyter notebook</strong>中输入如下代码，按shift+Enter运行，输出如下。</p>
<p><img src="http://www.ai-start.com/dl2017/images/56f38d09498335ae1155d2102f9b435d.png" alt="img"></p>
<p>下面使用如下代码计算每列的和，可以看到输出是每种食物(100g)的卡路里总和。</p>
<p><img src="http://www.ai-start.com/dl2017/images/c74557c22e724b1aa84ceeb3e5b6685d.png" alt="img"></p>
<p>其中<code>sum</code>的参数<code>axis=0</code>表示求和运算按列执行，之后会详细解释。</p>
<p>接下来计算百分比，这条指令将 的矩阵除以一个的矩阵，得到了一个 的结果矩阵，这个结果矩阵就是我们要求的百分比含量。</p>
<p><img src="http://www.ai-start.com/dl2017/images/aa852c608c711a73cb4d834a2956d9ae.png" alt="img"></p>
<p>下面再来解释一下<code>A.sum(axis = 0)</code>中的参数<code>axis</code>。<strong>axis用来指明将要进行的运算是沿着哪个轴执行，在numpy中，0轴是垂直的，也就是列，而1轴是水平的，也就是行。</strong></p>
<p>而第二个<code>A/cal.reshape(1,4)</code>指令则调用了<strong>numpy</strong>中的广播机制。这里使用 的矩阵除以 的矩阵。技术上来讲，其实并不需要再将矩阵 <code>reshape</code>(重塑)成 ，因为矩阵本身已经是 了。但是当我们写代码时不确定矩阵维度的时候，通常会对矩阵进行重塑来确保得到我们想要的列向量或行向量。重塑操作<code>reshape</code>是一个常量时间的操作，时间复杂度是，它的调用代价极低。</p>
<p>那么一个  的矩阵是怎么和 的矩阵做除法的呢？让我们来看一些更多的广播的例子。</p>
<p><img src="http://www.ai-start.com/dl2017/images/537ee092a262cbdb874fe6e775039823.png" alt="img"></p>
<p>在numpy中，当一个 的列向量与一个常数做加法时，实际上会将常数扩展为一个 的列向量，然后两者做逐元素加法。结果就是右边的这个向量。这种广播机制对于行向量和列向量均可以使用。</p>
<p>再看下一个例子。</p>
<p><img src="http://www.ai-start.com/dl2017/images/d03358cec4b5ea22eb5dbef06fb6a1b7.png" alt="img"></p>
<p>用一个 的矩阵和一个  的矩阵相加，其泛化形式是  的矩阵和 的矩阵相加。在执行加法操作时，其实是将  的矩阵复制成为  的矩阵，然后两者做逐元素加法得到结果。针对这个具体例子，相当于在矩阵的第一列加100，第二列加200，第三列加300。这就是在前一张幻灯片中计算卡路里百分比的广播机制，只不过这里是除法操作（广播机制与执行的运算种类无关）。</p>
<p>下面是最后一个例子</p>
<p><img src="http://www.ai-start.com/dl2017/images/0fd16b22ad9b31d531ad7aa50f95cbbc.png" alt="img"></p>
<p>这里相当于是一个  的矩阵加上一个  的矩阵。在进行运算时，会先将  矩阵水平复制  次，变成一个  的矩阵，然后再执行逐元素加法。</p>
<p>广播机制的一般原则如下：</p>
<p><img src="http://www.ai-start.com/dl2017/images/bdc1b1f6f0ba18659f140e47b26bf38b.png" alt="img"></p>
<p>这里我先说一下我本人对<strong>numpy</strong>广播机制的理解，再解释上面这张PPT。</p>
<p>首先是<strong>numpy</strong>广播机制</p>
<p><strong>如果两个数组的后缘维度的轴长度相符或其中一方的轴长度为1，则认为它们是广播兼容的。广播会在缺失维度和轴长度为1的维度上进行。</strong></p>
<p>后缘维度的轴长度：<code>A.shape[-1]</code> 即矩阵维度元组中的最后一个位置的值</p>
<p>对于视频中卡路里计算的例子，矩阵  后缘维度的轴长度是4，而矩阵  的后缘维度也是4，则他们满足后缘维度轴长度相符，可以进行广播。广播会在轴长度为1的维度进行，轴长度为1的维度对应<code>axis=0</code>，即垂直方向，矩阵  沿<code>axis=0</code>(垂直方向)复制成为  ，之后两者进行逐元素除法运算。</p>
<p>现在解释上图中的例子</p>
<p>矩阵  和矩阵  进行四则运算，后缘维度轴长度相符，可以广播，广播沿着轴长度为1的轴进行，即 广播成为  ，之后做逐元素四则运算。</p>
<p>矩阵  和矩阵  进行四则运算，后缘维度轴长度不相符，但其中一方轴长度为1，可以广播，广播沿着轴长度为1的轴进行，即  广播成为  ，之后做逐元素四则运算。</p>
<p>矩阵  和常数 进行四则运算，后缘维度轴长度不相符，但其中一方轴长度为1，可以广播，广播沿着缺失维度和轴长度为1的轴进行，缺失维度就是<code>axis=0</code>,轴长度为1的轴是<code>axis=1</code>，即广播成为  ，之后做逐元素四则运算。</p>
<p>最后，对于<strong>Matlab/Octave</strong> 有类似功能的函数<code>bsxfun</code>。</p>
<p>总结一下<code>broadcasting</code>，可以看看下面的图：</p>
<p><img src="http://www.ai-start.com/dl2017/images/695618c70fd2922182dc89dca8eb83cc.png" alt="img"></p>
<h3 id="2-16-关于-python-numpy-向量的说明（A-note-on-python-or-numpy-vectors）参考视频："><a href="#2-16-关于-python-numpy-向量的说明（A-note-on-python-or-numpy-vectors）参考视频：" class="headerlink" title="2.16 关于 python _ numpy 向量的说明（A note on python or numpy vectors）参考视频："></a>2.16 关于 python _ numpy 向量的说明（A note on python or numpy vectors）参考视频：</h3><p>本节主要讲<strong>Python</strong>中的<strong>numpy</strong>一维数组的特性，以及与行向量或列向量的区别。并介绍了老师在实际应用中的一些小技巧，去避免在coding中由于这些特性而导致的bug。</p>
<p><strong>Python</strong>的特性允许你使用广播（<strong>broadcasting</strong>）功能，这是<strong>Python</strong>的<strong>numpy</strong>程序语言库中最灵活的地方。而我认为这是程序语言的优点，也是缺点。优点的原因在于它们创造出语言的表达性，<strong>Python</strong>语言巨大的灵活性使得你仅仅通过一行代码就能做很多事情。但是这也是缺点，由于广播巨大的灵活性，有时候你对于广播的特点以及广播的工作原理这些细节不熟悉的话，你可能会产生很细微或者看起来很奇怪的bug。例如，如果你将一个列向量添加到一个行向量中，你会以为它报出维度不匹配或类型错误之类的错误，但是实际上你会得到一个行向量和列向量的求和。</p>
<p>在<strong>Python</strong>的这些奇怪的影响之中，其实是有一个内在的逻辑关系的。但是如果对Python不熟悉的话，我就曾经见过的一些学生非常生硬、非常艰难地去寻找bug。所以我在这里想做的就是分享给你们一些技巧，这些技巧对我非常有用，它们能消除或者简化我的代码中所有看起来很奇怪的bug。同时我也希望通过这些技巧，你也能更容易地写没有bug的<strong>Python</strong>和<strong>numpy</strong>代码。</p>
<p>为了演示Python-numpy的一个容易被忽略的效果，特别是怎样在Python-numpy中构造向量，让我来做一个快速示范。首先设置，这样会生成存储在数组  中的5个高斯随机数变量。之后输出 ，从屏幕上可以得知，此时  的<strong>shape</strong>（形状）是一个的结构。这在<strong>Python</strong>中被称作<strong>一个一维数组</strong>。它既不是一个行向量也不是一个列向量，这也导致它有一些不是很直观的效果。举个例子，如果我输出一个转置阵，最终结果它会和看起来一样，所以和的转置阵最终结果看起来一样。而如果我输出和的转置阵的内积，你可能会想：乘以的转置返回给你的可能会是一个矩阵。但是如果我这样做，你只会得到一个数。</p>
<p><img src="http://www.ai-start.com/dl2017/images/a44df591ad815cc67d74a275bd444342.png" alt="img"></p>
<p>所以我建议当你编写神经网络时，不要在它的<strong>shape</strong>是还是或者一维数组时使用数据结构。相反，如果你设置  为，那么这就将置于5行1列向量中。在先前的操作里  和  的转置看起来一样，而现在这样的  变成一个新的  的转置，并且它是一个行向量。请注意一个细微的差别，在这种数据结构中，当我们输出  的转置时有两对方括号，而之前只有一对方括号，所以这就是1行5列的矩阵和一维数组的差别。</p>
<p><img src="http://www.ai-start.com/dl2017/images/264aac3c4b28d894821771308be61417.png" alt="img"></p>
<p>如果你输出  和  的转置的乘积，然后会返回给你一个向量的外积，是吧？所以这两个向量的外积返回给你的是一个矩阵。</p>
<p><img src="http://www.ai-start.com/dl2017/images/424925f28c12891f7807b24e5bb41d8b.png" alt="img"></p>
<p>就我们刚才看到的，再进一步说明。首先我们刚刚运行的命令是这个 ，而且它生成了一个数据结构 ，是，一个有趣的东西。这被称作  的一维数组，同时这也是一个非常有趣的数据结构。它不像行向量和列向量那样表现的很一致，这也让它的一些影响不那么明显。所以我建议，当你在编程练习或者在执行逻辑回归和神经网络时，你不需要使用这些一维数组。</p>
<p><img src="http://www.ai-start.com/dl2017/images/5648345cf0ca282be13961bdc5b1681e.png" alt="img"></p>
<p>相反，如果你每次创建一个数组，你都得让它成为一个列向量，产生一个向量或者你让它成为一个行向量，那么你的向量的行为可能会更容易被理解。所以在这种情况下，等同于。这种表现很像 ，但是实际上却是一个列向量。同时这也是为什么当它是一个列向量的时候，你能认为这是矩阵；同时这里 将要变成，这就像行向量一样。所以当你需要一个向量时，我会说用这个或那个(<strong>column vector or row vector</strong>)，但绝不会是一维数组。</p>
<p><img src="http://www.ai-start.com/dl2017/images/2102548104812bdfbccd421f3848f900.png" alt="img"></p>
<p>我写代码时还有一件经常做的事，那就是如果我不完全确定一个向量的维度(<strong>dimension</strong>)，我经常会扔进一个断言语句(<strong>assertion statement</strong>)。像这样，去确保在这种情况下是一个向量，或者说是一个列向量。这些断言语句实际上是要去执行的，并且它们也会有助于为你的代码提供信息。所以不论你要做什么，不要犹豫直接插入断言语句。如果你不小心以一维数组来执行，你也能够重新改变数组维数 ，表明一个数组或者一个数组，以致于它表现更像列向量或行向量。</p>
<p><img src="http://www.ai-start.com/dl2017/images/147d3a15b0a4dae283662b2c44f91778.png" alt="img"></p>
<p>我有时候看见学生因为一维数组不直观的影响，难以定位bug而告终。通过在原先的代码里清除一维数组，我的代码变得更加简洁。而且实际上就我在代码中表现的事情而言，我从来不使用一维数组。因此，要去简化你的代码，而且不要使用一维数组。总是使用  维矩阵（基本上是列向量），或者  维矩阵（基本上是行向量），这样你可以减少很多<strong>assert</strong>语句来节省核矩阵和数组的维数的时间。另外，为了确保你的矩阵或向量所需要的维数时，不要羞于 <strong>reshape</strong> 操作。</p>
<p>总之，我希望这些建议能帮助你解决一个<strong>Python</strong>中的bug，从而使你更容易地完成练习。</p>
<h3 id="2-17-Jupyter-iPython-Notebooks快速入门（Quick-tour-of-Jupyter-iPython-Notebooks）"><a href="#2-17-Jupyter-iPython-Notebooks快速入门（Quick-tour-of-Jupyter-iPython-Notebooks）" class="headerlink" title="2.17 Jupyter/iPython Notebooks快速入门（Quick tour of Jupyter/iPython Notebooks）"></a>2.17 Jupyter/iPython Notebooks快速入门（Quick tour of Jupyter/iPython Notebooks）</h3><p>学到现在，你即将要开始处理你的第一个编程作业。但在那之前，让我快速地给你介绍一下在Coursera上的iPython Notebooks工具。</p>
<p><img src="http://www.ai-start.com/dl2017/images/ad2f424687e9a89e7874a32d65e4f0b8.png" alt="img"></p>
<p>这就是<strong>Jupyter iPython Notebooks</strong>的界面，你可以通过它连接到<strong>Coursera</strong>。让我快速地讲解下它的一些特性。关于它的说明已经被写入这个<strong>Notebook</strong>中。</p>
<p><img src="http://www.ai-start.com/dl2017/images/bf2ad2256d9b9180f44120571c0415f5.png" alt="img"></p>
<p>这里有一些空白区域的代码块，你可以在这里编写代码。有时，你也会看到一些函数块。而关于这些的说明都已经在<strong>iPython Notebook</strong>的文本中。在<strong>iPython Notebook</strong>中，在这些较长的灰色的区域就是代码块。</p>
<p><img src="http://www.ai-start.com/dl2017/images/c0a121b42faa4cd0ca2d3f2fc6215428.png" alt="img"></p>
<p>有时，你会看到代码块中有像这样的开始代码和结束代码。在进行编程练习时，请确保你的代码写在开始代码和结束代码之间。</p>
<p><img src="http://www.ai-start.com/dl2017/images/067c3d492df18e2ef262857eff04f4b1.png" alt="img"></p>
<p>比如，编写打印输出<strong>Hello World</strong>的代码，然后执行这一代码块（你可以按shift +enter来执行这一代码块）。最终，它就会输出我们想要的<strong>Hello World</strong>。</p>
<p><img src="http://www.ai-start.com/dl2017/images/c328b10a036e525eebdf7b25e307b967.png" alt="img"></p>
<p>在运行一个单元格<strong>cell</strong>时，你也可以选择运行其中的一块代码区域。通过点击<strong>Cell</strong>菜单的<strong>Run Cells</strong>执行这部分代码。</p>
<p>也许，在你的计算机上，运行<strong>cell</strong>的键盘快捷方式可能并非是<strong>shift enter</strong>。但是，Mac应该和我的个人电脑一样，可以使用<strong>shift + enter</strong>来运行<strong>cell</strong>。</p>
<p><img src="http://www.ai-start.com/dl2017/images/777048a0bae0db1cb0af3070a2a36aa4.png" alt="img"></p>
<p>当你正在阅读指南时，如果不小心双击了它，点中的区域就会变成<strong>markdown</strong>语言形式。如果你不小心使其变成了这样的文本框，只要运行下单元格<strong>cell</strong>，就可以回到原来的形式。所以，点击<strong>cell</strong>菜单的<strong>Run Cells</strong>或者使用<strong>shift + enter</strong>，就可以使得它变回原样。</p>
<p><img src="http://www.ai-start.com/dl2017/images/e218670e323ffef8c7f182184ae133a3.png" alt="img"></p>
<p>这里还有一些其他的小技巧。比如当你执行上面所使用的代码时，它实际上会使用一个内核在服务器上运行这段代码。如果你正在运行超负荷的进程，或者电脑运行了很长一段时间，或者在运行中出了错，又或者网络连接失败，这里依然有机会让<strong>Kernel</strong>重新工作。你只要点击<strong>Kernel</strong>，选择<strong>Restart</strong>，它会重新运行<strong>Kernel</strong>使程序继续工作。</p>
<p>所以，如果你只是运行相对较小的工作并且才刚刚启动你的<strong>ipad</strong>或笔记本电脑，这种情况应该是不会发生的。但是，如果你看见错误信息，比如<strong>Kernel</strong>已经中断或者其他信息,你可以试着重启<strong>Kernel</strong>。</p>
<p><img src="http://www.ai-start.com/dl2017/images/97a410e8ab2e386e2f617b00b5c60dfe.png" alt="img"></p>
<p>当我使用<strong>iPython Notebook</strong>时会有多个代码区域块。尽管我并没有在前面的代码块中添加自己的代码，但还是要确保先执行这块代码。因为在这个例子，它导入了<strong>numpy</strong>包并另命名为<strong>np</strong>等，并声明了一些你可能需要的变量。为了能顺利地执行下面的代码，就必须确保先执行上面的代码，即使不要求你去写其他的代码。</p>
<p><img src="http://www.ai-start.com/dl2017/images/8764cd16b111b5601a5bbb95ed600fe5.png" alt="img"></p>
<p>最后，当你完成作业后，可以通过点击右上方蓝色的<strong>Submit Assignment</strong>按钮提交你的作业。</p>
<p>我发现这种交互式的<strong>shell</strong>命令，在<strong>iPython Notebooks</strong>是非常有用的，能使你快速地实现代码并且查看输出结果，便于学习。所以我希望这些练习和<strong>Jupyter iPython Notebooks</strong>会帮助你更快地学习和实践，并且帮助你了解如何去实现这些学习算法。后面一个视频是一个选学视频，它主要是讲解逻辑回归中的代价函数。你可以选择是否观看。不管怎样，都祝愿你能通过这两次编程作业。我会在新一周的课程里等待着你。</p>
<h3 id="2-18-（选修）logistic-损失函数的解释（Explanation-of-logistic-regression-cost-function）"><a href="#2-18-（选修）logistic-损失函数的解释（Explanation-of-logistic-regression-cost-function）" class="headerlink" title="2.18 （选修）logistic 损失函数的解释（Explanation of logistic regression cost function）"></a>2.18 （选修）logistic 损失函数的解释（Explanation of logistic regression cost function）</h3><p>在前面的视频中，我们已经分析了逻辑回归的损失函数表达式，在这节选修视频中，我将给出一个简洁的证明来说明逻辑回归的损失函数为什么是这种形式。</p>
<p><img src="http://www.ai-start.com/dl2017/images/5b658991f4c61d088aa962310cee99eb.png" alt="img"></p>
<p>回想一下，在逻辑回归中，需要预测的结果,可以表示为，是我们熟悉的型函数  。我们约定  ，即算法的输出 是给定训练样本  条件下  等于1的概率。换句话说，如果，在给定训练样本  条件下等于；反过来说，如果，在给定训练样本条件下  等于1减去，因此，如果  代表  的概率，那么就是 的概率。接下来，我们就来分析这两个条件概率公式。</p>
<p><img src="http://www.ai-start.com/dl2017/images/c389bc9f882cf1a7d8ebbcf479b1b534.png" alt="img"></p>
<p>这两个条件概率公式定义形式为 并且代表了  或者  这两种情况，我们可以将这两个公式合并成一个公式。需要指出的是我们讨论的是二分类问题的损失函数，因此，的取值只能是0或者1。上述的两个条件概率公式可以合并成如下公式：</p>
<p>接下来我会解释为什么可以合并成这种形式的表达式：的次方这行表达式包含了上面的两个条件概率公式，我来解释一下为什么。</p>
<p><img src="http://www.ai-start.com/dl2017/images/09d86925e834ff665b6073fc2129fb57.png" alt="img"></p>
<p>第一种情况，假设 ，由于，那么，因为 的1次方等于，的指数项等于0，由于任何数的0次方都是1，乘以1等于。因此当时 （图中绿色部分）。</p>
<p>第二种情况，当  时  等于多少呢? 假设，的次方就是  的0次方，任何数的0次方都等于1，因此  ，前面假设  因此就等于1，因此 。因此在这里当时，。这就是这个公式(第二个公式，图中紫色字体部分)的结果。</p>
<p>因此，刚才的推导表明 ，就是  的完整定义。由于 log 函数是严格单调递增的函数，最大化  等价于最大化  并且地计算  的 log对数，就是计算  (其实就是将  代入)，通过对数函数化简为：</p>
<p>而这就是我们前面提到的损失函数的负数  ，前面有一个负号的原因是当你训练学习算法时需要算法输出值的概率是最大的（以最大的概率预测这个值），然而在逻辑回归中我们需要最小化损失函数，因此最小化损失函数与最大化条件概率的对数  关联起来了，因此这就是单个训练样本的损失函数表达式。</p>
<p><img src="http://www.ai-start.com/dl2017/images/5a3c2208dfbc9dbcfac57fde07dfac0e.png" alt="img"></p>
<p>在 个训练样本的整个训练集中又该如何表示呢，让我们一起来探讨一下。</p>
<p>让我们一起来探讨一下，整个训练集中标签的概率，更正式地来写一下。假设所有的训练样本服从同一分布且相互独立，也即独立同分布的，所有这些样本的联合概率就是每个样本概率的乘积:</p>
<p>。</p>
<p><img src="http://www.ai-start.com/dl2017/images/6ce9e585fab07fe9fc124dd2dc14bdbf.png" alt="img"></p>
<p>如果你想做最大似然估计，需要寻找一组参数，使得给定样本的观测值概率最大，但令这个概率最大化等价于令其对数最大化，在等式两边取对数：</p>
<p>在统计学里面，有一个方法叫做最大似然估计，即求出一组参数，使这个式子取最大值，也就是说，使得这个式子取最大值，，可以将负号移到求和符号的外面，，这样我们就推导出了前面给出的logistic回归的成本函数。</p>
<p><img src="http://www.ai-start.com/dl2017/images/6ce9e585fab07fe9fc124dd2dc14bdbf.png" alt="img"></p>
<p>由于训练模型时，目标是让成本函数最小化，所以我们不是直接用最大似然概率，要去掉这里的负号，最后为了方便，可以对成本函数进行适当的缩放，我们就在前面加一个额外的常数因子，即:。</p>
<p>总结一下，为了最小化成本函数，我们从<strong>logistic</strong>回归模型的最大似然估计的角度出发，假设训练集中的样本都是独立同分布的条件下。尽管这节课是选修性质的，但还是感谢观看本节视频。我希望通过本节课您能更好地明白逻辑回归的损失函数，为什么是那种形式，明白了损失函数的原理，希望您能继续完成课后的练习，前面课程的练习以及本周的测验，在课后的小测验和编程练习中,祝您好运。</p>
<h2 id="第三周：浅层神经网络-Shallow-neural-networks"><a href="#第三周：浅层神经网络-Shallow-neural-networks" class="headerlink" title="第三周：浅层神经网络(Shallow neural networks)"></a>第三周：浅层神经网络(Shallow neural networks)</h2><p>待续中。。。。。</p>
<h3 id="3-1-神经网络概述（Neural-Network-Overview）"><a href="#3-1-神经网络概述（Neural-Network-Overview）" class="headerlink" title="3.1 神经网络概述（Neural Network Overview）"></a>3.1 神经网络概述（Neural Network Overview）</h3><p>本周你将学习如何实现一个神经网络。在我们深入学习具体技术之前，我希望快速的带你预览一下本周你将会学到的东西。如果这个视频中的某些细节你没有看懂你也不用担心，我们将在后面的几个视频中深入讨论技术细节。</p>
<p>现在我们开始快速浏览一下如何实现神经网络。上周我们讨论了逻辑回归，我们了解了这个模型(见图3.1.1)如何与下面公式3.1建立联系。 图3.1.1 : <img src="http://www.ai-start.com/dl2017/images/L1_week3_1.png" alt="img"></p>
<p>公式3.1：</p>
<p>如上所示，首先你需要输入特征，参数和，通过这些你就可以计算出，公式3.2：</p>
<p>接下来使用就可以计算出。我们将的符号换为表示输出,然后可以计算出<strong>loss function</strong> </p>
<p>神经网络看起来是如下这个样子（图3.1.2）。正如我之前已经提到过，你可以把许多<strong>sigmoid</strong>单元堆叠起来形成一个神经网络。对于图3.1.1中的节点，它包含了之前讲的计算的两个步骤：首先通过公式3.1计算出值，然后通过计算值。</p>
<p><img src="http://www.ai-start.com/dl2017/images/L1_week3_2.png" alt="w600"></p>
<p>图3.1.2</p>
<p>在这个神经网络（图3.1.2）对应的3个节点，首先计算第一层网络中的各个节点相关的数，接着计算，在计算下一层网络同理； 我们会使用符号表示第层网络中节点相关的数，这些节点的集合被称为第层网络。这样可以保证不会和我们之前用来表示单个的训练样本的(即我们使用表示第i个训练样本)混淆； 整个计算过程，公式如下: 公式3.3：</p>
<p>公式3.4：</p>
<p>类似逻辑回归，在计算后需要使用计算，接下来你需要使用另外一个线性方程对应的参数计算， 计算，此时就是整个神经网络最终的输出，用 表示网络的输出。</p>
<p>公式3.5：</p>
<p>公式3.6：</p>
<p>我知道这其中有很多细节，其中有一点非常难以理解，即在逻辑回归中，通过直接计算得到结果。而这个神经网络中，我们反复的计算和，计算和，最后得到了最终的输出<strong>loss function</strong>。</p>
<p>你应该记得逻辑回归中，有一些从后向前的计算用来计算导数、。同样，在神经网络中我们也有从后向前的计算，看起来就像这样，最后会计算 、，计算出来之后，然后计算计算、 等，按公式3.5、3.6箭头表示的那样，从右到左反向计算。</p>
<p>现在你大概了解了一下什么是神经网络，基于逻辑回归重复使用了两次该模型得到上述例子的神经网络。我清楚这里面多了很多新符号和细节，如果没有理解也不用担心，在接下来的视频中我们会仔细讨论具体细节。</p>
<p>那么，下一个视频讲述神经网络的表示。</p>
<h3 id="3-2-神经网络的表示（Neural-Network-Representation）"><a href="#3-2-神经网络的表示（Neural-Network-Representation）" class="headerlink" title="3.2 神经网络的表示（Neural Network Representation）"></a>3.2 神经网络的表示（Neural Network Representation）</h3><p>先回顾一下我在上一个视频画几张神经网络的图片，在这次课中我们将讨论这些图片的具体含义，也就是我们画的这些神经网络到底代表什么。</p>
<p>我们首先关注一个例子，本例中的神经网络只包含一个隐藏层（图3.2.1）。这是一张神经网络的图片，让我们给此图的不同部分取一些名字。</p>
<p><img src="http://www.ai-start.com/dl2017/images/L1_week3_3.png" alt="img"></p>
<p>图3.2.1</p>
<p>我们有输入特征、、，它们被竖直地堆叠起来，这叫做神经网络的<strong>输入层</strong>。它包含了神经网络的输入；然后这里有另外一层我们称之为<strong>隐藏层</strong>（图3.2.1的四个结点）。待会儿我会回过头来讲解术语”隐藏”的意义；在本例中最后一层只由一个结点构成，而这个只有一个结点的层被称为<strong>输出层</strong>，它负责产生预测值。解释隐藏层的含义：在一个神经网络中，当你使用监督学习训练它的时候，训练集包含了输入也包含了目标输出，所以术语隐藏层的含义是在训练集中，这些中间结点的准确值我们是不知道到的，也就是说你看不见它们在训练集中应具有的值。你能看见输入的值，你也能看见输出的值，但是隐藏层中的东西，在训练集中你是无法看到的。所以这也解释了词语隐藏层，只是表示你无法在训练集中看到他们。</p>
<p>现在我们再引入几个符号，就像我们之前用向量表示输入特征。这里有个可代替的记号可以用来表示输入特征。表示激活的意思，它意味着网络中不同层的值会传递到它们后面的层中，输入层将传递给隐藏层，所以我们将输入层的激活值称为；下一层即隐藏层也同样会产生一些激活值，那么我将其记作，所以具体地，这里的第一个单元或结点我们将其表示为，第二个结点的值我们记为以此类推。所以这里的是一个四维的向量如果写成Python代码，那么它是一个规模为4x1的矩阵或一个大小为4的列向量，如下公式，它是四维的，因为在本例中，我们有四个结点或者单元，或者称为四个隐藏层单元； 公式3.7</p>
<p>最后输出层将产生某个数值，它只是一个单独的实数，所以的值将取为。这与逻辑回归很相似，在逻辑回归中，我们有直接等于，在逻辑回归中我们只有一个输出层，所以我们没有用带方括号的上标。但是在神经网络中，我们将使用这种带上标的形式来明确地指出这些值来自于哪一层，有趣的是在约定俗成的符号传统中，在这里你所看到的这个例子，只能叫做一个两层的神经网络（图3.2.2）。原因是当我们计算网络的层数时，输入层是不算入总层数内，所以隐藏层是第一层，输出层是第二层。第二个惯例是我们将输入层称为第零层，所以在技术上，这仍然是一个三层的神经网络，因为这里有输入层、隐藏层，还有输出层。但是在传统的符号使用中，如果你阅读研究论文或者在这门课中，你会看到人们将这个神经网络称为一个两层的神经网络，因为我们不将输入层看作一个标准的层。</p>
<p><img src="http://www.ai-start.com/dl2017/images/L1_week3_4.png" alt="w600"> 图3.2.2</p>
<p>最后，我们要看到的隐藏层以及最后的输出层是带有参数的，这里的隐藏层将拥有两个参数和，我将给它们加上上标(,)，表示这些参数是和第一层这个隐藏层有关系的。之后在这个例子中我们会看到是一个4x3的矩阵，而是一个4x1的向量，第一个数字4源自于我们有四个结点或隐藏层单元，然后数字3源自于这里有三个输入特征，我们之后会更加详细地讨论这些矩阵的维数，到那时你可能就更加清楚了。相似的输出层也有一些与之关联的参数以及。从维数上来看，它们的规模分别是1x4以及1x1。1x4是因为隐藏层有四个隐藏层单元而输出层只有一个单元，之后我们会对这些矩阵和向量的维度做出更加深入的解释，所以现在你已经知道一个两层的神经网络什么样的了，即它是一个只有一个隐藏层的神经网络。</p>
<p>在下一个视频中。我们将更深入地了解这个神经网络是如何进行计算的，也就是这个神经网络是怎么输入，然后又是怎么得到。</p>
<h3 id="3-3-计算一个神经网络的输出（Computing-a-Neural-Network’s-output）"><a href="#3-3-计算一个神经网络的输出（Computing-a-Neural-Network’s-output）" class="headerlink" title="3.3 计算一个神经网络的输出（Computing a Neural Network’s output）"></a>3.3 计算一个神经网络的输出（Computing a Neural Network’s output）</h3><p>在上一节的视频中，我们介绍只有一个隐藏层的神经网络的结构与符号表示。在这节的视频中让我们了解神经网络的输出究竟是如何计算出来的。</p>
<p>首先，回顾下只有一个隐藏层的简单两层<strong>神经网络结构</strong>：</p>
<p><img src="http://www.ai-start.com/dl2017/images/L1_week3_5.png" alt="w600"> 图3.3.1</p>
<p>其中，表示输入特征，表示每个神经元的输出，表示特征的权重，上标表示神经网络的层数（隐藏层为1），下标表示该层的第几个神经元。这是神经网络的<strong>符号惯例</strong>，下同。</p>
<p><strong>神经网络的计算</strong></p>
<p>关于神经网络是怎么计算的，从我们之前提及的逻辑回归开始，如下图所示。用圆圈表示神经网络的计算单元，逻辑回归的计算有两个步骤，首先你按步骤计算出，然后在第二步中你以<strong>sigmoid</strong>函数为激活函数计算（得出），一个神经网络只是这样子做了好多次重复计算。</p>
<p><img src="http://www.ai-start.com/dl2017/images/L1_week3_6.png" alt="img"> 图3.3.2</p>
<p>回到两层的神经网络，我们从隐藏层的第一个神经元开始计算，如上图第一个最上面的箭头所指。从上图可以看出，输入与逻辑回归相似，这个神经元的计算与逻辑回归一样分为两步，小圆圈代表了计算的两个步骤。</p>
<p>第一步，计算。</p>
<p>第二步，通过激活函数计算。</p>
<p>隐藏层的第二个以及后面两个神经元的计算过程一样，只是注意符号表示不同，最终分别得到、、，详细结果见下:</p>
<p><strong>向量化计算</strong> 如果你执行神经网络的程序，用for循环来做这些看起来真的很低效。所以接下来我们要做的就是把这四个等式向量化。向量化的过程是将神经网络中的一层神经元参数纵向堆积起来，例如隐藏层中的纵向堆积起来变成一个的矩阵，用符号表示。另一个看待这个的方法是我们有四个逻辑回归单元，且每一个逻辑回归单元都有相对应的参数——向量，把这四个向量堆积在一起，你会得出这4×3的矩阵。 因此， 公式3.8：     </p>
<p>公式3.9：</p>
<p>​        </p>
<p>详细过程见下: 公式3.10：</p>
<p>公式3.11：</p>
<p>对于神经网络的第一层，给予一个输入，得到，可以表示为。通过相似的衍生你会发现，后一层的表示同样可以写成类似的形式，得到，，具体过程见公式3.8、3.9。</p>
<p><img src="http://www.ai-start.com/dl2017/images/L1_week3_7.png" alt="w600"> 图3.3.3</p>
<p>如上图左半部分所示为神经网络，把网络左边部分盖住先忽略，那么最后的输出单元就相当于一个逻辑回归的计算单元。当你有一个包含一层隐藏层的神经网络，你需要去实现以计算得到输出的是右边的四个等式，并且可以看成是一个向量化的计算过程，计算出隐藏层的四个逻辑回归单元和整个隐藏层的输出结果，如果编程实现需要的也只是这四行代码。</p>
<p><strong>总结</strong> 通过本视频，你能够根据给出的一个单独的输入特征向量，运用四行代码计算出一个简单神经网络的输出。接下来你将了解的是如何一次能够计算出不止一个样本的神经网络输出，而是能一次性计算整个训练集的输出。</p>
<h3 id="3-4-多样本向量化（Vectorizing-across-multiple-examples）"><a href="#3-4-多样本向量化（Vectorizing-across-multiple-examples）" class="headerlink" title="3.4 多样本向量化（Vectorizing across multiple examples）"></a>3.4 多样本向量化（Vectorizing across multiple examples）</h3><p>在上一个视频，了解到如何针对于单一的训练样本，在神经网络上计算出预测值。</p>
<p>在这个视频，将会了解到如何向量化多个训练样本，并计算出结果。该过程与你在逻辑回归中所做类似。</p>
<p>逻辑回归是将各个训练样本组合成矩阵，对矩阵的各列进行计算。神经网络是通过对逻辑回归中的等式简单的变形，让神经网络计算出输出值。这种计算是所有的训练样本同时进行的，以下是实现它具体的步骤：</p>
<p><img src="http://www.ai-start.com/dl2017/images/L1_week3_8.png" alt="w800"> 图3.4.1</p>
<p>上一节视频中得到的四个等式。它们给出如何计算出，，，。</p>
<p>对于一个给定的输入特征向量，这四个等式可以计算出等于。这是针对于单一的训练样本。如果有个训练样本,那么就需要重复这个过程。</p>
<p>用第一个训练样本来计算出预测值，就是第一个训练样本上得出的结果。</p>
<p>然后，用来计算出预测值，循环往复，直至用计算出。</p>
<p>用激活函数表示法，如上图左下所示，它写成、和。</p>
<p>【注】：，是指第个训练样本而是指第二层。</p>
<p>如果有一个非向量化形式的实现，而且要计算出它的预测值，对于所有训练样本，需要让从1到实现这四个等式：</p>
<p>对于上面的这个方程中的，是所有依赖于训练样本的变量，即将添加到，和。如果想计算个训练样本上的所有输出，就应该向量化整个计算，以简化这列。</p>
<p>本课程需要使用很多线性代数的内容，重要的是能够正确地实现这一点，尤其是在深度学习的错误中。实际上本课程认真地选择了运算符号，这些符号只是针对于这个课程的，并且能使这些向量化容易一些。</p>
<p>所以，希望通过这个细节可以更快地正确实现这些算法。接下来讲讲如何向量化这些： 公式3.12：</p>
<p>公式3.13：</p>
<p>公式3.14：</p>
<p>公式3.15：</p>
<p>前一张幻灯片中的<strong>for</strong>循环是来遍历所有个训练样本。 定义矩阵等于训练样本，将它们组合成矩阵的各列，形成一个维或乘以维矩阵。接下来计算见公式3.15：</p>
<p>以此类推，从小写的向量到这个大写的矩阵，只是通过组合向量在矩阵的各列中。</p>
<p>同理，，等等都是的列向量，将所有都组合在各列中，就的到矩阵。</p>
<p>同理，，，……，将其组合在矩阵各列中，如同从向量到矩阵，以及从向量到矩阵一样，就能得到矩阵。</p>
<p>同样的，对于和，也是这样得到。</p>
<p>这种符号其中一个作用就是，可以通过训练样本来进行索引。这就是水平索引对应于不同的训练样本的原因，这些训练样本是从左到右扫描训练集而得到的。</p>
<p>在垂直方向，这个垂直索引对应于神经网络中的不同节点。例如，这个节点，该值位于矩阵的最左上角对应于激活单元，它是位于第一个训练样本上的第一个隐藏单元。它的下一个值对应于第二个隐藏单元的激活值。它是位于第一个训练样本上的，以及第一个训练示例中第三个隐藏单元，等等。</p>
<p>当垂直扫描，是索引到隐藏单位的数字。当水平扫描，将从第一个训练示例中从第一个隐藏的单元到第二个训练样本，第三个训练样本……直到节点对应于第一个隐藏单元的激活值，且这个隐藏单元是位于这m个训练样本中的最终训练样本。</p>
<p>从水平上看，矩阵A代表了各个训练样本。从竖直上看，矩阵A的不同的索引对应于不同的隐藏单元。</p>
<p>对于矩阵，情况也类似，水平方向上，对应于不同的训练样本；竖直方向上，对应不同的输入特征，而这就是神经网络输入层中各个节点。</p>
<p>神经网络上通过在多样本情况下的向量化来使用这些等式。</p>
<p>在下一个视频中，将证明为什么这是一种正确向量化的实现。这种证明将会与逻辑回归中的证明类似。</p>
<h3 id="3-5-向量化实现的解释（Justification-for-vectorized-implementation）"><a href="#3-5-向量化实现的解释（Justification-for-vectorized-implementation）" class="headerlink" title="3.5 向量化实现的解释（Justification for vectorized implementation）"></a>3.5 向量化实现的解释（Justification for vectorized implementation）</h3><p>在上一个视频中，我们学习到如何将多个训练样本横向堆叠成一个矩阵，然后就可以推导出神经网络中前向传播（forward propagation）部分的向量化实现。</p>
<p>在这个视频中，我们将会继续了解到，为什么上一节中写下的公式就是将多个样本向量化的正确实现。</p>
<p>我们先手动对几个样本计算一下前向传播，看看有什么规律： 公式3.16： </p>
<p>这里，为了描述的简便，我们先忽略掉 后面你将会看到利用<strong>Python</strong> 的广播机制，可以很容易的将 加进来。</p>
<p>现在  是一个矩阵，都是列向量，矩阵乘以列向量得到列向量，下面将它们用图形直观的表示出来: 公式3.17：</p>
<p>视频中，吴恩达老师很细心的用不同的颜色表示不同的样本向量，及其对应的输出。所以从图中可以看出，当加入更多样本时，只需向矩阵中加入更多列。</p>
<p>所以从这里我们也可以了解到，为什么之前我们对单个样本的计算要写成  这种形式，因为当有不同的训练样本时，将它们堆到矩阵的各列中，那么它们的输出也就会相应的堆叠到矩阵  的各列中。现在我们就可以直接计算矩阵  加上，因为列向量  和矩阵 的列向量有着相同的尺寸，而<strong>Python</strong>的广播机制对于这种矩阵与向量直接相加的处理方式是，将向量与矩阵的每一列相加。 所以这一节只是说明了为什么公式 是前向传播的第一步计算的正确向量化实现，但事实证明，类似的分析可以发现，前向传播的其它步也可以使用非常相似的逻辑，即如果将输入按列向量横向堆叠进矩阵，那么通过公式计算之后，也能得到成列堆叠的输出。</p>
<p>最后，对这一段视频的内容做一个总结:</p>
<p>由公式3.12、公式3.13、公式3.14、公式3.15可以看出，使用向量化的方法，可以不需要显示循环，而直接通过矩阵运算从就可以计算出 ，实际上可以记为 ，使用同样的方法就可以由神经网络中的每一层的输入  计算输出 。其实这些方程有一定对称性，其中第一个方程也可以写成，你看这对方程，还有这对方程形式其实很类似，只不过这里所有指标加了1。所以这样就显示出神经网络的不同层次，你知道大概每一步做的都是一样的，或者只不过同样的计算不断重复而已。这里我们有一个双层神经网络，我们在下周视频里会讲深得多的神经网络，你看到随着网络的深度变大，基本上也还是重复这两步运算，只不过是比这里你看到的重复次数更多。在下周的视频中将会讲解更深层次的神经网络，随着层数的加深，基本上也还是重复同样的运算。</p>
<p>以上就是对神经网络向量化实现的正确性的解释，到目前为止，我们仅使用<strong>sigmoid</strong>函数作为激活函数，事实上这并非最好的选择，在下一个视频中，将会继续深入的讲解如何使用更多不同种类的激活函数。</p>
<h3 id="3-6-激活函数（Activation-functions）"><a href="#3-6-激活函数（Activation-functions）" class="headerlink" title="3.6 激活函数（Activation functions）"></a>3.6 激活函数（Activation functions）</h3><p>使用一个神经网络时，需要决定使用哪种激活函数用隐藏层上，哪种用在输出节点上。到目前为止，之前的视频只用过<strong>sigmoid</strong>激活函数，但是，有时其他的激活函数效果会更好。</p>
<p>在神经网路的前向传播中，的和这两步会使用到<strong>sigmoid</strong>函数。<strong>sigmoid</strong>函数在这里被称为激活函数。 公式3.18： </p>
<p>更通常的情况下，使用不同的函数，可以是除了<strong>sigmoid</strong>函数意外的非线性函数。<strong>tanh</strong>函数或者双曲正切函数是总体上都优于<strong>sigmoid</strong>函数的激活函数。</p>
<p>如图，的值域是位于+1和-1之间。 公式3.19： </p>
<p>事实上，<strong>tanh</strong>函数是<strong>sigmoid</strong>的向下平移和伸缩后的结果。对它进行了变形后，穿过了点，并且值域介于+1和-1之间。</p>
<p>结果表明，如果在隐藏层上使用函数 公式3.20：  效果总是优于<strong>sigmoid</strong>函数。因为函数值域在-1和+1的激活函数，其均值是更接近零均值的。在训练一个算法模型时，如果使用<strong>tanh</strong>函数代替<strong>sigmoid</strong>函数中心化数据，使得数据的平均值更接近0而不是0.5.</p>
<p>这会使下一层学习简单一点，在第二门课中会详细讲解。</p>
<p>在讨论优化算法时，有一点要说明：我基本已经不用<strong>sigmoid</strong>激活函数了，<strong>tanh</strong>函数在所有场合都优于<strong>sigmoid</strong>函数。</p>
<p>但有一个例外：在二分类的问题中，对于输出层，因为的值是0或1，所以想让的数值介于0和1之间，而不是在-1和+1之间。所以需要使用<strong>sigmoid</strong>激活函数。这里的 公式3.21：  在这个例子里看到的是，对隐藏层使用<strong>tanh</strong>激活函数，输出层使用<strong>sigmoid</strong>函数。</p>
<p>所以，在不同的神经网络层中，激活函数可以不同。为了表示不同的激活函数，在不同的层中，使用方括号上标来指出上标为的激活函数，可能会跟上标为不同。方括号上标代表隐藏层，方括号上标表示输出层。</p>
<p><strong>sigmoid</strong>函数和<strong>tanh</strong>函数两者共同的缺点是，在特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度。</p>
<p>在机器学习另一个很流行的函数是：修正线性单元的函数（<strong>ReLu</strong>），<strong>ReLu</strong>函数图像是如下图。 公式3.22：  所以，只要是正值的情况下，导数恒等于1，当是负值的时候，导数恒等于0。从实际上来说，当使用的导数时，=0的导数是没有定义的。但是当编程实现的时候，的取值刚好等于0.0000000，这个值相当小，所以，在实践中，不需要担心这个值，是等于0的时候，假设一个导数是1或者0效果都可以。</p>
<p>这有一些选择激活函数的经验法则：</p>
<p>如果输出是0、1值（二分类问题），则输出层选择<strong>sigmoid</strong>函数，然后其它的所有单元都选择<strong>Relu</strong>函数。</p>
<p>这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用Relu激活函数。有时，也会使用<strong>tanh</strong>激活函数，但<strong>Relu</strong>的一个优点是：当是负值的时候，导数等于0。</p>
<p>这里也有另一个版本的<strong>Relu</strong>被称为<strong>Leaky Relu</strong>。</p>
<p>当是负值时，这个函数的值不是等于0，而是轻微的倾斜，如图。</p>
<p>这个函数通常比<strong>Relu</strong>激活函数效果要好，尽管在实际中<strong>Leaky ReLu</strong>使用的并不多。</p>
<p><img src="http://www.ai-start.com/dl2017/images/L1_week3_9.jpg" alt="w600"> 图3.6.1</p>
<p>两者的优点是：</p>
<p>第一，在的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个<strong>if-else</strong>语句，而<strong>sigmoid</strong>函数需要进行浮点四则运算，在实践中，使用<strong>ReLu</strong>激活函数神经网络通常会比使用<strong>sigmoid</strong>或者<strong>tanh</strong>激活函数学习的更快。</p>
<p>第二，<strong>sigmoid</strong>和<strong>tanh</strong>函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，而<strong>Relu</strong>和<strong>Leaky ReLu</strong>函数大于0部分都为常熟，不会产生梯度弥散现象。(同时应该注意到的是，<strong>Relu</strong>进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而<strong>Leaky ReLu</strong>不会有这问题)</p>
<p>在<strong>ReLu</strong>的梯度一半都是0，但是，有足够的隐藏层使得z值大于0，所以对大多数的训练数据来说学习过程仍然可以很快。</p>
<p>快速概括一下不同激活函数的过程和结论。</p>
<p><strong>sigmoid</strong>激活函数：除了输出层是一个二分类问题基本不会用它。</p>
<p><strong>tanh</strong>激活函数：<strong>tanh</strong>是非常优秀的，几乎适合所有场合。</p>
<p><strong>ReLu</strong>激活函数：最常用的默认函数，，如果不确定用哪个激活函数，就使用<strong>ReLu</strong>或者<strong>Leaky ReLu</strong>。公式3.23：  为什么常数是0.01？当然，可以为学习算法选择不同的参数。</p>
<p>在选择自己神经网络的激活函数时，有一定的直观感受，在深度学习中的经常遇到一个问题：在编写神经网络的时候，会有很多选择：隐藏层单元的个数、激活函数的选择、初始化权值……这些选择想得到一个对比较好的指导原则是挺困难的。</p>
<p>鉴于以上三个原因，以及在工业界的见闻，提供一种直观的感受，哪一种工业界用的多，哪一种用的少。但是，自己的神经网络的应用，以及其特殊性，是很难提前知道选择哪些效果更好。所以通常的建议是：如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者发展集上进行评价。然后看哪一种表现的更好，就去使用它。</p>
<p>为自己的神经网络的应用测试这些不同的选择，会在以后检验自己的神经网络或者评估算法的时候，看到不同的效果。如果仅仅遵守使用默认的<strong>ReLu</strong>激活函数，而不要用其他的激励函数，那就可能在近期或者往后，每次解决问题的时候都使用相同的办法。</p>
<h3 id="3-7-为什么需要非线性激活函数？（why-need-a-nonlinear-activation-function-）"><a href="#3-7-为什么需要非线性激活函数？（why-need-a-nonlinear-activation-function-）" class="headerlink" title="3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）"></a>3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）</h3><p>为什么神经网络需要非线性激活函数？事实证明：要让你的神经网络能够计算出有趣的函数，你必须使用非线性激活函数，证明如下：</p>
<p>这是神经网络正向传播的方程，现在我们去掉函数，然后令，或者我们也可以令，这个有时被叫做线性激活函数（更学术点的名字是恒等激励函数，因为它们就是把输入值输出）。为了说明问题我们把，那么这个模型的输出或仅仅只是输入特征的线性组合。</p>
<p>如果我们改变前面的式子，令： (1) </p>
<p>(2)  将式子(1)代入式子(2)中，则： </p>
<p>(3)  简化多项式得  如果你是用线性激活函数或者叫恒等激励函数，那么神经网络只是把输入线性组合再输出。</p>
<p>我们稍后会谈到深度网络，有很多层的神经网络，很多隐藏层。事实证明，如果你使用线性激活函数或者没有使用一个激活函数，那么无论你的神经网络有多少层一直在做的只是计算线性函数，所以不如直接去掉全部隐藏层。在我们的简明案例中，事实证明如果你在隐藏层用线性激活函数，在输出层用<strong>sigmoid</strong>函数，那么这个模型的复杂度和没有任何隐藏层的标准<strong>Logistic</strong>回归是一样的，如果你愿意的话，可以证明一下。</p>
<p>在这里线性隐层一点用也没有，因为这两个线性函数的组合本身就是线性函数，所以除非你引入非线性，否则你无法计算更有趣的函数，即使你的网络层数再多也不行；只有一个地方可以使用线性激活函数——，就是你在做机器学习中的回归问题。 是一个实数，举个例子，比如你想预测房地产价格， 就不是二分类任务0或1，而是一个实数，从0到正无穷。如果 是个实数，那么在输出层用线性激活函数也许可行，你的输出也是一个实数，从负无穷到正无穷。</p>
<p>总而言之，不能在隐藏层用线性激活函数，可以用<strong>ReLU</strong>或者<strong>tanh</strong>或者<strong>leaky ReLU</strong>或者其他的非线性激活函数，唯一可以用线性激活函数的通常就是输出层；除了这种情况，会在隐层用线性函数的，除了一些特殊情况，比如与压缩有关的，那方面在这里将不深入讨论。在这之外，在隐层使用线性激活函数非常少见。因为房价都是非负数，所以我们也可以在输出层使用<strong>ReLU</strong>函数这样你的都大于等于0。</p>
<p>理解为什么使用非线性激活函数对于神经网络十分关键，接下来我们讨论梯度下降，并在下一个视频中开始讨论梯度下降的基础——激活函数的导数。</p>
<h3 id="3-8-激活函数的导数（Derivatives-of-activation-functions）"><a href="#3-8-激活函数的导数（Derivatives-of-activation-functions）" class="headerlink" title="3.8 激活函数的导数（Derivatives of activation functions）"></a>3.8 激活函数的导数（Derivatives of activation functions）</h3><p>在神经网络中使用反向传播的时候，你真的需要计算激活函数的斜率或者导数。针对以下四种激活，求其导数如下：</p>
<p>1）<strong>sigmoid activation function</strong></p>
<p><img src="http://www.ai-start.com/dl2017/images/L1_week3_10.png" alt="w600"> 图3.8.1</p>
<p>其具体的求导如下： 公式3.25： </p>
<p>注：</p>
<p>当 = 10或  </p>
<p>当= 0  </p>
<p>在神经网络中; </p>
<p>2) <strong>Tanh activation function</strong></p>
<p><img src="http://www.ai-start.com/dl2017/images/L1_week3_11.png" alt="w600"> 图3.8.2</p>
<p>其具体的求导如下： 公式3.26： </p>
<p>公式3.27：  注：</p>
<p>当 = 10或  </p>
<p>当 = 0  </p>
<p>在神经网络中;</p>
<p>3）<strong>Rectified Linear Unit (ReLU)</strong></p>
<p><img src="http://www.ai-start.com/dl2017/images/L1_week3_12.png" alt="w600"> </p>
<p>注：通常在= 0的时候给定其导数1,0；当然=0的情况很少</p>
<p>4）<strong>Leaky linear unit (Leaky ReLU)</strong></p>
<p>与<strong>ReLU</strong>类似</p>
<p>注：通常在的时候给定其导数1,0.01；当然的情况很少</p>
<h3 id="3-9-神经网络的梯度下降（Gradient-descent-for-neural-networks）"><a href="#3-9-神经网络的梯度下降（Gradient-descent-for-neural-networks）" class="headerlink" title="3.9 神经网络的梯度下降（Gradient descent for neural networks）"></a>3.9 神经网络的梯度下降（Gradient descent for neural networks）</h3><p>在这个视频中，我会给你实现反向传播或者说梯度下降算法的方程组，在下一个视频我们会介绍为什么这几个特定的方程是针对你的神经网络实现梯度下降的正确方程。</p>
<p>你的单隐层神经网络会有，，，这些参数，还有个表示输入特征的个数，表示隐藏单元个数，表示输出单元个数。</p>
<p>在我们的例子中，我们只介绍过的这种情况，那么参数:</p>
<p>矩阵的维度就是()，就是维向量，可以写成，就是一个的列向量。 矩阵的维度就是()，的维度就是维度。</p>
<p>你还有一个神经网络的成本函数，假设你在做二分类任务，那么你的成本函数等于：</p>
<p><strong>Cost function</strong>: 公式3.28：  <strong>loss function</strong>和之前做<strong>logistic</strong>回归完全一样。</p>
<p>训练参数需要做梯度下降，在训练神经网络的时候，随机初始化参数很重要，而不是初始化成全零。当你参数初始化成某些值后，每次梯度下降都会循环计算以下预测值：</p>
<p> 公式3.28：  公式3.29： </p>
<p>其中</p>
<p>公式3.30： </p>
<p>公式3.31：  正向传播方程如下（之前讲过）： <strong>forward propagation</strong>： (1)  (2)  (3)  (4) </p>
<p>反向传播方程如下:</p>
<p><strong>back propagation</strong>： 公式3.32：  公式3.33：  公式3.34：  公式3.35：  公式3.36：  公式3.37： </p>
<p>上述是反向传播的步骤，注：这些都是针对所有样本进行过向量化，是的矩阵；这里<code>np.sum</code>是python的numpy命令，<code>axis=1</code>表示水平相加求和，<code>keepdims</code>是防止python输出那些古怪的秩数，加上这个确保阵矩阵这个向量输出的维度为这样标准的形式。 </p>
<p>目前为止，我们计算的都和<strong>Logistic</strong>回归十分相似，但当你开始计算反向传播时，你需要计算，是隐藏层函数的导数，输出在使用<strong>sigmoid</strong>函数进行二元分类。这里是进行逐个元素乘积，因为和这两个都为矩阵；</p>
<p>还有一种防止<strong>python</strong>输出奇怪的秩数，需要显式地调用<code>reshape</code>把<code>np.sum</code>输出结果写成矩阵形式。</p>
<p>以上就是正向传播的4个方程和反向传播的6个方程，这里我是直接给出的，在下个视频中，我会讲如何导出反向传播的这6个式子的。如果你要实现这些算法，你必须正确执行正向和反向传播运算，你必须能计算所有需要的导数，用梯度下降来学习神经网络的参数；你也可以许多成功的深度学习从业者一样直接实现这个算法，不去了解其中的知识。</p>
<h3 id="3-10（选修）直观理解反向传播（Backpropagation-intuition）"><a href="#3-10（选修）直观理解反向传播（Backpropagation-intuition）" class="headerlink" title="3.10（选修）直观理解反向传播（Backpropagation intuition）"></a>3.10（选修）直观理解反向传播（Backpropagation intuition）</h3><p>这个视频主要是推导反向传播。</p>
<p>下图是逻辑回归的推导：</p>
<p>回想一下逻辑回归的公式(参考公式3.2、公式3.5、公式3.6、公式3.15) 公式3.38：</p>
<p>所以回想当时我们讨论逻辑回归的时候，我们有这个正向传播步骤，其中我们计算，然后，然后损失函数。</p>
<p>公式3.39：</p>
<p>神经网络的计算中，与逻辑回归十分类似，但中间会有多层的计算。下图是一个双层神经网络，有一个输入层，一个隐藏层和一个输出层。</p>
<p>前向传播：</p>
<p>计算，，再计算，，最后得到<strong>loss function</strong>。</p>
<p>反向传播：</p>
<p>向后推算出，然后推算出，接着推算出，然后推算出。我们不需要对求导，因为是固定的，我们也不是想优化。向后推算出，然后推算出的步骤可以合为一步： 公式3.40： ， (注意：逻辑回归中；为什么多了个转置：中的(视频里是)是一个列向量，而是个行向量，故需要加个转置); 公式3.41：  公式3.42：  注意：这里的矩阵：的维度是：。</p>
<p> ， 的维度都是：，如果是二分类，那维度就是。</p>
<p>，的维度都是：。</p>
<p>证明过程： 见公式3.42，其中dz^{[2]}维度为：(n^{[1]},n^{[2]})、(n^{[2]},1)相乘得到(n^{[1]},1)，和z^{[1]}$维度相同，</p>
<p>的维度为，这就变成了两个都是向量逐元素乘积。</p>
<p>实现后向传播有个技巧，就是要保证矩阵的维度相互匹配。最后得到和，公式3.43： </p>
<p>可以看出 和 非常相似，其中扮演了的角色， 等同于。</p>
<p>由：  得到： </p>
<p>注意：大写的表示的列向量堆叠成的矩阵，以下类同。</p>
<p>下图写了主要的推导过程： 公式3.44： ， 公式3.45：  公式3.46：  公式3.47：  公式3.48：  公式3.49： </p>
<p>吴恩达老师认为反向传播的推导是机器学习领域最难的数学推导之一，矩阵的导数要用链式法则来求，如果这章内容掌握不了也没大的关系，只要有这种直觉就可以了。还有一点，就是初始化你的神经网络的权重，不要都是0，而是随机初始化，下一章将详细介绍原因。</p>
<h3 id="3-11-随机初始化（Random-Initialization）"><a href="#3-11-随机初始化（Random-Initialization）" class="headerlink" title="3.11 随机初始化（Random+Initialization）"></a>3.11 随机初始化（Random+Initialization）</h3><p>当你训练神经网络时，权重随机初始化是很重要的。对于逻辑回归，把权重初始化为0当然也是可以的。但是对于一个神经网络，如果你把权重或者参数都初始化为0，那么梯度下降将不会起作用。</p>
<p>让我们看看这是为什么。有两个输入特征，，2个隐藏层单元就等于2。 因此与一个隐藏层相关的矩阵，或者说是2_2的矩阵，假设把它初始化为0的2_2矩阵，也等于 ，把偏置项初始化为0是合理的，但是把初始化为0就有问题了。 那这个问题如果按照这样初始化的话，你总是会发现 和 相等，这个激活单元和这个激活单元就会一样。因为两个隐含单元计算同样的函数，当你做反向传播计算时，这会导致和 也会一样，对称这些隐含单元会初始化得一样，这样输出的权值也会一模一样，由此等于；</p>
<p><img src="http://www.ai-start.com/dl2017/images/L1_week3_13.png" alt="w600"></p>
<p>图3.11.1 但是如果你这样初始化这个神经网络，那么这两个隐含单元就会完全一样，因此他们完全对称，也就意味着计算同样的函数，并且肯定的是最终经过每次训练的迭代，这两个隐含单元仍然是同一个函数，令人困惑。会是一个这样的矩阵，每一行有同样的值因此我们做权重更新把权重每次迭代后的，第一行等于第二行。</p>
<p>由此可以推导，如果你把权重都初始化为0，那么由于隐含单元开始计算同一个函数，所有的隐含单元就会对输出单元有同样的影响。一次迭代后同样的表达式结果仍然是相同的，即隐含单元仍是对称的。通过推导，两次、三次、无论多少次迭代，不管你训练网络多长时间，隐含单元仍然计算的是同样的函数。因此这种情况下超过1个隐含单元也没什么意义，因为他们计算同样的东西。当然更大的网络，比如你有3个特征，还有相当多的隐含单元。</p>
<p>如果你要初始化成0，由于所有的隐含单元都是对称的，无论你运行梯度下降多久，他们一直计算同样的函数。这没有任何帮助，因为你想要两个不同的隐含单元计算不同的函数，这个问题的解决方法就是随机初始化参数。你应该这么做：把设为<code>np.random.randn(2,2)</code>(生成高斯分布)，通常再乘上一个小的数，比如0.01，这样把它初始化为很小的随机数。然后没有这个对称的问题（叫做<strong>symmetry breaking problem</strong>），所以可以把  初始化为0，因为只要随机初始化你就有不同的隐含单元计算不同的东西，因此不会有<strong>symmetry breaking</strong>问题了。相似的，对于你可以随机初始化，可以初始化为0。</p>
<p>你也许会疑惑，这个常数从哪里来，为什么是0.01，而不是100或者1000。我们通常倾向于初始化为很小的随机数。因为如果你用<strong>tanh</strong>或者<strong>sigmoid</strong>激活函数，或者说只在输出层有一个<strong>Sigmoid</strong>，如果（数值）波动太大，当你计算激活值时如果很大，就会很大。的一些值就会很大或者很小，因此这种情况下你很可能停在<strong>tanh</strong>/<strong>sigmoid</strong>函数的平坦的地方(见图3.8.2)，这些地方梯度很小也就意味着梯度下降会很慢，因此学习也就很慢。</p>
<p>回顾一下：如果很大，那么你很可能最终停在（甚至在训练刚刚开始的时候）很大的值，这会造成<strong>tanh</strong>/<strong>Sigmoid</strong>激活函数饱和在龟速的学习上，如果你没有<strong>sigmoid</strong>/<strong>tanh</strong>激活函数在你整个的神经网络里，就不成问题。但如果你做二分类并且你的输出单元是<strong>Sigmoid</strong>函数，那么你不会想让初始参数太大，因此这就是为什么乘上0.01或者其他一些小数是合理的尝试。对于一样，就是<code>np.random.randn((1,2))</code>，我猜会是乘以0.01。</p>
<p>事实上有时有比0.01更好的常数，当你训练一个只有一层隐藏层的网络时（这是相对浅的神经网络，没有太多的隐藏层），设为0.01可能也可以。但当你训练一个非常非常深的神经网络，你可能会选择一个不同于的常数而不是0.01。下一节课我们会讨论怎么并且何时去选择一个不同于0.01的常数，但是无论如何它通常都会是个相对小的数。</p>
<p>好了，这就是这周的视频。你现在已经知道如何建立一个一层的神经网络了，初始化参数，用前向传播预测，还有计算导数，结合反向传播用在梯度下降中。</p>
<p>站长统计</p>
<h1 id="人工智能大师实录"><a href="#人工智能大师实录" class="headerlink" title="人工智能大师实录"></a>人工智能大师实录</h1><h2 id="Geoffery-Hinton-专访"><a href="#Geoffery-Hinton-专访" class="headerlink" title="Geoffery Hinton 专访"></a>Geoffery Hinton 专访</h2><p>Hinton 的专访让我印象很深刻，不仅仅是他是深度学习教父一般的存在，他的传奇求学经历和时刻保持着新颖的思想让人印象深刻。这次 Ng 的专访实例文本可见：<a href="http://www.ai-start.com/dl2017/interview.html" target="_blank" rel="noopener">XX</a></p>
<p><img src="https://i.loli.net/2018/02/15/5a852baf8b7dc.png" alt=""></p>
<blockquote>
<p><strong>Geoffrey Hinton：</strong>当我还在高中时有一个什么都比我强的同学，他是个才华横溢的数学家，有天他来学校并且问我，你知道大脑是用全息图运作的吗？那时应该是1966年，我回答他：全息图是个啥？他就解释了一下，在全息图中，你可以切掉它的一半，但依然了解得到全貌，还有大脑中的记忆可能是分布于整个大脑中的，我大概猜到他可能是读过关于Karl Lashley的实验，其中讲到切掉老鼠几个小部分的脑子，然后发现很难找到哪一部分存储哪种特别的记忆。。。</p>
</blockquote>
<p>一个自负的家伙跑来卖弄自己还一知半解的见识。。。这种现象在学术圈屡见不鲜。</p>
<blockquote>
<p><strong>Geoffrey Hinton</strong>：。。。大脑怎么储存记忆产生兴趣。所以当我去上大学的时候，我就学习<strong>生理学</strong>和<strong>物理学</strong>，当我在剑桥的时候我是唯一一个在学生理学和物理学的本科生，之后我放弃了这个选择并且尝试学<strong>哲学</strong>，因为我觉得那可能会给我更多的深入了解，但是后来我又觉得<strong>缺乏真正能够辨别错误说法的方法</strong>，然后我就转去学了<strong>心理学</strong>而在心理学中有着非常非常过于简单的理论，对我个人来说用来解释大脑的运作看起来无可救药的不充分，之后我花了点时间做一个<strong>木匠</strong>，然后我又决定想去试试看人工智能，于是就跑去爱丁堡跟Longuet Higgins学人工智能，他已经做了很棒的关于神经网络的研究并且刚刚决定放弃于此，转而对Terry Winograd的学说表示赞赏，我刚去的时候他觉得我做这个（神经网络）已经过时了，应该开始搞符号主义人工智能，关于这个我们有很多争论，但我还是坚持做自己相信的事情，然后呢？最终我拿到了<strong>人工智能博士学位</strong>。但我在英国找不到工作。。。</p>
</blockquote>
<p>同时学习生理和物理，这在现在也几乎是不可能发生的事情；哲学这个学科也是挺醉人的，被评价为“缺乏真正能够辨别错误说法的方法”也不足为奇，毕竟是物理系出来的人嘛；心理学是最让外行人朝思幻想的学科了，被评价为过于简单也可以理解，毕竟没法与数学和物理里建立的庞大公理体系相提并论；最后，在导师不支持的 PhD 方向上最后让Hinton毕业了，这也算是听上去很不可思议，但是现今又屡见不鲜的现象。至于博士毕业会找不到工作。。。。似乎对应了自己的现状吧。。。。</p>
<blockquote>
<p><strong>Geoffrey Hinton：</strong>最后我们的论文（反向传播算法）上了《自然》，为了论文被接受，我做了不少<strong>人事工作</strong>，我想到其中一个审稿人很可能会是Stuart Sutherland英国一位很有名的心理学家，我跑去和他聊了很久，跟他解释这到底是怎么一回事，给他留下了很深刻的印象，因为我们给他展示了反向传播法可以学习字元表示，… 等等，这震惊了Stuart Sutherland，我想，这是论文被通过的原因。</p>
</blockquote>
<p>Hinton 著名的 backprop 算法不是全新的想法，但他使得这个算法得到了大推广，关键得益于他大力的<strong>游说</strong> reviewer 让他的文章发在了 nature 上。我想说的是：在科研学术界，类似的现象也可谓见怪不怪，习以为常，人面对真相和客观真理时，并不能保证有足够的能力和信心看到事物背后的本质，于是承载主观意志的说服力和修辞技巧就成了人必不可少的内禀品质，这亦可谓是人性的弱点之一吧。</p>
<blockquote>
<p><strong>Geoffrey Hinton：</strong>我认为最具学术之美的是我和Terry Sejnowski做的<strong>Boltzmann机</strong>，我们发现它能用非常非常简单的学习算法去应用到密度很高的连接起来的网络…..</p>
</blockquote>
<p>深度信念网络……</p>
<blockquote>
<p><strong>Geoffrey Hinton：</strong>… 我们在受限Boltzmann机上花功夫证明了，这是其中之一推进ReLU前进的力量。… 我显然已经知道ReLU还有logistic单元，由于我花了心血在Boltzmann机上，全都是用的logistic单元，那时候面临的问题是，这个学习算法可能用在ReLU吗，证明完<strong>ReLU几乎等同一叠logistic单元</strong>后，我们展示了所有的数学证明。。。。2014年，我在Google讲ReLU的用法以及怎么用单位矩阵初始化，因为ReLU的一大优点是，如果不断复制隐藏层，又用单位矩阵初始化，它会复制下层的模式。。。。</p>
</blockquote>
<p>出于好奇，我找到了这个证明的 paper：<a href="http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf" target="_blank" rel="noopener">Rectified linear units improve restricted Boltzmann machines</a>，同时，还无意间找到了一个编年体格式讲述ReLU激活函数：<a href="http://www.gabormelli.com/RKB/Rectified_Linear_Unit_(ReLU" target="_blank" rel="noopener">Gabormelli</a>_Activation_Function#cite_note-nair2010-4)。</p>
<blockquote>
<p><strong>吴恩达：</strong>你应该常常被问到，如果有人想要入门深度学习，该做什么，你有什么建议吗？对于要学深度学习的人们，你有什么样的建议？</p>
<p><strong>Geoffrey Hinton：</strong>好，我的建议是多读论文，但别读太多，我从导师那里得到这个建议，很不像大多数人说的，大多数人会告诉你尽量多读，然后开始自己的研究，对一些研究人员应该是正确的，但是对<strong>有创意的人</strong>应该读一少部分，然后发现一点你认为所有人都错了的东西，在这点我一般都逆着来，你看到它，感觉不太对，然后想怎么才能做对，当人们反对你时，要坚持自我，我支持人们坚持自我的原则，是判断直觉的对错，你直觉还不错的话，就该坚持，最后一定会成功，要是你直觉一般的话，做啥都无所谓。</p>
</blockquote>
<p>相信直觉，不相信就没意义了。。。。</p>
<blockquote>
<p><strong>Geoffrey Hinton：</strong>另一个建议是，<strong>永远不要停止编程</strong>，因为如果你给学生布置任务，他们三天打鱼两天晒网，回头就会告诉你看，没做成，没做成的原因，往往是他们所做的小决定，当时不觉得很重要。举个例子，如果你给一个好学生以任务，你可以给他任何任务，他都会做成。我记得曾经有一次，我说。诶等等，我们上次讨论时，因为某某原因，是不可能成功呀，学生回答说：“对呀，你说完我就发现了，就假设不是你真的这个意思”。。。。我认为基本上，<strong>开始锻炼直觉时要读够</strong>，<strong>然后相信直觉，自己动手</strong>，不要担心别人有反对意见。。。。<strong>如果你有个绝好的想法，别人都觉得完全荒谬，那你就找对东西了</strong>。举个例子，当我刚想出来变分法时，我给之前一个叫Peter Brown的学生写了封信，他懂得很多EN相关知识，他就拿去给一起工作的人看，名字叫俩兄弟，可能是双胞胎吧，然后他说，俩兄弟说了，你要么是你喝多了，要么是傻，俩兄弟当真认为是荒谬之论，部分原因可能是我用的解释方式，因为我只解释了直觉，但当你有个很不错的想法时，其他人觉得完全是垃圾，就是个好想法的信号了。</p>
</blockquote>
<p>这些建议就非常干货了！不要停止编程！记得去年隔了仅4个月没有敲 Python 代码，于是后来还要重头来熟悉。。。。</p>
<blockquote>
<p><strong>Geoffrey Hinton：</strong>对新研究生的一个好建议是，<strong>找一个和你意见一致的导师</strong>，因为如果你做的东西，导师也深深赞同，你会得到很好的建议，要是做你导师不感兴趣的东西，你会得到没啥用的建议。</p>
</blockquote>
<p>这个经验我深深的感到苟同。。。此处省略几千字。。。。</p>
<blockquote>
<p><strong>Geoffrey Hinton：</strong>（顶级公司）大多数部门很少有真正懂得这场革命的人，我基本上同意，这并不是二次工业革命，但是规模接近。有如此巨大的改变，基本是因为<strong>我们和计算机的关系发生了改变</strong>，<strong>不再只是编程序，而是让它们有能力自动解决问题，从根本上改变了计算机的用法，计算机科学部门，却是在之前基础上建立起来的</strong>，他们暂且还不懂训练计算机会和编程一样重要，公司部门中de 一半的人得实际去试过训练计算机。。。</p>
</blockquote>
<p>这一段评价真的解释了我一直以来的一个疑问，我接触过好多在努力入门和学习AI的程序员学员，而在交流的过程中发觉，有相当一部分朋友在入门的过程中并没有如我意料中的比非计算机编程背景的人入门更快，更容易理解 Python 编程以及 ML 和 DL 的知识原理。</p>
<blockquote>
<p><strong>Geoffrey Hinton：</strong>AI早期时，人们完全被说服为智力的表示该是某种符号表达，比较整洁的逻辑，而不完全是逻辑，但是类似逻辑，智力的本质是推理。然而，对比这种符号化的表示，我认为那些把 AI 想成是符号表达的人，大错特错。我认为其该是一个大向量，<strong>有因果能力的大向量，能引发出其他大向量，这于 AI 的标准观点</strong> ，符号化表达完全不同。</p>
</blockquote>
<p>Hinton 最后在这里的描述让我印象深刻，符号化的理解智能正是理论家最爱用也是最常用的方式，毕竟他们很不乐意把逻辑推理彻底摒弃掉，不过相信当我们构架好 DNN 的数据理论基础后，就能够真正彻底理解深度学习了吧。</p>
</section>
    <!-- Tags START -->
    
      <div class="tags">
        <span>Tags:</span>
        
  <a href="/tags#品味DeepLearning.ai" >
    <span class="tag-code">品味DeepLearning.ai</span>
  </a>

      </div>
    
    <!-- Tags END -->
    <!-- NAV START -->
    
  <div class="nav-container">
    <!-- reverse left and right to put prev and next in a more logic postition -->
    
      <a class="nav-left" href="/2018/02/cn231n_MLP5/">
        <span class="nav-arrow">← </span>
        
          一段关于神经网络的故事：最终章！
        
      </a>
    
    
      <a class="nav-right" href="/2018/02/CS231n_MXNet1/">
        
          CS231n 与 MXNet 实现：图像分类
        
        <span class="nav-arrow"> →</span>
      </a>
    
  </div>

    <!-- NAV END -->
    <!-- 打赏 START -->
    
      <div class="money-like">
        <div class="reward-btn">
          赏
          <span class="money-code">
            <span class="alipay-code">
              <div class="code-image"></div>
              <b>使用支付宝打赏</b>
            </span>
            <span class="wechat-code">
              <div class="code-image"></div>
              <b>使用微信打赏</b>
            </span>
          </span>
        </div>
        <p class="notice">若你觉得我的文章对你有帮助，欢迎点击上方按钮对我打赏</p>
      </div>
    
    <!-- 打赏 END -->
    <!-- 二维码 START -->
    
      <div class="qrcode">
        <canvas id="share-qrcode"></canvas>
        <p class="notice">扫描二维码，分享此文章</p>
      </div>
    
    <!-- 二维码 END -->
    
      <!-- Gitment START -->
      <div id="comments"></div>
      <!-- Gitment END -->
    
  </article>
  <!-- Article END -->
  <!-- Catalog START -->
  
    <aside class="catalog-container">
  <div class="toc-main">
    <strong class="toc-title">Catalog</strong>
    
      <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#1-深度学习引言"><span class="toc-nav-text">1. 深度学习引言</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#1-1-1-2-欢迎-amp-什么是神经网络？"><span class="toc-nav-text">1.1-1.2 欢迎 &amp; 什么是神经网络？</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#1-3-神经网络的监督学习"><span class="toc-nav-text">1.3 神经网络的监督学习</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#1-4-1-6-为什么深度学习会兴起？-amp-关于本课程-amp-课程资源"><span class="toc-nav-text">1.4-1.6 为什么深度学习会兴起？&amp; 关于本课程 &amp; 课程资源</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#2-神经网络的编程基础"><span class="toc-nav-text">2. 神经网络的编程基础</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-1-二分类"><span class="toc-nav-text">2.1 二分类</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-2-逻辑回归-Logistic-Regression"><span class="toc-nav-text">2.2 逻辑回归(Logistic Regression)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-3-逻辑回归的代价函数（Logistic-Regression-Cost-Function）"><span class="toc-nav-text">2.3 逻辑回归的代价函数（Logistic Regression Cost Function）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-4-梯度下降法（Gradient-Descent）"><span class="toc-nav-text">2.4 梯度下降法（Gradient Descent）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-5-导数（Derivatives）"><span class="toc-nav-text">2.5 导数（Derivatives）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-6-更多的导数例子（More-Derivative-Examples）"><span class="toc-nav-text">2.6 更多的导数例子（More Derivative Examples）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-7-计算图（Computation-Graph）"><span class="toc-nav-text">2.7 计算图（Computation Graph）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-8-计算图的导数计算（Derivatives-with-a-Computation-Graph）"><span class="toc-nav-text">2.8 计算图的导数计算（Derivatives with a Computation Graph）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-9-逻辑回归中的梯度下降（Logistic-Regression-Gradient-Descent）"><span class="toc-nav-text">2.9 逻辑回归中的梯度下降（Logistic Regression Gradient Descent）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-10-m-个样本的梯度下降-Gradient-Descent-on-m-Examples"><span class="toc-nav-text">2.10  m 个样本的梯度下降(Gradient Descent on m Examples)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-11-向量化-Vectorization"><span class="toc-nav-text">2.11 向量化(Vectorization)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-12-向量化的更多例子（More-Examples-of-Vectorization）"><span class="toc-nav-text">2.12 向量化的更多例子（More Examples of Vectorization）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-13-向量化逻辑回归-Vectorizing-Logistic-Regression"><span class="toc-nav-text">2.13 向量化逻辑回归(Vectorizing Logistic Regression)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-14-向量化-logistic-回归的梯度输出（Vectorizing-Logistic-Regression’s-Gradient）"><span class="toc-nav-text">2.14 向量化 logistic 回归的梯度输出（Vectorizing Logistic Regression’s Gradient）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-15-Python-中的广播（Broadcasting-in-Python）"><span class="toc-nav-text">2.15 Python 中的广播（Broadcasting in Python）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-16-关于-python-numpy-向量的说明（A-note-on-python-or-numpy-vectors）参考视频："><span class="toc-nav-text">2.16 关于 python _ numpy 向量的说明（A note on python or numpy vectors）参考视频：</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-17-Jupyter-iPython-Notebooks快速入门（Quick-tour-of-Jupyter-iPython-Notebooks）"><span class="toc-nav-text">2.17 Jupyter/iPython Notebooks快速入门（Quick tour of Jupyter/iPython Notebooks）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-18-（选修）logistic-损失函数的解释（Explanation-of-logistic-regression-cost-function）"><span class="toc-nav-text">2.18 （选修）logistic 损失函数的解释（Explanation of logistic regression cost function）</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#第三周：浅层神经网络-Shallow-neural-networks"><span class="toc-nav-text">第三周：浅层神经网络(Shallow neural networks)</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#3-1-神经网络概述（Neural-Network-Overview）"><span class="toc-nav-text">3.1 神经网络概述（Neural Network Overview）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#3-2-神经网络的表示（Neural-Network-Representation）"><span class="toc-nav-text">3.2 神经网络的表示（Neural Network Representation）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#3-3-计算一个神经网络的输出（Computing-a-Neural-Network’s-output）"><span class="toc-nav-text">3.3 计算一个神经网络的输出（Computing a Neural Network’s output）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#3-4-多样本向量化（Vectorizing-across-multiple-examples）"><span class="toc-nav-text">3.4 多样本向量化（Vectorizing across multiple examples）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#3-5-向量化实现的解释（Justification-for-vectorized-implementation）"><span class="toc-nav-text">3.5 向量化实现的解释（Justification for vectorized implementation）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#3-6-激活函数（Activation-functions）"><span class="toc-nav-text">3.6 激活函数（Activation functions）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#3-7-为什么需要非线性激活函数？（why-need-a-nonlinear-activation-function-）"><span class="toc-nav-text">3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#3-8-激活函数的导数（Derivatives-of-activation-functions）"><span class="toc-nav-text">3.8 激活函数的导数（Derivatives of activation functions）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#3-9-神经网络的梯度下降（Gradient-descent-for-neural-networks）"><span class="toc-nav-text">3.9 神经网络的梯度下降（Gradient descent for neural networks）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#3-10（选修）直观理解反向传播（Backpropagation-intuition）"><span class="toc-nav-text">3.10（选修）直观理解反向传播（Backpropagation intuition）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#3-11-随机初始化（Random-Initialization）"><span class="toc-nav-text">3.11 随机初始化（Random+Initialization）</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#人工智能大师实录"><span class="toc-nav-text">人工智能大师实录</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Geoffery-Hinton-专访"><span class="toc-nav-text">Geoffery Hinton 专访</span></a></li></ol></li></ol>
    
  </div>
</aside>
  
  <!-- Catalog END -->
</main>

<script>
  (function () {
    var url = 'http://iphysresearch.github.io/2018/02/NG_DeepLearning1/';
    var banner = 'https://i.loli.net/2018/02/15/5a852c10adc97.png'
    if (banner !== '' && banner !== 'undefined' && banner !== 'null') {
      $('#article-banner').css({
        'background-image': 'url(' + banner + ')'
      })
    } else {
      $('#article-banner').geopattern(url)
    }
    $('.header').removeClass('fixed-header')

     // error image
    $(".markdown-content img").on('error', function() {
      $(this).attr('src', 'http://file.muyutech.com/error-img.png')
      $(this).css({
        'cursor': 'default'
      })
    })

    // zoom image
    $(".markdown-content img").on('click', function() {
      var src = $(this).attr('src')
      if (src !== 'http://file.muyutech.com/error-img.png') {
        var imageW = $(this).width()
        var imageH = $(this).height()
        
        var zoom = ($(window).width() * 0.95 / imageW).toFixed(2)
        zoom = zoom < 1 ? 1 : zoom
        zoom = zoom > 2 ? 2 : zoom
        var transY = (($(window).height() - imageH) / 2).toFixed(2)

        $('body').append('<div class="image-view-wrap"><div class="image-view-inner"><img src="'+ src +'" /></div></div>')
        $('.image-view-wrap').addClass('wrap-active')
        $('.image-view-wrap img').css({
          'width': `${imageW}`,
          'transform': `translate3d(0, ${transY}px, 0) scale3d(${zoom}, ${zoom}, 1)`
        })
        $('html').css('overflow', 'hidden')

        $('.image-view-wrap').on('click', function() {
          $(this).remove()
          $('html').attr('style', '')
        })
      }
    })

    // qrcode
    var qr = new QRious({
      element: document.getElementById('share-qrcode'),
      value: document.location.href
    });

    // gitment
    var gitmentConfig = "iphysresearch";
    if (gitmentConfig !== 'undefined') {
      var gitment = new Gitment({
        id: "品读 A. Ng 的 DeepLearning.ai 之“神经网络和深度学习”",
        owner: "iphysresearch",
        repo: "iphysresearch.github.io",
        oauth: {
          client_id: "6b978dc207dc30e58ec8",
          client_secret: "2bc56895d0221e8c27ab87b072f8f18523231e22"
        },
        theme: {
          render(state, instance) {
            const container = document.createElement('div')
            container.lang = "en-US"
            container.className = 'gitment-container gitment-root-container'
            container.appendChild(instance.renderHeader(state, instance))
            container.appendChild(instance.renderEditor(state, instance))
            container.appendChild(instance.renderComments(state, instance))
            container.appendChild(instance.renderFooter(state, instance))
            return container;
          }
        }
      })
      gitment.render(document.getElementById('comments'))
    }
  })();
</script>

    <div class="scroll-top">
  <span class="arrow-icon"></span>
</div>
    <footer class="app-footer">
  <p class="copyright">
    &copy; 2018 | Proudly powered by <a href="https://hexo.io" target="_blank">Hexo</a>
    <br>
    Theme by <a href="https://github.com/yanm1ng">yanm1ng</a>
  </p>
</footer>

<script>
  function async(u, c) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }
</script>
<script>
  async("//cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
    FastClick.attach(document.body);
  })
</script>

<script>
  var hasLine = 'true';
  async("//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js", function(){
    $('figure pre').each(function(i, block) {
      var figure = $(this).parents('figure');
      if (hasLine === 'false') {
        figure.find('.gutter').hide();
      }
      var lang = figure.attr('class').split(' ')[1] || 'code';
      var codeHtml = $(this).html();
      var codeTag = document.createElement('code');
      codeTag.className = lang;
      codeTag.innerHTML = codeHtml;
      $(this).attr('class', '').empty().html(codeTag);
      figure.attr('data-lang', lang.toUpperCase());
      hljs.highlightBlock(block);
    });
  })
</script>
<!-- Baidu Tongji -->

<script src="/js/script.js"></script>
  </body>
</html>