<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
  <meta name="description" content="Machine Learning, Deep Learning, Physics">
  <meta name="keyword" content="hexo-theme, vuejs">
  
    <link rel="shortcut icon" href="/css/images/logo.png">
  
  <title>
    
      一段关于神经网络的故事：高速运转的加强版神经网络 | Teaching is Learning
    
  </title>
  <link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.css" rel="stylesheet">
  <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/tomorrow.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/plugins/gitment.css">
  <script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
  <script src="//cdn.bootcss.com/geopattern/1.2.3/js/geopattern.min.js"></script>
  <script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>
  <script src="/js/qrious.js"></script>
<script src="/js/gitment.js"></script>
  
  
    <!-- MathJax support START -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <!-- MathJax support END -->
  


</head>
<div class="wechat-share">
  <img src="/css/images/logo.png" />
</div>

  <body>
    <header class="header fixed-header">
  <div class="header-container">
    <a class="home-link" href="/">
      <div class="logo"></div>
      <span>Teaching is Learning</span>
    </a>
    <ul class="right-list">
      
        <li class="list-item">
          
            <a href="/" class="item-link">Home</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/tags/" class="item-link">Tags</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/archives/" class="item-link">Archives</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/project/" class="item-link">Projects</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/about/" class="item-link">About</a>
          
        </li>
      
    </ul>
    <div class="menu">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </div>
    <div class="menu-mask">
      <ul class="menu-list">
        
          <li class="menu-item">
            
              <a href="/" class="menu-link">Home</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/tags/" class="menu-link">Tags</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/archives/" class="menu-link">Archives</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/project/" class="menu-link">Projects</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/about/" class="menu-link">About</a>
            
          </li>
        
      </ul>
    </div>
  </div>
</header>

    <div id="article-banner">
  <h2>一段关于神经网络的故事：高速运转的加强版神经网络</h2>
  <p class="post-date">2018-02-06</p>
  <div class="arrow-down">
    <a href="javascript:;"></a>
  </div>
</div>
<main class="app-body flex-box">
  <!-- Article START -->
  <article class="post-article">
    <section class="markdown-content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><ul>
<li>Contents：<ol>
<li>论自我惩罚后的救赎之路——正则化</li>
<li>有膜更健康！——批量归一化</li>
<li>就是这么任性！——Dropout</li>
<li>构造任意深度的神经网络！</li>
</ol>
</li>
</ul>
<a id="more"></a>
<blockquote>
<p>这篇系列长文的灵感来源于自己参与【大数据文摘&amp;稀牛学院】的一个 CS231n 作业资料整理与讲解 case，详情可见：大数据文摘<a href="https://study.163.com/provider/10146755/index.htm" target="_blank" rel="noopener">网易云课堂专栏</a>，<a href="http://blog.csdn.net/bigdatadigest/article/details/79286510" target="_blank" rel="noopener">CSDN专栏</a> 和 <a href="https://github.com/theBigDataDigest/Stanford-CS231n-assignments-in-Chinese/tree/master/assignment2/Q1-Q3" target="_blank" rel="noopener">GitHub专栏</a>。 其中，我负责的是 CS231n 的 Assignment2 中的Q1-Q3部分，也正是此故事系列的主要内容，探讨的是基于全连接网络下各种常用技术的 Python 代码实现。</p>
</blockquote>
<p><br></p>
<p>（接上文：<a href="https://iphysresearch.github.io/2018/02/cs231n_MLP3/">一段关于神经网络的故事：传说中的反向传播</a>）</p>
<p><br></p>
<h1 id="高速运转的加强版神经网络"><a href="#高速运转的加强版神经网络" class="headerlink" title="高速运转的加强版神经网络"></a>高速运转的加强版神经网络</h1><p>在前面的故事里，我们总算是学会搭建一个全连接的神经网络了，那么它究竟好用么？如果你直接去用那个简易的神经网络在CIFAR-10数据集上训练和检查分类的准确率，你可能会有些失望。</p>
<blockquote>
<p>训练的速度好慢啊！</p>
<p>在测试集上怎么准确率不高啊？</p>
<p>该怎么增加神经网络的深度呢？</p>
<p>。。。</p>
</blockquote>
<p>接踵而至的问题和疑惑可能还有很多，但是不用担心，在故事谈到模型的最优化——“训练”之前，我们可以”事后诸葛亮的”在上述简易神经网络的基础上添砖加瓦，让它变得更加强大，更加高效运转起来！</p>
<p><br></p>
<h2 id="论自我惩罚后的救赎之路——正则化"><a href="#论自我惩罚后的救赎之路——正则化" class="headerlink" title="论自我惩罚后的救赎之路——正则化"></a>论自我惩罚后的救赎之路——正则化</h2><p>在我们的故事讲图像样本数据在神经网络中如何流动的时候，从每个神经元层的评价打分，到激活，再到最后的输出层给出loss损失值，以及最终反向传播到loss关于参数(W, B)的梯度，我们的最终目的是为了得到最小化的loss损失值，因为它所对应的参数(W, B)正是我们想要的理想神经网络参数。</p>
<p><br></p>
<p>神经网络的最优化过程，正是我们上述的反复迭代逼近最小loss损失值的过程，然而在这个过程中很容易出现一个问题，那就是<strong>“过拟合”</strong>。换句话说，在最小化loss损失值时，我们的神经网络训练学习得太好了，都快把训练集的答案背下来了，但是一旦做测试考试题，就答的一塌糊涂、惨不忍睹，准确率远比做训练题低，这就是所谓的“过拟合”。</p>
<p><br></p>
<p>“背答案”的后果可是很严重的，必须要接受惩罚！</p>
<p><br></p>
<p>于是，我们要求图片样本数据每通过一次神经网络时，都要对得到的loss值和参数的梯度加一个惩罚项——<strong>正则项，其本质是约束（限制）要优化的参数</strong>。如下图：</p>
<hr>
<p><img src="http://rasbt.github.io/mlxtend/user_guide/general_concepts/regularization-linear_files/l2.png" alt=""></p>
<p>上图是在$w_1,w_2$参数平面内，绘制的loss损失函数值的等高线图。圈圈最中心的点即使loss损失函数最小值所对应的参数$w_1,w_2$。而正则化是我们对于神经网络中的每个权重$w$，向损失函数中增加一个正则项 $\frac{1}{2}\lambda w^2$ (<strong>L2正则化</strong>)，其中 $\lambda$ 是正则化强度。于是，在最优化的过程中，解空间缩小了，解参数$w_1,w_2$的可选范围不再是整个平面，而是被限制在图中阴影的圆形区域内。所以，经过惩罚且最小化的loss损失值所对应的最优参数$w_1,w_2$，就是距离未经历惩罚的最小化loss损失值最近，位于圆形区域边界上的一点。(See more: <a href="http://blog.csdn.net/wsj998689aa/article/details/39547771" target="_blank" rel="noopener">XX</a>，<a href="http://rasbt.github.io/mlxtend/user_guide/general_concepts/regularization-linear/" target="_blank" rel="noopener">XX</a>)</p>
<hr>
<p>废话说了很多，反映在代码上其实很简单，只需要对神经网络给出的loss和其梯度稍加修改即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">loss += <span class="number">0.5</span> * self.reg * (np.sum(self.params[<span class="string">"W1"</span>] ** <span class="number">2</span>) + \</span><br><span class="line">                                  np.sum(self.params[<span class="string">"W2"</span>] ** <span class="number">2</span>))</span><br><span class="line">dW2 += self.reg * self.params[<span class="string">"W2"</span>]</span><br><span class="line">dW1 += self.reg * self.params[<span class="string">"W1"</span>]</span><br></pre></td></tr></table></figure>
<p><strong>代码详解：</strong></p>
<ul>
<li><code>self.reg</code> 是正则化强度，这不是一个要优化的学习参数，需要在训练之前设置好，取其值为0就意味着不考虑正则化。</li>
<li><code>np.sum(self.params[&quot;W1&quot;] ** 2)</code> 表示对参数矩阵 $w_1$ 中的每一个元素都取平方，并全部加起来得到一个scalar数。</li>
</ul>
<p><br></p>
<p>小注：</p>
<p><br></p>
<p>我们的L2正则化可以直观理解为它对于大数值的权重参数进行严厉惩罚，倾向于更加分散的权重参数。由于在每层的神经元中输入数据和权重参数矩阵之间的乘法操作，这样就有了一个优良的特性：使得神经网络网络更倾向于使用所有输入数据的特征，而不是严重依赖输入特征中某些小部分特征。</p>
<p><br></p>
<h2 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h2><p><strong>注意：</strong>接下来的故事就开始变的非常微妙了，由于故事的大boss——神经网络的最优化位于本文的最后一部分(要搞定大boss才能大结局嘛)，所以接下来的故事中将会时不时地出现大boss的身影，比如区分训练模式和测试模式、优化算法等身影。所以不要担心觉得太陌生，它会在故事最终大结局时还会露面的。</p>
<p><br></p>
<p>前面的故事提到过，神经网络的初始化是一个很棘手的问题。即使到现在，谁也不敢说自己的初始化方案就是最完美、最合适的。不过，我们有一个接近完美的方案来减轻如何合理初始化神经网络这个棘手问题带来的头痛，这个方案就是<strong>批量归一化（Batch Normalization）</strong>。</p>
<p><br></p>
<p>细说之前，先飞图两张来看看它什么样子～</p>
<hr>
<p><img src="http://upload-images.jianshu.io/upload_images/2301760-7a7167abb6b6cc10.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上图是我们已经不再陌生的3层全连接神经网络，2个隐藏层神经元中的f表示神经元们给出打分后的激活函数f。</p>
<p><br></p>
<p>而<strong>批量归一化（Batch Normalization）</strong>所做的事情，就是要在神经元们给出打分和拿去做激活之间添加一个步骤，对所有的得分做一个数据预处理，然后再送给激活函数。如下图所示：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/2301760-ebbfe8659b9e6b27.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，我们在每一层神经网络的激活函数前都进行批量归一化处理，要求数据和梯度在正反向传播中都要有这一步骤。千万不要小瞧神经元体内的这一步骤，它的存在意义重大且深刻。这个在每一个隐藏层中增加”批量归一化”作为数据预处理的方法可以在未来的学习过程中，进一步加速收敛，其效果非同小可。它的作用就像高透光的手机贴膜，既能保护手机对外界环境的磨损，又能保证膜内手机屏幕透光性，我们的手机当然就更健康啦！</p>
<hr>
<p>下面直接给出前向传播Batch Normalization(BN)层的 <code>batchnorm_forward(x, gamma, beta, bn_param)=(out,cache)</code> 函数代码，会更清晰明了！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span><span class="params">(x, gamma, beta, bn_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        During training the sample mean and (uncorrected) sample variance are</span></span><br><span class="line"><span class="string">    computed from minibatch statistics and used to normalize the incoming data.</span></span><br><span class="line"><span class="string">    During training we also keep an exponentially decaying running mean of the mean</span></span><br><span class="line"><span class="string">    and variance of each feature, and these averages are used to normalize data</span></span><br><span class="line"><span class="string">    t test-time.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    At each timestep we update the running averages for mean and variance using</span></span><br><span class="line"><span class="string">    an exponential decay based on the momentum parameter:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span></span><br><span class="line"><span class="string">    running_var = momentum * running_var + (1 - momentum) * sample_var</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that the batch normalization paper suggests a different test-time</span></span><br><span class="line"><span class="string">    behavior: they compute sample mean and variance for each feature using a</span></span><br><span class="line"><span class="string">    large number of training images rather than using a running average. For</span></span><br><span class="line"><span class="string">    this implementation we have chosen to use running averages instead since</span></span><br><span class="line"><span class="string">    they do not require an additional estimation step; the torch7 implementation</span></span><br><span class="line"><span class="string">    of batch normalization also uses running averages.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Data of shape (N, D)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></span><br><span class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></span><br><span class="line"><span class="string">    - bn_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">    - mode: 'train' or 'test'; required</span></span><br><span class="line"><span class="string">    - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string">    - momentum: Constant for running mean / variance.</span></span><br><span class="line"><span class="string">    - running_mean: Array of shape (D,) giving running mean of features</span></span><br><span class="line"><span class="string">    - running_var Array of shape (D,) giving running variance of features</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    mode = bn_param[<span class="string">'mode'</span>]</span><br><span class="line">    eps = bn_param.get(<span class="string">'eps'</span>, <span class="number">1e-5</span>)</span><br><span class="line">    momentum = bn_param.get(<span class="string">'momentum'</span>, <span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    N, D = x.shape</span><br><span class="line">    running_mean = bn_param.get(<span class="string">'running_mean'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line">    running_var = bn_param.get(<span class="string">'running_var'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line"></span><br><span class="line">    out, cache = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:		<span class="comment"># 训练模式</span></span><br><span class="line">		sample_mean = np.mean(x, axis=<span class="number">0</span>)	<span class="comment"># 矩阵x每一列的平均值 shaple: (D,)</span></span><br><span class="line">    	sample_var = np.var(x, axis=<span class="number">0</span>)		<span class="comment"># 矩阵x每一列的方差 shape: (D,)</span></span><br><span class="line">    	x_hat = (x - sample_mean) / (np.sqrt(sample_var + eps))</span><br><span class="line">    	out = gamma * x_hat + beta</span><br><span class="line">    	cache = (x, sample_mean, sample_var, x_hat, eps, gamma, beta)</span><br><span class="line">        running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * sample_mean</span><br><span class="line">        running_var = momentum * running_var + (<span class="number">1</span> - momentum) * sample_var</span><br><span class="line">        </span><br><span class="line">	<span class="keyword">elif</span> mode == <span class="string">'test'</span>:	<span class="comment"># 测试模式</span></span><br><span class="line">      	out = (x - running_mean) * gamma / (np.sqrt(running_var + eps)) + beta</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Invalid forward batchnorm mode "%s"'</span> % mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the updated running means back into bn_param</span></span><br><span class="line">    bn_param[<span class="string">'running_mean'</span>] = running_mean</span><br><span class="line">    bn_param[<span class="string">'running_var'</span>] = running_var</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<p><strong>代码详解：</strong></p>
<ul>
<li><code>batchnorm_forward()</code> 函数的输入矩阵x可以看做是输入的样本数据(或者是经过线性函数的输出值)，shape为(N, D)，N是样本图片的个数，D是特征个数(本例中就是像素点数)。参数(gamma, beta)是我们新的待优化学习的参数，shape都为(D, )，这样一来，我们的神经网络模型参数增加到四个(w, b, gamma, beta)。</li>
<li>输入进来的 <code>bn_param</code> 是BN算法的参数字典，其中存有 <code>eps</code> 数值变量精度。 是为了避免分母除数为0的情况所使用的微小正数。<code>mode</code> 存好了当前神经网络的状态。我们在之前的故事中所谈过的数据正向流动和损失函数梯度的反向传播都已经默认了是 <code>train</code> 模式，因为我们最终的目标是希望根据样本图片训练我们的神经网络模型(中的参数)，使得它进入 <code>test</code> 模式下时，输入没有正确标签的样本图片可以给出正确的标签。参数字典中 <code>momentum</code>  参数是最优化中的”超参数”常数，我们会在最后说明其中的内涵，在这里只要知道它会默认取值为0.9就好了。<code>running_mean, running_var</code>即移动平均值和移动方差值，会在train阶段随着样本的输入不断变化，</li>
<li>在 <code>train</code> 训练模式下，<code>sample_mean</code> 和 <code>sample_var</code> 分别是对x的每一列算得平均值和标准差向量，shape都为(D, )。若这是第一隐藏层中神经元的BN层，这相当于是把每张样本图像对应像素维度看作结构化数据的一个”特征”(共3072个)，然后算出所有样本图片里每个像素特征下的平均值和标准差。</li>
<li>接下来就是重点了，我们要操作的数学公式是这样的：</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/2301760-bbbed97563402350.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Batch Normalization, algorithm1"></p>
<p>前两行公式我们已经谈过了，它们分别是 <code>sample_mean</code> 和 <code>sample_var</code> 所对应得分矩阵的每一列$x_i$算得平均值向量和标准差向量$(\mu_{B},\sigma^2_B)$。然后，我们就要利用这两个信息，对得分矩阵的每一列$x_i$特征数据进行<strong>“归一化”</strong>，即使得每一维特征均值为0，标准差为1，服从<strong>标准高斯分布</strong>。又因为归一化是一个简单且可求导的操作，所以这是可行的。最后一步更关键了，一招惊天地泣鬼神的招式：<strong>变换重构</strong>，引入了可学习参数$\gamma, \beta$，让该层神经元可以学习恢复出原始神经网络在该层所要学习的特征分布。</p>
<ul>
<li>在代码中，<code>x_hat = (x - sample_mean) / (np.sqrt(sample_var + eps))</code> 和 <code>out = gamma * x_hat + beta</code> ，仍可以明显的看到Broadcasting机制的应用。参数向量(gamma, beta) 都被broadcasting拉成与矩阵x相同shape(N, D)的矩阵，然后在每一个对应元素上做如数学公式后两行所示的运算。在公式中 <code>eps</code> 的存在是必要的，因为数据某一列的标准差 <code>sample_var</code> 为0是可能的，比如仅一张图片流入神经网络的时候。</li>
<li>在 <code>train</code> 模式里，<code>running_mean, running_var</code> 会在每经过一个BN层进行一次迭代计算～并在离开每一个BN层之前保存替换掉原BN参数字典中的 <code>running_mean, runing_var</code> 值。在训练结束后，这两个参数将会用于 <code>test</code> 测试模式下的每一个BN层中。其实你若留意英文注释的话，就会明白上面代码中的操作并不是BN算法的作者原始的处理方式，其本意是由于神经网络一旦训练完毕，参数都会基本固定下来，这个时候即使是每批样本图片再进入我们的神经网络，那么BN层计算的平均值和标准差都是固定不变的，所以我们可以采用这些数值来作为测试样本所需要的均值、标准差。然而，我们上述代码中所采用的估计整个训练集的方法，会高效简洁得多，严格地说，这叫<strong>一次指数平滑法(Single exponential smoothing)</strong>：</li>
</ul>
<blockquote>
<p>在市场预测中，一次指数平滑法是根据前期的实测数和预测数，以加权因子为权数，进行加权平均，来预测未来时间趋势的方法。</p>
<p>详情可查看这两个资料：<a href="http://wiki.mbalib.com/wiki/一次指数平滑法" target="_blank" rel="noopener">MBAlib：一次指数平滑法</a> and <a href="https://en.wikipedia.org/wiki/Exponential_smoothing" target="_blank" rel="noopener">Wiki: Exponential smoothing</a></p>
<p>从上述资料中，我们就可以知晓：参数momentum对应于平滑系数，或者叫加权因子。其值大小的取定是要考虑到每一次BN层算得的平均值和标准差的变化特性所决定的，算是一个经验超参数：在我们的神经网络中，若每层每次算得波动不大，比较平稳，则参数momentum可取0.7～0.9；若具有迅速且明显的变动倾向，则应取为0.1～0.4。</p>
<p>关于一次指数平滑法的初值取法也是有讲究的。可以看到我们上面的代码就是取第一次算得的平均值或标准差来作为初始默认值的。</p>
</blockquote>
<p>那么，清楚了神经元层中 <code>batchnorm_forward()</code> 函数干的活，就可以很轻松的改写原一层神经元们在前向传播中的 <code>affine_relu_forward()</code> 函数为含有Batch Normalization层的 <code>affine_bn_relu_forward(x, w, b, gamma, beta, bn_param) = (out, cache)</code> 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_bn_relu_forward</span><span class="params">(x, w, b, gamma, beta, bn_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Array of shape (N, D1); input to the affine layer</span></span><br><span class="line"><span class="string">    - w, b: Arrays of shape (D2, D2) and (D2,) giving the weight and bias for</span></span><br><span class="line"><span class="string">      the affine transform.</span></span><br><span class="line"><span class="string">    - gamma, beta: Arrays of shape (D2,) and (D2,) giving scale and shift</span></span><br><span class="line"><span class="string">      parameters for batch normalization.</span></span><br><span class="line"><span class="string">    - bn_param: Dictionary of parameters for batch normalization.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - out: Output from ReLU, of shape (N, D2)</span></span><br><span class="line"><span class="string">    - cache: Object to give to the backward pass.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    a, fc_cache = affine_forward(x, w, b)</span><br><span class="line">    a_bn, bn_cache = batchnorm_forward(a, gamma, beta, bn_param) <span class="comment">#BN层，注意！它在ReLU层前</span></span><br><span class="line">    out, relu_cache = relu_forward(a_bn)	<span class="comment"># ReLU层</span></span><br><span class="line">    cache = (fc_cache, bn_cache, relu_cache)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<p>显然在反向传播中，损失函数的梯度也需要添加一步Batch Normalization的局部梯度：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/2301760-925f85e899bfd47a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Backpropagate the gradient of loss ℓ"></p>
<p>其中数学公式有点复杂，证明可见网络。</p>
<p><br></p>
<p>看见就想吐就不要看了，还是直接飞代码才能清晰我们的思路，要理清楚损失函数的数据梯度是如何经过这一层的就好：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: Variable of intermediates from batchnorm_forward.</span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to inputs x, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span></span><br><span class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x, mean, var, x_hat, eps, gamma, beta = cache</span><br><span class="line">    N = x.shape[<span class="number">0</span>]</span><br><span class="line">    dgamma = np.sum(dout * x_hat, axis=<span class="number">0</span>)	<span class="comment"># 第5行公式</span></span><br><span class="line">    dbeta = np.sum(dout * <span class="number">1.0</span>, axis=<span class="number">0</span>)		<span class="comment"># 第6行公式</span></span><br><span class="line">    dx_hat = dout * gamma					<span class="comment"># 第1行公式</span></span><br><span class="line">    dx_hat_numerator = dx_hat / np.sqrt(var + eps)		<span class="comment"># 第3行第1项(未负求和)</span></span><br><span class="line">    dx_hat_denominator = np.sum(dx_hat * (x - mean), axis=<span class="number">0</span>)	<span class="comment"># 第2行前半部分</span></span><br><span class="line">    dx_1 = dx_hat_numerator					<span class="comment"># 第4行第1项</span></span><br><span class="line">    dvar = <span class="number">-0.5</span> * ((var + eps) ** (<span class="number">-1.5</span>)) * dx_hat_denominator	<span class="comment"># 第2行公式</span></span><br><span class="line">    <span class="comment"># Note var is also a function of mean</span></span><br><span class="line">    dmean = <span class="number">-1.0</span> * np.sum(dx_hat_numerator, axis=<span class="number">0</span>) + \</span><br><span class="line">              dvar * np.mean(<span class="number">-2.0</span> * (x - mean), axis=<span class="number">0</span>)  <span class="comment"># 第3行公式(部分)</span></span><br><span class="line">    dx_var = dvar * <span class="number">2.0</span> / N * (x - mean) 	<span class="comment"># 第4行第2项</span></span><br><span class="line">    dx_mean = dmean * <span class="number">1.0</span> / N  				<span class="comment"># 第4行第3项</span></span><br><span class="line">    <span class="comment"># with shape (D,), no trouble with broadcast</span></span><br><span class="line">    dx = dx_1 + dx_var + dx_mean			<span class="comment"># 第4行公式</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure>
<p><strong>代码详解：</strong></p>
<ul>
<li>代码中已经给出了详细的与数学公式对应的注释。</li>
<li>提前提个醒，在 <code>test</code> 模式下，我们并不需要有反向传播这一步骤，只需要样本图片数据经过神经网络后，在输出层给出的得分即可。</li>
</ul>
<p>同样地，我们可以改写原一层神经元们在反向传播中的 <code>affine_relu_backward()</code> 函数为含有Batch Normalization层的 <code>affine_bn_relu_backward(dout, cache) = (dx, dw, db, dgamma, dbeta)</code> 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_bn_relu_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for the affine-batchnorm-relu convenience layer.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    fc_cache, bn_cache, relu_cache = cache</span><br><span class="line">    da_bn = relu_backward(dout, relu_cache)		<span class="comment"># ReLU层</span></span><br><span class="line">    da, dgamma, dbeta = batchnorm_backward(da_bn, bn_cache) <span class="comment"># BN层，反向传播时在ReLU后</span></span><br><span class="line">    dx, dw, db = affine_backward(da, fc_cache)	</span><br><span class="line">    <span class="keyword">return</span> dx, dw, db, dgamma, dbeta</span><br></pre></td></tr></table></figure>
<p><br></p>
<p><strong>小注：</strong></p>
<p><br></p>
<p>批量归一化（Batch Normalization）算法在2015年被提出来（<a href="http://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">原论文</a>），这个方法可以进一步加速收敛，因此学习率(后文会提到)可以适当增大，加快训练速度；过拟合现象可以得到一定程度的缓解，所以可以不用Dropout(后文会提到)或用较低的Dropout，而且可以减小L2正则化系数，训练速度又再一次得到了提升，即Batch Normalization可以降低我们对正则化的依赖程度。</p>
<p><br></p>
<p><u>现如今，深度神经网络基本都会用到Batch Normalization。</u></p>
<p><br></p>
<p>可以阅读原论文了解详细细节，也可以参考以下博文：</p>
<ul>
<li><a href="http://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="noopener">Batch Normalization 学习笔记</a></li>
<li><a href="http://blog.csdn.net/shuzfan/article/details/50723877" target="_blank" rel="noopener">解读Batch Normalization</a></li>
<li><a href="http://blog.csdn.net/happynear/article/details/44238541" target="_blank" rel="noopener">《Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift》阅读笔记与实现</a></li>
<li><a href="https://www.zhihu.com/question/38102762" target="_blank" rel="noopener">深度学习中 Batch Normalization为什么效果好？</a></li>
</ul>
<p><br></p>
<h2 id="就是这么任性！——Dropout"><a href="#就是这么任性！——Dropout" class="headerlink" title="就是这么任性！——Dropout"></a>就是这么任性！——Dropout</h2><p>故事听到这里了，想必你也已经能感觉到，想要让我们的神经网络提高对样本图片的分类能力，最直接粗暴的办法就是使用更深的网络和更多的神经元，即所谓<strong>deeper and wider</strong>。然而，越复杂的网络越容易过拟合，于是，我们可以让隐藏层的神经元们也可以任性一下，不要太卖力的工作，偶尔放个假休息下，这种”劳逸结合”的管理方式就是所谓的<strong>Dropout（随机失活）</strong>，大部分实验表明其具有一定的防止过拟合的能力。</p>
<p><br></p>
<p>来看图理解一下这种任性的管理方式：</p>
<hr>
<p><img src="https://sungjk.github.io/images/2017/04/26/dropout.png" alt=""></p>
<p>左图是标准的全连接神经元网络。我们的管理方式很简单：在训练的时候，让神经元以超参数p的概率被激活或者被设置为0。右图就是应用dropout的效果。</p>
<hr>
<p>训练过程中，dropout可以被认为是对完整的神经网络抽样出一些子集，每次基于激活函数的输出数据只更新子网络的参数（然而，数量巨大的子网络们并不是相互独立的，因为它们都共享参数）。</p>
<p><img src="http://cs.nyu.edu/~wanli/dropc/nn_do.jpg" alt=""></p>
<p>上图是前向传播中，神经元们把得分输出给激活函数a之后，会经过一个函数m，它会根据一个超参数p概率地让部分神经元不工作（其输出置为0），并且利用生成的随机失活遮罩(mask)对输出数据矩阵进行数值范围调整。在测试过程中不使用dropout，可以理解为是对数量巨大的子网络们做了模型集成（model ensemble），以此来计算出一个平均的预测。反向传播保持不变，但是肯定需要将对应的遮罩考虑进去。</p>
<hr>
<p>还是那句话，再空洞的理论阐述，都不如直接飞代码来的实际！</p>
<p><br></p>
<p>下面是在前向传播中，在dropout层处定义的 <code>dropout_forward(x, dropout_param) = (out, cache)</code> 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_forward</span><span class="params">(x, dropout_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Performs the forward pass for (inverted) dropout.</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of any shape</span></span><br><span class="line"><span class="string">    - dropout_param: A dictionary with the following keys:</span></span><br><span class="line"><span class="string">    - p: Dropout parameter. We drop each neuron output with probability p.</span></span><br><span class="line"><span class="string">    - mode: 'test' or 'train'. If the mode is train, then perform dropout;</span></span><br><span class="line"><span class="string">      if the mode is test, then just return the input.</span></span><br><span class="line"><span class="string">    - seed: Seed for the random number generator. Passing seed makes this</span></span><br><span class="line"><span class="string">      function deterministic, which is needed for gradient checking but not in</span></span><br><span class="line"><span class="string">      real networks.</span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">    - out: Array of the same shape as x.</span></span><br><span class="line"><span class="string">    - cache: A tuple (dropout_param, mask). In training mode, mask is the dropout</span></span><br><span class="line"><span class="string">      mask that was used to multiply the input; in test mode, mask is None.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    p, mode = dropout_param[<span class="string">'p'</span>], dropout_param[<span class="string">'mode'</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'seed'</span> <span class="keyword">in</span> dropout_param:</span><br><span class="line">        np.random.seed(dropout_param[<span class="string">'seed'</span>])</span><br><span class="line"></span><br><span class="line">    mask = <span class="keyword">None</span></span><br><span class="line">    out = <span class="keyword">None</span></span><br><span class="line">	<span class="comment"># 训练模式</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">        keep_prob = <span class="number">1</span> - p</span><br><span class="line">        mask = (np.random.rand(*x.shape) &lt; keep_prob) / keep_prob</span><br><span class="line">        out = mask * x</span><br><span class="line">    <span class="comment"># 测试模式</span></span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">        out = x</span><br><span class="line"></span><br><span class="line">    cache = (dropout_param, mask)</span><br><span class="line">    out = out.astype(x.dtype, copy=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<p><strong>代码详解：</strong></p>
<ul>
<li><p>输入数据矩阵x，其shape可以任意。dropout的参数字典 <code>dropout_param</code> 中存有超参数 <code>p</code> 和 <code>mode</code>，分别代表每层神经元们被失活的概率和当前 dropout 层是训练模式，还是测试模式。该参数字典中的可以有随机数生成种子 <code>seed</code>，要来标记随机性。</p>
</li>
<li><p>在训练模式下，首先，代码 <code>(np.random.rand(*x.shape)</code>，表示根据输入数据矩阵x，亦即经过”激活”后的得分，生成一个相同shape的随机矩阵，其为均匀分布的随机样本[0,1)。然后将其与可被保留神经元的概率 <code>keep_prob</code> 做比较，就可以得到一个随机真值表作为随机失活遮罩(mask)。原始的办法是：由于在训练模式时，我们丢掉了部分的激活值，数值调整 <code>out = mask * x</code> 后造成整体分布的期望值的下降，因此在预测时就需要乘上一个概率 <code>1/keep_prob</code>，才能保持分布的统一。不过，我们用一种叫做<strong>inverted dropout</strong>的技巧，就是如上面代码所示，直接在训练模式下多除以一个概率 <code>keep_prob</code>，那么在测试模式下就不用做任何操作了，直接让数据通过dropout层即可。如下图所示：</p>
<p>​</p>
<p><img src="http://ovj0qranm.bkt.clouddn.com/%E9%9A%8F%E6%9C%BA%E5%A4%B1%E6%B4%BB%E7%A4%BA%E4%BE%8B.jpg" alt=""></p>
</li>
<li><p>在最后，函数输出经过dropout且与输入x相同shape的out输出矩阵和缓冲保留dropout的参数字典 <code>dropout_param</code>，其中含有该层的失活概率超参数 <code>p</code> ，模式状态 <code>mode</code>，和随机失活遮罩 <code>mask</code>，以在反向传播中备用。</p>
</li>
</ul>
<p>在训练模式下的反向传播中，损失函数的梯度通过dropout层的 <code>dropout_backward(dout, cache) = dx</code> 函数恐怕是写起来最简单的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Perform the backward pass for (inverted) dropout.</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of any shape</span></span><br><span class="line"><span class="string">    - cache: (dropout_param, mask) from dropout_forward.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dropout_param, mask = cache</span><br><span class="line">    mode = dropout_param[<span class="string">'mode'</span>]</span><br><span class="line">  </span><br><span class="line">    dx = <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">        dx = mask * dout</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">        dx = dout</span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<p><strong>代码详解：</strong></p>
<ul>
<li>梯度反向传播时使用同样的 <code>mask</code>将被遮罩的梯度置零。</li>
</ul>
<p><br></p>
<p>小注：</p>
<p><br></p>
<p>最早的Dropout可以看Hinton的这篇文章 <a href="http://arxiv.org/pdf/1207.0580.pdf" target="_blank" rel="noopener">《Improving neural networks by preventing co-adaptation of feature Detectors》</a>，在Dropout发布后，很快有大量研究为什么它的实践效果如此之好，以及它和其他正则化方法之间的关系。如果你感兴趣，可以看看这些文献：</p>
<ul>
<li><a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%257Ersalakhu/papers/srivastava14a.pdf" target="_blank" rel="noopener">Dropout paper</a> by Srivastava et al. 2014.</li>
<li><a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf" target="_blank" rel="noopener">Dropout Training as Adaptive Regularization</a>：“我们认为：在使用费希尔信息矩阵（<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Fisher_information_metric" target="_blank" rel="noopener">fisher information matrix</a>）的对角逆矩阵的期望对特征进行数值范围调整后，再进行L2正则化这一操作，与随机失活正则化是一阶相等的。”</li>
</ul>
<p>对于Dropout这样的操作为何可以防止训练过拟合，原作者也没有给出数学证明，只是有一些直观的理解或者说猜想，比如：由于随机的让一些神经元不工作了，因此可以避免某些特征只在固定组合下才生效，有意识地让神经网络去学习一些普遍的共性（而不是某些训练样本的一些特性）。</p>
<p><br></p>
<p>更多细节可以阅读原论文了解，也可以参考以下博文：</p>
<p><br></p>
<p><a href="http://blog.csdn.net/shuzfan/article/details/50580915" target="_blank" rel="noopener">系列解读Dropout</a></p>
<p><br></p>
<p><a href="http://blog.csdn.net/stdcoutzyx/article/details/49022443" target="_blank" rel="noopener">理解dropout</a></p>
<p><br></p>
<p><a href="https://yq.aliyun.com/articles/68901" target="_blank" rel="noopener">深度学习网络大杀器之Dropout——深入解析Dropout</a></p>
<p><br></p>
<p><a href="http://cs.nyu.edu/~wanli/dropc/" target="_blank" rel="noopener">Regularization of Neural Networks using DropConnect</a></p>
<p><br></p>
<h2 id="构造任意深度的神经网络！"><a href="#构造任意深度的神经网络！" class="headerlink" title="构造任意深度的神经网络！"></a>构造任意深度的神经网络！</h2><p>接下来，我们要做一件很酷的事情，那就是构造一张更加强大的全连接神经网络，不仅包含了我们之前故事里提到的所有算法和技巧，同时不限定神经网络的深度(隐藏层的层数)和厚度(隐藏层神经元的个数)。</p>
<p><br></p>
<p>下面直接一行一行的阅读代码，我们定义一个 <code>FullyConnectedNet()</code> 类，里面只有一个初始化函数 <code>__init__()</code> 和一个损失函数 <code>loss()</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullyConnectedNet</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    一个任意隐藏层数和神经元数的全连接神经网络，其中 ReLU 激活函数，sofmax 损失函数，同时可选的</span></span><br><span class="line"><span class="string">    采用 dropout 和 batch normalization(批量归一化)。那么，对于一个L层的神经网络来说，其</span></span><br><span class="line"><span class="string">    框架是：  </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &#123;affine - [batch norm] - relu - [dropout]&#125; x (L - 1) - affine - softmax</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    其中的[batch norm]和[dropout]是可选非必须的，框架中&#123;...&#125;部分将会重复L-1次，代表</span></span><br><span class="line"><span class="string">	L-1 个隐藏层。</span></span><br><span class="line"><span class="string">  	</span></span><br><span class="line"><span class="string">  	与我们在上面的故事中定义的 TwoLayerNet() 类保持一致，所有待学习的参数都会存在 </span></span><br><span class="line"><span class="string">  	self.params 字典中，并且最终会被最优化 Solver() 类训练学习得到(后面的故事会谈到)。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">	<span class="string">"""</span></span><br><span class="line"><span class="string">	"""</span></span><br><span class="line">    <span class="comment">#1# 第一步是初始化我们的 FullyConnectedNet() 类：</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self</span></span></span><br><span class="line"><span class="function"><span class="params">                 ,hidden_dims		# 一个列表，元素个数是隐藏层数，元素值为该层神经元数</span></span></span><br><span class="line"><span class="function"><span class="params">                 ,input_dim=<span class="number">3</span>*<span class="number">32</span>*<span class="number">32</span>	# 默认输入神经元的个数是<span class="number">3072</span>个<span class="params">(匹配CIFAR<span class="number">-10</span>数据集)</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 ,num_classes=<span class="number">10</span>	# 默认输出神经元的个数是<span class="number">10</span>个<span class="params">(匹配CIFAR<span class="number">-10</span>数据集)</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 ,dropout=<span class="number">0</span>			# 默认不开启dropout，若取<span class="params">(<span class="number">0</span>,<span class="number">1</span>)</span>表示失活概率</span></span></span><br><span class="line"><span class="function"><span class="params">                 ,use_batchnorm=False	# 默认不开启批量归一化，若开启取True</span></span></span><br><span class="line"><span class="function"><span class="params">                 ,reg=<span class="number">0.0</span>			# 默认无L2正则化，取某scalar表示正则化的强度</span></span></span><br><span class="line"><span class="function"><span class="params">                 ,weight_scale=<span class="number">1e-2</span>	# 默认<span class="number">0.01</span>，表示权重参数初始化的标准差</span></span></span><br><span class="line"><span class="function"><span class="params">                 ,dtype=np.float64	# 默认np.float64精度，要求所有的计算都应该在此精度下。</span></span></span><br><span class="line"><span class="function"><span class="params">                 ,seed=None)</span>:</span>		<span class="comment"># 默认无随机种子，若有会传递给dropout层。</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 实例(Instance)中增加变量并赋予初值，以方便后面的 loss() 函数调用：</span></span><br><span class="line">        self.use_batchnorm = use_batchnorm</span><br><span class="line">        self.use_dropout = dropout &gt; <span class="number">0</span>	<span class="comment"># 可见，若dropout若为0时，为False</span></span><br><span class="line">        self.reg = reg</span><br><span class="line">        self.num_layers = <span class="number">1</span> + len(hidden_dims)	<span class="comment"># 在loss()函数里，</span></span><br><span class="line">        										<span class="comment"># 我们用神经网络的层数来标记规模</span></span><br><span class="line">        self.dtype = dtype</span><br><span class="line">        self.params = &#123;&#125;				<span class="comment"># self.params 空字典保存待训练学习的参数</span></span><br><span class="line">		<span class="string">"""</span></span><br><span class="line"><span class="string">		"""</span></span><br><span class="line">        <span class="comment"># 定义所有隐藏层的参数到字典 self.params 中：</span></span><br><span class="line">        in_dim = input_dim	<span class="comment"># Eg: in_dim = D</span></span><br><span class="line">        <span class="keyword">for</span> i, h_dim <span class="keyword">in</span> enumerate(hidden_dims):	<span class="comment"># Eg:(i, h_dim)=(0, H1)、(1, H2)...</span></span><br><span class="line">            <span class="comment"># Eg: W1(D, H1)、W2(H1, H2)... 			小随机数为初始值</span></span><br><span class="line">            self.params[<span class="string">"W%d"</span> % (i+<span class="number">1</span>,)] = weight_scale * \</span><br><span class="line">            							np.random.randn(in_dim</span><br><span class="line">                                                        ,h_dim)</span><br><span class="line">            <span class="comment"># Eg: b1(H1,)、b2(H2,)...				0为初始值</span></span><br><span class="line">            self.params[<span class="string">"b%d"</span> % (i+<span class="number">1</span>,)] = np.zeros((h_dim,))</span><br><span class="line">            <span class="keyword">if</span> use_batchnorm:					<span class="comment"># 若有批量归一化层</span></span><br><span class="line">              	<span class="comment"># Eg: gamma1(H1,)、gamma2(H2,)...	1为初始值</span></span><br><span class="line">                <span class="comment"># Eg: beta1(H1,)、beta2(H2)...		0为初始值</span></span><br><span class="line">                self.params[<span class="string">"gamma%d"</span> % (i+<span class="number">1</span>,)] = np.ones((h_dim,))	</span><br><span class="line">                self.params[<span class="string">"beta%d"</span> % (i+<span class="number">1</span>,)] = np.zeros((h_dim,))</span><br><span class="line">            in_dim = h_dim	<span class="comment"># 将该隐藏层的列数传递给下一层的行数</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 定义输出层的参数到字典 params 中：</span></span><br><span class="line">        self.params[<span class="string">"W%d"</span> % (self.num_layers,)] = weight_scale * \</span><br><span class="line">                                                np.random.randn(in_dim</span><br><span class="line">                                                                ,num_classes)</span><br><span class="line">        self.params[<span class="string">"b%d"</span> % (self.num_layers,)] = np.zeros((num_classes,))</span><br><span class="line">		<span class="string">"""</span></span><br><span class="line"><span class="string">		"""</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        当开启 dropout 时，我们需要在每一个神经元层中传递一个相同的</span></span><br><span class="line"><span class="string">        dropout 参数字典 self.dropout_param ，以保证每一层的神经元们</span></span><br><span class="line"><span class="string">        都知晓失活概率p和当前神经网络的模式状态mode(训练／测试)。        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.dropout_param = &#123;&#125;	<span class="comment"># dropout的参数字典</span></span><br><span class="line">        <span class="keyword">if</span> self.use_dropout:    <span class="comment"># 如果use_dropout的值是(0,1)，即启用dropout</span></span><br><span class="line">          	<span class="comment"># 设置mode默认为训练模式，取p为失活概率</span></span><br><span class="line">            self.dropout_param = &#123;<span class="string">'mode'</span>: <span class="string">'train'</span>, <span class="string">'p'</span>: dropout&#125;</span><br><span class="line">        <span class="keyword">if</span> seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:    <span class="comment"># 如果有seed随机种子，存入seed</span></span><br><span class="line">            self.dropout_param[<span class="string">'seed'</span>] = seed</span><br><span class="line">    	<span class="string">"""</span></span><br><span class="line"><span class="string">    	"""</span></span><br><span class="line">    	<span class="string">"""</span></span><br><span class="line"><span class="string">    	当开启批量归一化时，我们要定义一个BN算法的参数列表 self.bn_params ，</span></span><br><span class="line"><span class="string">    	以用来跟踪记录每一层的平均值和标准差。其中，第0个元素 self.bn_params[0] </span></span><br><span class="line"><span class="string">    	表示前向传播第1个BN层的参数，第1个元素 self.bn_params[1] 表示前向传播</span></span><br><span class="line"><span class="string">    	第2个BN层的参数，以此类推。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.bn_params = []		<span class="comment"># BN算法的参数列表</span></span><br><span class="line">        <span class="keyword">if</span> self.use_batchnorm:	<span class="comment"># 如果开启批量归一化，设置每层mode默认为训练模式</span></span><br><span class="line">            self.bn_params = [&#123;<span class="string">'mode'</span>: <span class="string">'train'</span>&#125; <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers - <span class="number">1</span>)] </span><br><span class="line">            <span class="comment"># 上面 self.bn_params 列表的元素个数是hidden layers的个数</span></span><br><span class="line">    	<span class="string">"""</span></span><br><span class="line"><span class="string">    	"""</span></span><br><span class="line">        <span class="comment"># 最后，调整所有的待学习神经网络参数为指定计算精度：np.float64</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> self.params.items():</span><br><span class="line">            self.params[k] = v.astype(dtype)</span><br><span class="line">	<span class="string">"""</span></span><br><span class="line"><span class="string">	"""</span></span><br><span class="line">	<span class="comment">#2# 第二步是定义我们的损失函数：</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        和 TwoLayerNet() 一样：</span></span><br><span class="line"><span class="string">        首先，输入的数据X是一个多维的array，shape为(样本图片的个数N*3*32*32),</span></span><br><span class="line"><span class="string">        y是与输入数据X对应的正确标签，shape为(N,)。</span></span><br><span class="line"><span class="string">        # 在训练模式下：#</span></span><br><span class="line"><span class="string">        我们loss函数目标输出一个损失值loss和一个grads字典，</span></span><br><span class="line"><span class="string">        其中存有loss关于隐藏层和输出层的参数(W,B,gamma,beta)的梯度值.</span></span><br><span class="line"><span class="string">        # 在测试模式下：#</span></span><br><span class="line"><span class="string">        我们的loss函数只需要直接给出输出层后的得分即可。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 把输入数据源矩阵X的精度调整一下</span></span><br><span class="line">        X = X.astype(self.dtype)     </span><br><span class="line">        <span class="comment"># 根据正确标签y是否为None来调整模式是test还是train</span></span><br><span class="line">        mode = <span class="string">'test'</span> <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> <span class="string">'train'</span>   </span><br><span class="line">		<span class="string">"""</span></span><br><span class="line"><span class="string">		"""</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        确定了当前神经网络所处的模式状态后，</span></span><br><span class="line"><span class="string">        就可以设置 dropout 的参数字典和 BN 算法的参数列表中的mode了，</span></span><br><span class="line"><span class="string">        因为他们在不同的模式下行为是不同的。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.dropout_param <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:  <span class="comment"># 如果开启dropout</span></span><br><span class="line">            self.dropout_param[<span class="string">'mode'</span>] = mode</span><br><span class="line">        <span class="keyword">if</span> self.use_batchnorm:				<span class="comment"># 如果开启批量归一化</span></span><br><span class="line">            <span class="keyword">for</span> bn_param <span class="keyword">in</span> self.bn_params:</span><br><span class="line">                bn_param[<span class="string">'mode'</span>] = mode</span><br><span class="line">		<span class="string">"""</span></span><br><span class="line"><span class="string">		"""</span></span><br><span class="line">        scores = <span class="keyword">None</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        %前向传播%</span></span><br><span class="line"><span class="string">        如果开启了dropout，我们需要将dropout的参数字典 self.dropout_param </span></span><br><span class="line"><span class="string">        在每一个dropout层中传递。</span></span><br><span class="line"><span class="string">        如果开启了批量归一化，我们需要指定BN算法的参数列表 self.bn_params[0]</span></span><br><span class="line"><span class="string">        对应前向传播第一层的参数，self.bn_params[1]对应第二层的参数，以此类推。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        fc_mix_cache = &#123;&#125;		<span class="comment"># 初始化每层前向传播的缓冲字典</span></span><br><span class="line">        <span class="keyword">if</span> self.use_dropout:	<span class="comment"># 如果开启了dropout，初始化其对应的缓冲字典</span></span><br><span class="line">            dp_cache = &#123;&#125;</span><br><span class="line">		<span class="string">"""</span></span><br><span class="line"><span class="string">		"""</span></span><br><span class="line">        <span class="comment"># 从第一个隐藏层开始循环每一个隐藏层，传递数据out，保存每一层的缓冲cache</span></span><br><span class="line">        out = X		</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers - <span class="number">1</span>):	<span class="comment"># 在每个hidden层中循环</span></span><br><span class="line">            w, b = self.params[<span class="string">"W%d"</span> % (i + <span class="number">1</span>,)], self.params[<span class="string">"b%d"</span> % (i + <span class="number">1</span>,)] </span><br><span class="line">            <span class="keyword">if</span> self.use_batchnorm:				<span class="comment"># 若开启批量归一化</span></span><br><span class="line">                gamma = self.params[<span class="string">"gamma%d"</span> % (i + <span class="number">1</span>,)]</span><br><span class="line">                beta = self.params[<span class="string">"beta%d"</span> % (i + <span class="number">1</span>,)]</span><br><span class="line">                out, fc_mix_cache[i] = affine_bn_relu_forward(out, w, b</span><br><span class="line">                                                              ,gamma</span><br><span class="line">                                                              ,beta</span><br><span class="line">                                                              ,self.bn_params[i])</span><br><span class="line">            <span class="keyword">else</span>:								<span class="comment"># 若未开启批量归一化</span></span><br><span class="line">                out, fc_mix_cache[i] = affine_relu_forward(out, w, b)</span><br><span class="line">            <span class="keyword">if</span> self.use_dropout:				<span class="comment"># 若开启dropout</span></span><br><span class="line">                out, dp_cache[i] = dropout_forward(out, self.dropout_param)</span><br><span class="line">        <span class="comment"># 最后的输出层</span></span><br><span class="line">        w = self.params[<span class="string">"W%d"</span> % (self.num_layers,)]</span><br><span class="line">        b = self.params[<span class="string">"b%d"</span> % (self.num_layers,)]</span><br><span class="line">        out, out_cache = affine_forward(out, w, b)</span><br><span class="line">        scores = out</span><br><span class="line">		<span class="string">"""</span></span><br><span class="line"><span class="string">		可以看到，上面对隐藏层的每次循环中，out变量实现了自我迭代更新；</span></span><br><span class="line"><span class="string">		fc_mix_cache 缓冲字典中顺序地存储了每个隐藏层的得分情况和模型参数(其中可内含BN层)；</span></span><br><span class="line"><span class="string">		dp_cache 缓冲字典中单独顺序地保存了每个dropout层的失活概率和遮罩；</span></span><br><span class="line"><span class="string">		out_cache 变量缓存了输出层处的信息；</span></span><br><span class="line"><span class="string">		值得留意的是，若开启批量归一化的话，BN层的参数列表 self.bn_params[i]，</span></span><br><span class="line"><span class="string">		从第一层开始多出'running_mean'和'running_var'的键值保存在参数列表的每一个元素中，</span></span><br><span class="line"><span class="string">		形如：[&#123;'mode': 'train','running_mean': ***,'running_var':***&#125;,&#123;...&#125;]</span></span><br><span class="line"><span class="string">		"""</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 接下来开始让loss函数区分不同模式：</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">'test'</span>:	<span class="comment"># 若是测试模式，输出scores表示预测的每个分类概率后，函数停止跳出。</span></span><br><span class="line">            <span class="keyword">return</span> scores</span><br><span class="line">		<span class="string">"""</span></span><br><span class="line"><span class="string">		"""</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        %反向传播%</span></span><br><span class="line"><span class="string">        既然运行到了这里，说明我们的神经网络是在训练模式下了，</span></span><br><span class="line"><span class="string">        接下来我们要计算损失值，并且通过反向传播，计算损失函数关于模型参数的梯度！</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss, grads = <span class="number">0.0</span>, &#123;&#125;	<span class="comment"># 初始化 loss 变量和梯度字典 grads</span></span><br><span class="line">        loss, dout = softmax_loss(scores, y)</span><br><span class="line">        loss += <span class="number">0.5</span> * self.reg * np.sum(self.params[<span class="string">"W%d"</span> % (self.num_layers,)]**<span class="number">2</span>)</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        你可能奇怪上面的loss损失值是不是有问题，还有其他隐藏层的权重矩阵的正则化呢？</span></span><br><span class="line"><span class="string">        别着急，我们要loss损失值的求解，跟随梯度的反向传播一点一点的算出来～</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 在输出层处梯度的反向传播，顺便把梯度保存在梯度字典 grad 中：</span></span><br><span class="line">        dout, dw, db = affine_backward(dout, out_cache)</span><br><span class="line">        grads[<span class="string">"W%d"</span> % (self.num_layers,)] = dw + self.reg * \</span><br><span class="line">                                            self.params[<span class="string">"W%d"</span> % (self.num_layers,)]</span><br><span class="line">        grads[<span class="string">"b%d"</span> % (self.num_layers,)] = db</span><br><span class="line">		<span class="comment"># 在每一个隐藏层处梯度的反向传播，不仅顺便更新了梯度字典 grad，还迭代算出了损失值loss:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers - <span class="number">1</span>):</span><br><span class="line">            ri = self.num_layers - <span class="number">2</span> - i	<span class="comment"># 倒数第ri+1隐藏层</span></span><br><span class="line">            loss += <span class="number">0.5</span> * self.reg * \		<span class="comment"># 迭代地补上每层的正则项给loss</span></span><br><span class="line">                                np.sum(self.params[<span class="string">"W%d"</span> % (ri+<span class="number">1</span>,)]**<span class="number">2</span>)</span><br><span class="line">            <span class="keyword">if</span> self.use_dropout:			<span class="comment"># 若开启dropout</span></span><br><span class="line">                dout = dropout_backward(dout, dp_cache[ri])</span><br><span class="line">            <span class="keyword">if</span> self.use_batchnorm:			<span class="comment"># 若开启批量归一化</span></span><br><span class="line">                dout, dw, db, dgamma, dbeta = affine_bn_relu_backward(dout, </span><br><span class="line">                                                            fc_mix_cache[ri])</span><br><span class="line">                grads[<span class="string">"gamma%d"</span> % (ri+<span class="number">1</span>,)] = dgamma</span><br><span class="line">                grads[<span class="string">"beta%d"</span> % (ri+<span class="number">1</span>,)] = dbeta</span><br><span class="line">            <span class="keyword">else</span>:							<span class="comment"># 若未开启批量归一化</span></span><br><span class="line">                dout, dw, db = affine_relu_backward(dout, fc_mix_cache[ri])</span><br><span class="line">            grads[<span class="string">"W%d"</span> % (ri+<span class="number">1</span>,)] = dw + self.reg * self.params[<span class="string">"W%d"</span> % (ri+<span class="number">1</span>,)]</span><br><span class="line">            grads[<span class="string">"b%d"</span> % (ri+<span class="number">1</span>,)] = db</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, grads	<span class="comment"># 输出训练模式下的损失值和损失函数的梯度！</span></span><br></pre></td></tr></table></figure>
<p>虽然注释已经给的足够详细了，但还是有必要为这个强大的神经网络画一张<strong>“数据流动走向图”</strong>才能真的让我们心有成竹，妈妈再也不用担心我的深度学习～</p>
<p><img src="http://ovj0qranm.bkt.clouddn.com/diy%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.004.jpeg" alt="diy神经网络.004"></p>
<p><img src="https://i.loli.net/2018/02/15/5a854ae2cd9f1.jpeg" alt=""></p>
<p>（接下文：<a href="https://iphysresearch.github.io/2018/02/cs231n_MLP5/">一段关于神经网络的故事：最终章！</a>）</p>
</section>
    <!-- Tags START -->
    
      <div class="tags">
        <span>Tags:</span>
        
  <a href="/tags#漫游CS231n" >
    <span class="tag-code">漫游CS231n</span>
  </a>

      </div>
    
    <!-- Tags END -->
    <!-- NAV START -->
    
  <div class="nav-container">
    <!-- reverse left and right to put prev and next in a more logic postition -->
    
      <a class="nav-left" href="/2018/02/cs231n_MLP3/">
        <span class="nav-arrow">← </span>
        
          一段关于神经网络的故事：传说中的反向传播
        
      </a>
    
    
      <a class="nav-right" href="/2018/02/cn231n_MLP5/">
        
          一段关于神经网络的故事：最终章！
        
        <span class="nav-arrow"> →</span>
      </a>
    
  </div>

    <!-- NAV END -->
    <!-- 打赏 START -->
    
      <div class="money-like">
        <div class="reward-btn">
          赏
          <span class="money-code">
            <span class="alipay-code">
              <div class="code-image"></div>
              <b>使用支付宝打赏</b>
            </span>
            <span class="wechat-code">
              <div class="code-image"></div>
              <b>使用微信打赏</b>
            </span>
          </span>
        </div>
        <p class="notice">若你觉得我的文章对你有帮助，欢迎点击上方按钮对我打赏</p>
      </div>
    
    <!-- 打赏 END -->
    <!-- 二维码 START -->
    
      <div class="qrcode">
        <canvas id="share-qrcode"></canvas>
        <p class="notice">扫描二维码，分享此文章</p>
      </div>
    
    <!-- 二维码 END -->
    
      <!-- Gitment START -->
      <div id="comments"></div>
      <!-- Gitment END -->
    
  </article>
  <!-- Article END -->
  <!-- Catalog START -->
  
    <aside class="catalog-container">
  <div class="toc-main">
    <strong class="toc-title">Catalog</strong>
    
      <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#高速运转的加强版神经网络"><span class="toc-nav-text">高速运转的加强版神经网络</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#论自我惩罚后的救赎之路——正则化"><span class="toc-nav-text">论自我惩罚后的救赎之路——正则化</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#批量归一化"><span class="toc-nav-text">批量归一化</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#就是这么任性！——Dropout"><span class="toc-nav-text">就是这么任性！——Dropout</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#构造任意深度的神经网络！"><span class="toc-nav-text">构造任意深度的神经网络！</span></a></li></ol></li></ol>
    
  </div>
</aside>
  
  <!-- Catalog END -->
</main>

<script>
  (function () {
    var url = 'http://iphysresearch.github.io/2018/02/cs231n_MLP4/';
    var banner = 'https://i.loli.net/2018/02/03/5a74d126a2aa9.png'
    if (banner !== '' && banner !== 'undefined' && banner !== 'null') {
      $('#article-banner').css({
        'background-image': 'url(' + banner + ')'
      })
    } else {
      $('#article-banner').geopattern(url)
    }
    $('.header').removeClass('fixed-header')

     // error image
    $(".markdown-content img").on('error', function() {
      $(this).attr('src', 'http://file.muyutech.com/error-img.png')
      $(this).css({
        'cursor': 'default'
      })
    })

    // zoom image
    $(".markdown-content img").on('click', function() {
      var src = $(this).attr('src')
      if (src !== 'http://file.muyutech.com/error-img.png') {
        var imageW = $(this).width()
        var imageH = $(this).height()
        
        var zoom = ($(window).width() * 0.95 / imageW).toFixed(2)
        zoom = zoom < 1 ? 1 : zoom
        zoom = zoom > 2 ? 2 : zoom
        var transY = (($(window).height() - imageH) / 2).toFixed(2)

        $('body').append('<div class="image-view-wrap"><div class="image-view-inner"><img src="'+ src +'" /></div></div>')
        $('.image-view-wrap').addClass('wrap-active')
        $('.image-view-wrap img').css({
          'width': `${imageW}`,
          'transform': `translate3d(0, ${transY}px, 0) scale3d(${zoom}, ${zoom}, 1)`
        })
        $('html').css('overflow', 'hidden')

        $('.image-view-wrap').on('click', function() {
          $(this).remove()
          $('html').attr('style', '')
        })
      }
    })

    // qrcode
    var qr = new QRious({
      element: document.getElementById('share-qrcode'),
      value: document.location.href
    });

    // gitment
    var gitmentConfig = "iphysresearch";
    if (gitmentConfig !== 'undefined') {
      var gitment = new Gitment({
        id: "一段关于神经网络的故事：高速运转的加强版神经网络",
        owner: "iphysresearch",
        repo: "iphysresearch.github.io",
        oauth: {
          client_id: "6b978dc207dc30e58ec8",
          client_secret: "2bc56895d0221e8c27ab87b072f8f18523231e22"
        },
        theme: {
          render(state, instance) {
            const container = document.createElement('div')
            container.lang = "en-US"
            container.className = 'gitment-container gitment-root-container'
            container.appendChild(instance.renderHeader(state, instance))
            container.appendChild(instance.renderEditor(state, instance))
            container.appendChild(instance.renderComments(state, instance))
            container.appendChild(instance.renderFooter(state, instance))
            return container;
          }
        }
      })
      gitment.render(document.getElementById('comments'))
    }
  })();
</script>

    <div class="scroll-top">
  <span class="arrow-icon"></span>
</div>
    <footer class="app-footer">
  <p class="copyright">
    &copy; 2018 | Proudly powered by <a href="https://hexo.io" target="_blank">Hexo</a>
    <br>
    Theme by <a href="https://github.com/yanm1ng">yanm1ng</a>
  </p>
</footer>

<script>
  function async(u, c) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }
</script>
<script>
  async("//cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
    FastClick.attach(document.body);
  })
</script>

<script>
  var hasLine = 'true';
  async("//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js", function(){
    $('figure pre').each(function(i, block) {
      var figure = $(this).parents('figure');
      if (hasLine === 'false') {
        figure.find('.gutter').hide();
      }
      var lang = figure.attr('class').split(' ')[1] || 'code';
      var codeHtml = $(this).html();
      var codeTag = document.createElement('code');
      codeTag.className = lang;
      codeTag.innerHTML = codeHtml;
      $(this).attr('class', '').empty().html(codeTag);
      figure.attr('data-lang', lang.toUpperCase());
      hljs.highlightBlock(block);
    });
  })
</script>
<!-- Baidu Tongji -->

<script src="/js/script.js"></script>
  </body>
</html>