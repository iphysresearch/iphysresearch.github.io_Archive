<!DOCTYPE html>
<html lang="en">
<head>
    <script src="https://distill.pub/template.v2.js"></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.0.17/webcomponents-loader.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.0.17/webcomponents-hi.js"></script>
    <style id="distill-prerendered-styles" type="text/css">/*
        * Copyright 2018 The Distill Template Authors
        *
        * Licensed under the Apache License, Version 2.0 (the "License");
        * you may not use this file except in compliance with the License.
        * You may obtain a copy of the License at
        *
        *      http://www.apache.org/licenses/LICENSE-2.0
        *
        * Unless required by applicable law or agreed to in writing, software
        * distributed under the License is distributed on an "AS IS" BASIS,
        * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        * See the License for the specific language governing permissions and
        * limitations under the License.
        */
       
       html {
         font-size: 14px;
           line-height: 1.6em;
         /* font-family: "Libre Franklin", "Helvetica Neue", sans-serif; */
         font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
         /*, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";*/
         text-size-adjust: 100%;
         -ms-text-size-adjust: 100%;
         -webkit-text-size-adjust: 100%;
       }
       
       @media(min-width: 768px) {
         html {
           font-size: 16px;
         }
       }
       
       body {
         margin: 0;
       }
       
       a {
         color: #004276;
       }
       
       figure {
         margin: 0;
       }
       
       table {
           border-collapse: collapse;
           border-spacing: 0;
       }
       
       table th {
           text-align: left;
       }
       
       table thead {
         border-bottom: 1px solid rgba(0, 0, 0, 0.05);
       }
       
       table thead th {
         padding-bottom: 0.5em;
       }
       
       table tbody :first-child td {
         padding-top: 0.5em;
       }
       
       pre {
         overflow: auto;
         max-width: 100%;
       }
       
       p {
         margin-top: 0;
         margin-bottom: 1em;
       }
       
       sup, sub {
         vertical-align: baseline;
         position: relative;
         top: -0.4em;
         line-height: 1em;
       }
       
       sub {
         top: 0.4em;
       }
       
       .kicker,
       .marker {
         font-size: 15px;
         font-weight: 600;
         color: rgba(0, 0, 0, 0.5);
       }
       
       
       /* Headline */
       
       @media(min-width: 1024px) {
         d-title h1 span {
           display: block;
         }
       }
       
       /* Figure */
       
       figure {
         position: relative;
         margin-bottom: 2.5em;
         margin-top: 1.5em;
       }
       
       figcaption+figure {
       
       }
       
       figure img {
         width: 100%;
       }
       
       figure svg text,
       figure svg tspan {
       }
       
       figcaption,
       .figcaption {
         color: rgba(0, 0, 0, 0.6);
         font-size: 12px;
         line-height: 1.5em;
       }
       
       @media(min-width: 1024px) {
       figcaption,
       .figcaption {
           font-size: 13px;
         }
       }
       
       figure.external img {
         background: white;
         border: 1px solid rgba(0, 0, 0, 0.1);
         box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
         padding: 18px;
         box-sizing: border-box;
       }
       
       figcaption a {
         color: rgba(0, 0, 0, 0.6);
       }
       
       figcaption b,
       figcaption strong, {
         font-weight: 600;
         color: rgba(0, 0, 0, 1.0);
       }
       /*
        * Copyright 2018 The Distill Template Authors
        *
        * Licensed under the Apache License, Version 2.0 (the "License");
        * you may not use this file except in compliance with the License.
        * You may obtain a copy of the License at
        *
        *      http://www.apache.org/licenses/LICENSE-2.0
        *
        * Unless required by applicable law or agreed to in writing, software
        * distributed under the License is distributed on an "AS IS" BASIS,
        * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        * See the License for the specific language governing permissions and
        * limitations under the License.
        */
       
       @supports not (display: grid) {
         .base-grid,
         distill-header,
         d-title,
         d-abstract,
         d-article,
         d-appendix,
         distill-appendix,
         d-byline,
         d-footnote-list,
         d-citation-list,
         distill-footer {
           display: block;
           padding: 8px;
         }
       }
       
       .base-grid,
       distill-header,
       d-title,
       d-abstract,
       d-article,
       d-appendix,
       distill-appendix,
       d-byline,
       d-footnote-list,
       d-citation-list,
       distill-footer {
         display: grid;
         justify-items: stretch;
         grid-template-columns: [screen-start] 8px [page-start kicker-start text-start gutter-start middle-start] 1fr 1fr 1fr 1fr 1fr 1fr 1fr 1fr [text-end page-end gutter-end kicker-end middle-end] 8px [screen-end];
         grid-column-gap: 8px;
       }
       
       .grid {
         display: grid;
         grid-column-gap: 8px;
       }
       
       @media(min-width: 768px) {
         .base-grid,
         distill-header,
         d-title,
         d-abstract,
         d-article,
         d-appendix,
         distill-appendix,
         d-byline,
         d-footnote-list,
         d-citation-list,
         distill-footer {
           grid-template-columns: [screen-start] 1fr [page-start kicker-start middle-start text-start] 45px 45px 45px 45px 45px 45px 45px 45px [ kicker-end text-end gutter-start] 45px [middle-end] 45px [page-end gutter-end] 1fr [screen-end];
           grid-column-gap: 16px;
         }
       
         .grid {
           grid-column-gap: 16px;
         }
       }
       
       @media(min-width: 1000px) {
         .base-grid,
         distill-header,
         d-title,
         d-abstract,
         d-article,
         d-appendix,
         distill-appendix,
         d-byline,
         d-footnote-list,
         d-citation-list,
         distill-footer {
           grid-template-columns: [screen-start] 1fr [page-start kicker-start] 50px [middle-start] 50px [text-start kicker-end] 50px 50px 50px 50px 50px 50px 50px 50px [text-end gutter-start] 50px [middle-end] 50px [page-end gutter-end] 1fr [screen-end];
           grid-column-gap: 16px;
         }
       
         .grid {
           grid-column-gap: 16px;
         }
       }
       
       @media(min-width: 1180px) {
         .base-grid,
         distill-header,
         d-title,
         d-abstract,
         d-article,
         d-appendix,
         distill-appendix,
         d-byline,
         d-footnote-list,
         d-citation-list,
         distill-footer {
           grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
           grid-column-gap: 32px;
         }
       
         .grid {
           grid-column-gap: 32px;
         }
       }
       
       
       
       
       .base-grid {
         grid-column: screen;
       }
       
       /* .l-body,
       d-article > *  {
         grid-column: text;
       }
       
       .l-page,
       d-title > *,
       d-figure {
         grid-column: page;
       } */
       
       .l-gutter {
         grid-column: gutter;
       }
       
       .l-text,
       .l-body {
         grid-column: text;
       }
       
       .l-page {
         grid-column: page;
       }
       
       .l-body-outset {
         grid-column: middle;
       }
       
       .l-page-outset {
         grid-column: page;
       }
       
       .l-screen {
         grid-column: screen;
       }
       
       .l-screen-inset {
         grid-column: screen;
         padding-left: 16px;
         padding-left: 16px;
       }
       
       
       /* Aside */
       
       d-article aside {
         grid-column: gutter;
         font-size: 12px;
         line-height: 1.6em;
         color: rgba(0, 0, 0, 0.6)
       }
       
       @media(min-width: 768px) {
         aside {
           grid-column: gutter;
         }
       
         .side {
           grid-column: gutter;
         }
       }
       /*
        * Copyright 2018 The Distill Template Authors
        *
        * Licensed under the Apache License, Version 2.0 (the "License");
        * you may not use this file except in compliance with the License.
        * You may obtain a copy of the License at
        *
        *      http://www.apache.org/licenses/LICENSE-2.0
        *
        * Unless required by applicable law or agreed to in writing, software
        * distributed under the License is distributed on an "AS IS" BASIS,
        * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        * See the License for the specific language governing permissions and
        * limitations under the License.
        */
       
       d-title {
         padding: 2rem 0 1.5rem;
         contain: layout style;
         overflow-x: hidden;
       }
       
       @media(min-width: 768px) {
         d-title {
           padding: 4rem 0 1.5rem;
         }
       }
       
       d-title h1 {
         grid-column: text;
         font-size: 40px;
         font-weight: 700;
         line-height: 1.1em;
         margin: 0 0 0.5rem;
       }
       
       @media(min-width: 768px) {
         d-title h1 {
           font-size: 50px;
         }
       }
       
       d-title p {
         font-weight: 300;
         font-size: 1.2rem;
         line-height: 1.55em;
         grid-column: text;
       }
       
       d-title .status {
         margin-top: 0px;
         font-size: 12px;
         color: #009688;
         opacity: 0.8;
         grid-column: kicker;
       }
       
       d-title .status span {
         line-height: 1;
         display: inline-block;
         padding: 6px 0;
         border-bottom: 1px solid #80cbc4;
         font-size: 11px;
         text-transform: uppercase;
       }
       /*
        * Copyright 2018 The Distill Template Authors
        *
        * Licensed under the Apache License, Version 2.0 (the "License");
        * you may not use this file except in compliance with the License.
        * You may obtain a copy of the License at
        *
        *      http://www.apache.org/licenses/LICENSE-2.0
        *
        * Unless required by applicable law or agreed to in writing, software
        * distributed under the License is distributed on an "AS IS" BASIS,
        * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        * See the License for the specific language governing permissions and
        * limitations under the License.
        */
       
       d-byline {
         contain: style;
         overflow: hidden;
         border-top: 1px solid rgba(0, 0, 0, 0.1);
         font-size: 0.8rem;
         line-height: 1.8em;
         padding: 1.5rem 0;
         min-height: 1.8em;
       }
       
       
       d-byline .byline {
         grid-template-columns: 1fr 1fr;
         grid-column: text;
       }
       
       @media(min-width: 768px) {
         d-byline .byline {
           grid-template-columns: 1fr 1fr 1fr 1fr;
         }
       }
       
       d-byline .authors-affiliations {
         grid-column-end: span 2;
         grid-template-columns: 1fr 1fr;
         margin-bottom: 1em;
       }
       
       @media(min-width: 768px) {
         d-byline .authors-affiliations {
           margin-bottom: 0;
         }
       }
       
       d-byline h3 {
         font-size: 0.6rem;
         font-weight: 400;
         color: rgba(0, 0, 0, 0.5);
         margin: 0;
         text-transform: uppercase;
       }
       
       d-byline p {
         margin: 0;
       }
       
       d-byline a,
       d-article d-byline a {
         color: rgba(0, 0, 0, 0.8);
         text-decoration: none;
         border-bottom: none;
       }
       
       d-article d-byline a:hover {
         text-decoration: underline;
         border-bottom: none;
       }
       
       d-byline p.author {
         font-weight: 500;
       }
       
       d-byline .affiliations {
       
       }
       /*
        * Copyright 2018 The Distill Template Authors
        *
        * Licensed under the Apache License, Version 2.0 (the "License");
        * you may not use this file except in compliance with the License.
        * You may obtain a copy of the License at
        *
        *      http://www.apache.org/licenses/LICENSE-2.0
        *
        * Unless required by applicable law or agreed to in writing, software
        * distributed under the License is distributed on an "AS IS" BASIS,
        * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        * See the License for the specific language governing permissions and
        * limitations under the License.
        */
       
       d-article {
         contain: layout style;
         overflow-x: hidden;
         border-top: 1px solid rgba(0, 0, 0, 0.1);
         padding-top: 2rem;
         color: rgba(0, 0, 0, 0.8);
       }
       
       d-article > * {
         grid-column: text;
       }
       
       @media(min-width: 768px) {
         d-article {
           font-size: 16px;
         }
       }
       
       @media(min-width: 1024px) {
         d-article {
           font-size: 1.06rem;
           line-height: 1.7em;
         }
       }
       
       
       /* H2 */
       
       
       d-article .marker {
         text-decoration: none;
         border: none;
         counter-reset: section;
         grid-column: kicker;
         line-height: 1.7em;
       }
       
       d-article .marker:hover {
         border: none;
       }
       
       d-article .marker span {
         padding: 0 3px 4px;
         border-bottom: 1px solid rgba(0, 0, 0, 0.2);
         position: relative;
         top: 4px;
       }
       
       d-article .marker:hover span {
         color: rgba(0, 0, 0, 0.7);
         border-bottom: 1px solid rgba(0, 0, 0, 0.7);
       }
       
       d-article h2 {
         font-weight: 600;
         font-size: 24px;
         line-height: 1.25em;
         margin: 2rem 0 1.5rem 0;
         border-bottom: 1px solid rgba(0, 0, 0, 0.1);
         padding-bottom: 1rem;
       }
       
       @media(min-width: 1024px) {
         d-article h2 {
           font-size: 36px;
         }
       }
       
       /* H3 */
       
       d-article h3 {
         font-weight: 700;
         font-size: 18px;
         line-height: 1.4em;
         margin-bottom: 1em;
         margin-top: 2em;
       }
       
       @media(min-width: 1024px) {
         d-article h3 {
           font-size: 20px;
         }
       }
       
       /* H4 */
       
       d-article h4 {
         font-weight: 600;
         text-transform: uppercase;
         font-size: 14px;
         line-height: 1.4em;
       }
       
       d-article a {
         color: inherit;
       }
       
       d-article p,
       d-article ul,
       d-article ol,
       d-article blockquote {
         margin-top: 0;
         margin-bottom: 1em;
         margin-left: 0;
         margin-right: 0;
       }
       
       d-article blockquote {
         border-left: 2px solid rgba(0, 0, 0, 0.2);
         padding-left: 2em;
         font-style: italic;
         color: rgba(0, 0, 0, 0.6);
       }
       
       d-article a {
         border-bottom: 1px solid rgba(0, 0, 0, 0.4);
         text-decoration: none;
       }
       
       d-article a:hover {
         border-bottom: 1px solid rgba(0, 0, 0, 0.8);
       }
       
       d-article .link {
         text-decoration: underline;
         cursor: pointer;
       }
       
       d-article ul,
       d-article ol {
         padding-left: 24px;
       }
       
       d-article li {
         margin-bottom: 1em;
         margin-left: 0;
         padding-left: 0;
       }
       
       d-article li:last-child {
         margin-bottom: 0;
       }
       
       d-article pre {
         font-size: 14px;
         margin-bottom: 20px;
       }
       
       d-article hr {
         grid-column: screen;
         width: 100%;
         border: none;
         border-bottom: 1px solid rgba(0, 0, 0, 0.1);
         margin-top: 60px;
         margin-bottom: 60px;
       }
       
       d-article section {
         margin-top: 60px;
         margin-bottom: 60px;
       }
       
       d-article span.equation-mimic {
         font-family: georgia;
         font-size: 115%;
         font-style: italic;
       }
       
       d-article > d-code,
       d-article section > d-code  {
         display: block;
       }
       
       d-article > d-math[block],
       d-article section > d-math[block]  {
         display: block;
       }
       
       @media (max-width: 768px) {
         d-article > d-code,
         d-article section > d-code,
         d-article > d-math[block],
         d-article section > d-math[block] {
             overflow-x: scroll;
             -ms-overflow-style: none;  // IE 10+
             overflow: -moz-scrollbars-none;  // Firefox
         }
       
         d-article > d-code::-webkit-scrollbar,
         d-article section > d-code::-webkit-scrollbar,
         d-article > d-math[block]::-webkit-scrollbar,
         d-article section > d-math[block]::-webkit-scrollbar {
           display: none;  // Safari and Chrome
         }
       }
       
       d-article .citation {
         color: #668;
         cursor: pointer;
       }
       
       d-include {
         width: auto;
         display: block;
       }
       
       d-figure {
         contain: layout style;
       }
       
       /* KaTeX */
       
       .katex, .katex-prerendered {
         contain: style;
         display: inline-block;
       }
       
       /* Tables */
       
       d-article table {
         border-collapse: collapse;
         margin-bottom: 1.5rem;
         border-bottom: 1px solid rgba(0, 0, 0, 0.2);
       }
       
       d-article table th {
         border-bottom: 1px solid rgba(0, 0, 0, 0.2);
       }
       
       d-article table td {
         border-bottom: 1px solid rgba(0, 0, 0, 0.05);
       }
       
       d-article table tr:last-of-type td {
         border-bottom: none;
       }
       
       d-article table th,
       d-article table td {
         font-size: 15px;
         padding: 2px 8px;
       }
       
       d-article table tbody :first-child td {
         padding-top: 2px;
       }
       /*
        * Copyright 2018 The Distill Template Authors
        *
        * Licensed under the Apache License, Version 2.0 (the "License");
        * you may not use this file except in compliance with the License.
        * You may obtain a copy of the License at
        *
        *      http://www.apache.org/licenses/LICENSE-2.0
        *
        * Unless required by applicable law or agreed to in writing, software
        * distributed under the License is distributed on an "AS IS" BASIS,
        * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        * See the License for the specific language governing permissions and
        * limitations under the License.
        */
       
       span.katex-display {
         text-align: left;
         padding: 8px 0 8px 0;
         margin: 0.5em 0 0.5em 1em;
       }
       
       span.katex {
         -webkit-font-smoothing: antialiased;
         color: rgba(0, 0, 0, 0.8);
         font-size: 1.18em;
       }
       /*
        * Copyright 2018 The Distill Template Authors
        *
        * Licensed under the Apache License, Version 2.0 (the "License");
        * you may not use this file except in compliance with the License.
        * You may obtain a copy of the License at
        *
        *      http://www.apache.org/licenses/LICENSE-2.0
        *
        * Unless required by applicable law or agreed to in writing, software
        * distributed under the License is distributed on an "AS IS" BASIS,
        * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        * See the License for the specific language governing permissions and
        * limitations under the License.
        */
       
       @media print {
       
         @page {
           size: 8in 11in;
           @bottom-right {
             content: counter(page) " of " counter(pages);
           }
         }
       
         html {
           /* no general margins -- CSS Grid takes care of those */
         }
       
         p, code {
           page-break-inside: avoid;
         }
       
         h2, h3 {
           page-break-after: avoid;
         }
       
         d-header {
           visibility: hidden;
         }
       
         d-footer {
           display: none!important;
         }
       
       }
       </style>
    <style>.subgrid {
            grid-column: screen; 
            display: grid; 
            grid-template-columns: inherit;
            grid-template-rows: inherit;
            grid-column-gap: inherit;
            grid-row-gap: inherit;
          }
          
          d-figure.base-grid {
            grid-column: screen;
            background: hsl(0, 0%, 97%);
            padding: 20px 0;
            border-top: 1px solid rgba(0, 0, 0, 0.1);
            border-bottom: 1px solid rgba(0, 0, 0, 0.1);
          }
          
          d-figure {
            margin-bottom: 1em;
            position: relative;
          }
          
          d-figure > figure {
            margin-top: 0;
            margin-bottom: 0;
          }
          
          .shaded-figure {
            background-color: hsl(0, 0%, 97%);
            border-top: 1px solid hsla(0, 0%, 0%, 0.1);
            border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
            padding: 30px 0;
          }
          
          .pointer {
            position: absolute;
            width: 26px;
            height: 26px;
            top: 26px;
            left: -48px;
          }
          
          .figure-element, .figure-line, .figure-path {
            stroke: #666;
            stroke-miterlimit: 10px;
            stroke-width: 1.5px;
          }
          
          .figure-element {
            fill: #fff;
            fill-opacity: 0.8;
          }
          
          .figure-line {
            fill: none;
          }
          
          .figure-path {
            fill: #666;
            stroke-width: 1px;
          }
          
          .figure-group {
            fill: #f9f9f9;
            stroke: #666;
            stroke-width: 1.5px;
            stroke-opacity: 0.6;
            stroke-miterlimit: 10px;
          }
          
          .figure-faded {
            opacity: 0.35;
          }
          
          .figure-box {
            rx: 6px;
            ry: 6px;
          }
          
          .figure-dashed {
            stroke: #666;
            stroke-width: 1.5px;
            stroke-miterlimit: 10px;
            stroke-dasharray: 5, 5;
          }
          
          .figure-text {
            fill: #000;
            opacity: 0.6;
            font-size: 13px;
          }
          
          .figure-text-faded {
            opacity: 0.35;
          }
          
          .figure-large-text {
            font-size: 18px;
          }
          
          .subscript {
            font-size: 8px;
          }
          
          .figure-film-generator {
            /* stroke: #006064; */
            /* fill: #80DEEA; */
            
            stroke: hsl(203, 65%, 70%);
            fill: hsl(203, 65%, 85%);
          }
          
          .figure-film-generator-shaded {
            /* stroke: #006064; */
            /* fill: #00838F; */
            
            stroke: hsl(203, 15%, 85%);
            fill: hsl(203, 15%, 95%);
          }
          
          .figure-filmed-network {
            /* stroke: #BF360C; */
            /* fill: #FFAB91; */
            
            stroke: hsl(11, 65%, 70%);
            fill: rgb(242, 203, 194);
          }
          
          .todo {
            color: red;
          }
          
          
          .tooltip {
            position: absolute;
            max-width: 300px;
            max-height: 300px;
            pointer-events: none;
            transition: opacity;
          }
          
          .collapsible {
            cursor: pointer;
            padding-top: 12px;
            padding-bottom: 12px;
            width: 100%;
            border: none;
            text-align: left;
            outline: none;
            font-size: 25px;
            font-weight: 700;
            background-color: white;
            color: rgba(0, 0, 0, 0.8);
            padding: 0.5em;
            margin: 0.2em;
            margin-left: 0;
            padding-left: 0;
            transform: translateX(0px);
            transition: 
                color 0.1s ease-out,
                transform 0.25s ease;
          }
          
          .collapsible:hover {
            border-bottom: 1px solid inset;
            color: rgba(0, 0, 0, 0.4);
            transform: translateX(10px);
            transition: 
                transform 0.25s ease;
          }
          
          d-article .content {
            display: none;
            overflow: hidden;
            background-color: none;
          }
          
          .expand-collapse-button {
            cursor: pointer;
            border: none;
            outline: none;
            font-size: 18px;
            font-weight: 700;
            float: right;
          }
          
          #clevr-plot-svg {
           width:440px;
           height:400px
          }
          
          #style-transfer-plot-svg {
           width:440px;
           height:400px
          }
    </style>
       <script src="https://d3js.org/d3.v4.min.js"></script>
       <script src="https://d3js.org/d3-selection-multi.v1.min.js"></script>
       <script src="https://ariutta.github.io/svg-pan-zoom/dist/svg-pan-zoom.js"></script>
       <link rel="stylesheet" href="https://distill.pub/third-party/katex/katex.min.css" crossorigin="anonymous">
       <link rel="icon" type="image/png" href="">
       <!-- <link href="/rss.xml" rel="alternate" type="application/rss+xml" title="Articles from Distill"> -->
       <title>Feature-wise transformations</title>
       <link rel="canonical" href="https://distill.pub/2018/feature-wise-transformations">

    <!--  https://schema.org/Article -->
    <meta property="description" itemprop="description" content="A simple and surprisingly effective family of conditioning mechanisms.">
    <meta property="article:published" itemprop="datePublished" content="2018-07-09">
    <meta property="article:created" itemprop="dateCreated" content="2018-07-09">
    
    <meta property="article:modified" itemprop="dateModified" content="2018-07-10T22:41:12.000Z">
    
    <meta property="article:author" content="Vincent Dumoulin">
    <meta property="article:author" content="Ethan Perez">
    <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="Feature-wise transformations">
    <meta property="og:description" content="A simple and surprisingly effective family of conditioning mechanisms.">
    <meta property="og:url" content="https://distill.pub/2018/feature-wise-transformations">
    <meta property="og:image" content="https://distill.pub/2018/feature-wise-transformations/thumbnail.jpg">
    <meta property="og:locale" content="en_US">
    <meta property="og:site_name" content="Distill">
  
    <!--  https://dev.twitter.com/cards/types/summary -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Feature-wise transformations">
    <meta name="twitter:description" content="A simple and surprisingly effective family of conditioning mechanisms.">
    <meta name="twitter:url" content="https://distill.pub/2018/feature-wise-transformations">
    <meta name="twitter:image" content="https://distill.pub/2018/feature-wise-transformations/thumbnail.jpg">
    <meta name="twitter:image:width" content="560">
    <meta name="twitter:image:height" content="295">
  
      <!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
    <meta name="citation_title" content="Feature-wise transformations">
    <meta name="citation_fulltext_html_url" content="https://distill.pub/2018/feature-wise-transformations">
    <meta name="citation_volume" content="3">
    <meta name="citation_issue" content="7">
    <meta name="citation_firstpage" content="e11">
    <meta name="citation_doi" content="10.23915/distill.00011">
    <meta name="citation_journal_title" content="Distill">
    <meta name="citation_journal_abbrev" content="Distill">
    <meta name="citation_issn" content="2476-0757">
    <meta name="citation_fulltext_world_readable" content="">
    <meta name="citation_online_date" content="2018/07/09">
    <meta name="citation_publication_date" content="2018/07/09">
    <meta name="citation_author" content="Dumoulin, Vincent">
    <meta name="citation_author" content="Perez, Ethan">
    <meta name="citation_author" content="Schucher, Nathan">
    <meta name="citation_author" content="Strub, Florian">
    <meta name="citation_author" content="Vries, Harm de">
    <meta name="citation_author" content="Courville, Aaron">
    <meta name="citation_author" content="Bengio, Yoshua">
    <meta name="citation_reference" content="citation_title=FiLM: Visual Reasoning with a General Conditioning Layer;citation_author=Ethan Perez;citation_author=Florian Strub;citation_author=Harm de Vries;citation_author=Vincent Dumoulin;citation_author=Aaron Courville;citation_publication_date=2018;citation_arxiv_id=1709.07871;">
    <meta name="citation_reference" content="citation_title=Learning visual reasoning without strong priors;citation_author=Ethan Perez;citation_author=Harm de Vries;citation_author=Florian Strub;citation_author=Vincent Dumoulin;citation_author=Aaron Courville;citation_publication_date=2017;citation_arxiv_id=1707.03017;">
    <meta name="citation_reference" content="citation_title=CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning;citation_author=Justin Johnson;citation_author=Fei-Fei Li;citation_author=Bharath Hariharan;citation_author=Lawrence C. Zitnick;citation_author=Laurens van der Maaten;citation_author=Ross Girshick;citation_publication_date=2017;citation_arxiv_id=1612.06890;">
    <meta name="citation_reference" content="citation_title=Visual Reasoning with Multi-hop Feature Modulation;citation_author=Florian Strub;citation_author=Mathieu Seurin;citation_author=Ethan Perez;citation_author=Harm de Vries;citation_author=J\'{e}r\'{e}mie Mary;citation_author=Philippe Preux;citation_author=Aaron Courville;citation_author=Olivier Pietquin;citation_publication_date=2018;">
    <meta name="citation_reference" content="citation_title=GuessWhat?! Visual object discovery through multi-modal dialogue;citation_author=Harm de Vries;citation_author=Florian Strub;citation_author=Sarath Chandar;citation_author=Olivier Pietquin;citation_author=Hugo Larochelle;citation_author=Aaron Courville;citation_publication_date=2017;citation_arxiv_id=1611.08481;">
    <meta name="citation_reference" content="citation_title=ReferItGame: Referring to objects in photographs of natural scenes;citation_author=Sahar Kazemzadeh;citation_author=Vicente Ordonez;citation_author=Mark Matten;citation_author=Tamara Berg;citation_publication_date=2014;">
    <meta name="citation_reference" content="citation_title=Modulating early visual processing by language;citation_author=Harm de Vries;citation_author=Florian Strub;citation_author=J\'{e}r\'{e}mie Mary;citation_author=Hugo Larochelle;citation_author=Olivier Pietquin;citation_author=Aaron Courville;citation_publication_date=2017;citation_arxiv_id=1707.00683;">
    <meta name="citation_reference" content="citation_title=VQA: visual question answering;citation_author=Aishwarya Agrawal;citation_author=Jiasen Lu;citation_author=Stanislaw Antol;citation_author=Margaret Mitchell;citation_author=C. Lawrence Zitnick;citation_author=Dhruv Batra;citation_author=Devi Parikh;citation_publication_date=2015;citation_arxiv_id=1505.00468;">
    <meta name="citation_reference" content="citation_title=A learned representation for artistic style;citation_author=Vincent Dumoulin;citation_author=Jonathon Shlens;citation_author=Manjunath Kudlur;citation_publication_date=2017;citation_arxiv_id=1610.07629;">
    <meta name="citation_reference" content="citation_title=Exploring the structure of a real-time, arbitrary neural artistic stylization network;citation_author=Golnaz Ghiasi;citation_author=Honglak Lee;citation_author=Manjunath Kudlur;citation_author=Vincent Dumoulin;citation_author=Jonathon Shlens;citation_publication_date=2017;citation_arxiv_id=1705.06830;">
    <meta name="citation_reference" content="citation_title=Efficient video object segmentation via network modulation;citation_author=Linjie Yang;citation_author=Yanran Wang;citation_author=Xuehan Xiong;citation_author=Jianchao Yang;citation_author=Aggelos K. Katsaggelos;citation_publication_date=2018;citation_arxiv_id=1802.01218;">
    <meta name="citation_reference" content="citation_title=Arbitrary style transfer in real-time with adaptive instance normalization;citation_author=Xun Huang;citation_author=Serge Belongie;citation_publication_date=2017;citation_arxiv_id=1703.06868;">
    <meta name="citation_reference" content="citation_title=Highway networks;citation_author=Rupesh Kumar Srivastava;citation_author=Klaus Greff;citation_author=Jurgen Schmidhuber;citation_publication_date=2015;citation_arxiv_id=1505.00387;">
    <meta name="citation_reference" content="citation_title=Long short-term memory;citation_author=Sepp Hochreiter;citation_author=Jurgen Schmidhuber;citation_publication_date=1997;citation_journal_title=Neural Computation;citation_volume=9;citation_number=8;">
    <meta name="citation_reference" content="citation_title=Squeeze-and-Excitation networks;citation_author=Jie Hu;citation_author=Li Shen;citation_author=Gang Sun;citation_publication_date=2017;citation_arxiv_id=1709.01507;">
    <meta name="citation_reference" content="citation_title=On the state of the art of evaluation in neural language models;citation_author=G{\'{a}}bor Melis;citation_author=Chris Dyer;citation_author=Phil Blunsom;citation_publication_date=2017;citation_arxiv_id=1707.05589;">
    <meta name="citation_reference" content="citation_title=Language modeling with gated convolutional networks;citation_author=Yann N Dauphin;citation_author=Angela Fan;citation_author=Michael Auli;citation_author=David Grangier;citation_publication_date=2017;citation_arxiv_id=1612.08083;">
    <meta name="citation_reference" content="citation_title=Convolution sequence-to-sequence learning;citation_author=Jonas Gehring;citation_author=Michael Auli;citation_author=David Grangier;citation_author=Denis Yarats;citation_author=Yann N. Dauphin;citation_publication_date=2017;citation_arxiv_id=1705.03122;">
    <meta name="citation_reference" content="citation_title=Gated-attention readers for text comprehension;citation_author=Bhuwan Dhingra;citation_author=Hanxiao Liu;citation_author=Zhilin Yang;citation_author=William W Cohen;citation_author=Ruslan Salakhutdinov;citation_publication_date=2017;citation_arxiv_id=1606.01549;">
    <meta name="citation_reference" content="citation_title=Gated-attention architectures for task-oriented language grounding;citation_author=Devendra Singh Chaplot;citation_author=Kanthashree Mysore Sathyendra;citation_author=Rama Kumar Pasumarthi;citation_author=Dheeraj Rajagopal;citation_author=Ruslan Salakhutdinov;citation_publication_date=2017;citation_arxiv_id=1706.07230;">
    <meta name="citation_reference" content="citation_title=Vizdoom: A doom-based AI research platform for visual reinforcement learning;citation_author=Michał Kempka;citation_author=Marek Wydmuch;citation_author=Grzegorz Runc;citation_author=Jakub Toczek;citation_author=Wojciech Jaśkowski;citation_publication_date=2016;citation_arxiv_id=1605.02097;">
    <meta name="citation_reference" content="citation_title=Learning to follow language instructions with adversarial reward induction;citation_author=Dzmitry Bahdanau;citation_author=Felix Hill;citation_author=Jan Leike;citation_author=Edward Hughes;citation_author=Pushmeet Kohli;citation_author=Edward Grefenstette;citation_publication_date=2018;citation_arxiv_id=1806.01946;">
    <meta name="citation_reference" content="citation_title=Neural module networks;citation_author=Jacob Andreas;citation_author=Marcus Rohrbach;citation_author=Trevor Darrell;citation_author=Dan Klein;citation_publication_date=2016;citation_arxiv_id=1511.02799;">
    <meta name="citation_reference" content="citation_title=Overcoming catastrophic forgetting in neural networks;citation_author=James Kirkpatrick;citation_author=Razvan Pascanu;citation_author=Neil Rabinowitz;citation_author=Joel Veness;citation_author=Guillaume Desjardins;citation_author=Andrei A. Rusu;citation_author=Kieran Milan;citation_author=John Quan;citation_author=Tiago Ramalho;citation_author=Agnieszka Grabska-Barwinska;citation_author=Demis Hassabis;citation_author=Claudia Clopath;citation_author=Dharshan Kumaran;citation_author=Raia Hadsell;citation_publication_date=2017;citation_journal_title=Proceedings of the National Academy of Sciences;citation_volume=114;citation_number=13;">
    <meta name="citation_reference" content="citation_title=Unsupervised representation learning with deep convolutional generative adversarial networks;citation_author=Alec Radford;citation_author=Luke Metz;citation_author=Soumith Chintala;citation_publication_date=2016;citation_arxiv_id=1511.06434;">
    <meta name="citation_reference" content="citation_title=Generative adversarial nets;citation_author=Ian Goodfellow;citation_author=Jean Pouget-Abadie;citation_author=Mehdi Mirza;citation_author=Bing Xu;citation_author=David Warde-Farley;citation_author=Sherjil Ozair;citation_author=Aaron Courville;citation_author=Yoshua Bengio;citation_publication_date=2014;">
    <meta name="citation_reference" content="citation_title=Conditional image generation with PixelCNN decoders;citation_author=Aaron van den Oord;citation_author=Nal Kalchbrenner;citation_author=Lasse Espeholt;citation_author=Oriol Vinyals;citation_author=Alex Graves;citation_author=Koray Kavukcuoglu;citation_publication_date=2016;citation_arxiv_id=1606.05328;">
    <meta name="citation_reference" content="citation_title=WaveNet: A generative model for raw audio;citation_author=Aaron van den Oord;citation_author=Sander Dieleman;citation_author=Heiga Zen;citation_author=Karen Simonyan;citation_author=Oriol Vinyals;citation_author=Alex Graves;citation_author=Nal Kalchbrenner;citation_author=Andrew Senior;citation_author=Koray Kavukcuoglu;citation_publication_date=2016;citation_arxiv_id=1609.03499;">
    <meta name="citation_reference" content="citation_title=Dynamic layer normalization for adaptive neural acoustic modeling in speech recognition;citation_author=Taesup Kim;citation_author=Inchul Song;citation_author=Yoshua Bengio;citation_publication_date=2017;citation_arxiv_id=1707.06065;">
    <meta name="citation_reference" content="citation_title=Adaptive batch normalization for practical domain adaptation;citation_author=Yanghao Li;citation_author=Naiyan Wang;citation_author=Jianping Shi;citation_author=Xiaodi Hou;citation_author=Jiaying Liu;citation_publication_date=2018;citation_journal_title=Pattern Recognition;citation_volume=80;">
    <meta name="citation_reference" content="citation_title=TADAM: Task dependent adaptive metric for improved few-shot learning;citation_author=Boris N. Oreshkin;citation_author=Pau Rodriguez;citation_author=Alexandre Lacoste;citation_publication_date=2018;citation_arxiv_id=1805.10123;">
    <meta name="citation_reference" content="citation_title=Prototypical networks for few-shot learning;citation_author=Jake Snell;citation_author=Kevin Swersky;citation_author=Richard Zemel;citation_publication_date=2017;citation_arxiv_id=1703.05175;">
    <meta name="citation_reference" content="citation_title=Devise: A deep visual-semantic embedding model;citation_author=Andrea Frome;citation_author=Greg S Corrado;citation_author=Jon Shlens;citation_author=Samy Bengio;citation_author=Jeff Dean;citation_author=Tomas Mikolov;citation_publication_date=2013;">
    <meta name="citation_reference" content="citation_title=Zero-shot learning through cross-modal transfer;citation_author=Richard Socher;citation_author=Milind Ganjoo;citation_author=Christopher D Manning;citation_author=Andrew Ng;citation_publication_date=2013;citation_arxiv_id=1301.3666;">
    <meta name="citation_reference" content="citation_title=Zero-shot learning by convex combination of semantic embeddings;citation_author=Mohammad Norouzi;citation_author=Tomas Mikolov;citation_author=Samy Bengio;citation_author=Yoram Singer;citation_author=Jonathon Shlens;citation_author=Andrea Frome;citation_author=Greg S Corrado;citation_author=Jeffrey Dean;citation_publication_date=2014;citation_arxiv_id=1312.5650;">
    <meta name="citation_reference" content="citation_title=HyperNetworks;citation_author=David Ha;citation_author=Andrew Dai;citation_author=Quoc Le;citation_publication_date=2016;citation_arxiv_id=1609.09106;">
    <meta name="citation_reference" content="citation_title=Separating style and content with bilinear models;citation_author=Joshua B. Tenenbaum;citation_author=William T. Freeman;citation_publication_date=2000;citation_journal_title=Neural Computation;citation_volume=12;citation_number=6;">
    <meta name="citation_reference" content="citation_title=Visualizing data using t-SNE;citation_author=Laurens van der Maaten;citation_author=Geoffrey Hinton;citation_publication_date=2008;citation_journal_title=Journal of machine learning research;citation_volume=9;citation_number=Nov;">
    <meta name="citation_reference" content="citation_title=A dataset and architecture for visual reasoning with a working memory;citation_author=Guangyu Robert Yang;citation_author=Igor Ganichev;citation_author=Xiao-Jing Wang;citation_author=Jonathon Shlens;citation_author=David Sussillo;citation_publication_date=2018;citation_arxiv_id=1803.06092;">
    <meta name="citation_reference" content="citation_title=A parallel computation that assigns canonical object-based frames of reference;citation_author=Geoffrey F. Hinton;citation_publication_date=1981;">
    <meta name="citation_reference" content="citation_title=The correlation theory of brain function;citation_author=Christoph von der Malsburg;citation_publication_date=1994;">
    <meta name="citation_reference" content="citation_title=Generating text with recurrent neural networks;citation_author=Ilya Sutskever;citation_author=James Martens;citation_author=Geoffrey Hinton;citation_publication_date=2011;">
    <meta name="citation_reference" content="citation_title=Robust boltzmann machines for recognition and denoising;citation_author=Y. Tang;citation_author=R. Salakhutdinov;citation_author=G. Hinton;citation_publication_date=2012;">
    <meta name="citation_reference" content="citation_title=Factored conditional restricted Boltzmann machines for modeling motion style;citation_author=Graham W. Taylor;citation_author=Geoffrey E. Hinton;citation_publication_date=2009;">
    <meta name="citation_reference" content="citation_title=Combining discriminative features to infer complex trajectories;citation_author=David A. Ross;citation_author=Simon Osindero;citation_author=Richard S. Zemel;citation_publication_date=2006;">
    <meta name="citation_reference" content="citation_title=Learning where to attend with deep architectures for image tracking;citation_author=Misha Denil;citation_author=Loris Bazzani;citation_author=Hugo Larochelle;citation_author=Nando de Freitas;citation_publication_date=2012;citation_journal_title=Neural Comput.;citation_volume=24;citation_number=8;">
    <meta name="citation_reference" content="citation_title=Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis;citation_author=Q. V. Le;citation_author=W. Y. Zou;citation_author=S. Y. Yeung;citation_author=A. Y. Ng;citation_publication_date=2011;">
    <meta name="citation_reference" content="citation_title=Convolutional learning of spatio-temporal features;citation_author=Graham W. Taylor;citation_author=Rob Fergus;citation_author=Yann LeCun;citation_author=Christoph Bregler;citation_publication_date=2010;">
    <meta name="citation_reference" content="citation_title=Learning to relate images;citation_author=R. Memisevic;citation_publication_date=2013;citation_journal_title=IEEE Transactions on Pattern Analysis and Machine Intelligence;citation_volume=35;citation_number=8;">
    <meta name="citation_reference" content="citation_title=Incorporating side information by adaptive convolution;citation_author=Di Kang;citation_author=Debarun Dhar;citation_author=Antoni Chan;citation_publication_date=2017;">
    <meta name="citation_reference" content="citation_title=Learning multiple visual domains with residual adapters;citation_author=Sylvestre-Alvise Rebuffi;citation_author=Hakan Bilen;citation_author=Andrea Vedaldi;citation_publication_date=2017;">
    <meta name="citation_reference" content="citation_title=Predicting deep zero-shot convolutional neural networks using textual descriptions;citation_author=Jimmy Ba;citation_author=Kevin Swersky;citation_author=Sanja Fidler;citation_author=Ruslan Salakhutdinov;citation_publication_date=2015;citation_arxiv_id=1506.00511;">
    <meta name="citation_reference" content="citation_title=Zero-shot task generalization with multi-task deep reinforcement learning;citation_author=Junhyuk Oh;citation_author=Satinder Singh;citation_author=Honglak Lee;citation_author=Pushmeet Kohli;citation_publication_date=2017;citation_arxiv_id=1706.05064;">
    <meta name="citation_reference" content="citation_title=Separating style and content;citation_author=Joshua B Tenenbaum;citation_author=William T Freeman;citation_publication_date=1997;">
    <meta name="citation_reference" content="citation_title=Facial expression space learning;citation_author=E. S. Chuang;citation_author=F. Deshpande;citation_author=C. Bregler;citation_publication_date=2002;">
    <meta name="citation_reference" content="citation_title=Personalized recommendation on dynamic content using predictive bilinear models;citation_author=Wei Chu;citation_author=Seung-Taek Park;citation_publication_date=2009;">
    <meta name="citation_reference" content="citation_title=Like like alike: joint friendship and interest propagation in social networks;citation_author=Shuang-Hong Yang;citation_author=Bo Long;citation_author=Alex Smola;citation_author=Narayanan Sadagopan;citation_author=Zhaohui Zheng;citation_author=Hongyuan Zha;citation_publication_date=2011;">
    <meta name="citation_reference" content="citation_title=Matrix factorization techniques for recommender systems;citation_author=Yehuda Koren;citation_author=Robert Bell;citation_author=Chris Volinsky;citation_publication_date=2009;citation_journal_title=Computer;citation_volume=42;citation_number=8;">
    <meta name="citation_reference" content="citation_title=Bilinear CNN models for fine-grained visual recognition;citation_author=Tsung-Yu Lin;citation_author=Aruni RoyChowdhury;citation_author=Subhransu Maji;citation_publication_date=2015;">
    <meta name="citation_reference" content="citation_title=Convolutional two-stream network fusion for video action recognition;citation_author=Christoph Feichtenhofer;citation_author=Axel Pinz;citation_author=AP Zisserman;citation_publication_date=2016;citation_arxiv_id=1604.06573;">
    <meta name="citation_reference" content="citation_title=Multimodal compact bilinear pooling for visual question answering and visual grounding;citation_author=Akira Fukui;citation_author=Dong Huk Park;citation_author=Daylen Yang;citation_author=Anna Rohrbach;citation_author=Trevor Darrell;citation_author=Marcus Rohrbach;citation_publication_date=2016;citation_arxiv_id=1606.01847;">
</head>

<body distill-prerendered="">
    <distill-header distill-prerendered="">
<style>
distill-header {
  position: relative;
  height: 60px;
  background-color: hsl(200, 60%, 15%);
  width: 100%;
  box-sizing: border-box;
  z-index: 2;
  color: rgba(0, 0, 0, 0.8);
  border-bottom: 1px solid rgba(0, 0, 0, 0.08);
  box-shadow: 0 1px 6px rgba(0, 0, 0, 0.05);
}
distill-header .content {
  height: 70px;
  grid-column: page;
}
distill-header a {
  font-size: 16px;
  height: 60px;
  line-height: 60px;
  text-decoration: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 22px 0;
}
distill-header a:hover {
  color: rgba(255, 255, 255, 1);
}
distill-header svg {
  width: 24px;
  position: relative;
  top: 4px;
  margin-right: 2px;
}
@media(min-width: 1080px) {
  distill-header {
    height: 70px;
  }
  distill-header a {
    height: 70px;
    line-height: 70px;
    padding: 28px 0;
  }
  distill-header .logo {
  }
}
distill-header svg path {
  fill: none;
  stroke: rgba(255, 255, 255, 0.8);
  stroke-width: 3px;
}
distill-header .logo {
  font-size: 17px;
  font-weight: 200;
}
distill-header .nav {
  float: right;
  font-weight: 300;
}
distill-header .nav a {
  font-size: 12px;
  margin-left: 24px;
  text-transform: uppercase;
}
</style>
        <div class="content">
            <a href="/" class="logo">
            <svg viewBox="-607 419 64 64">
                <path d="M-573.4,478.9c-8,0-14.6-6.4-14.6-14.5s14.6-25.9,14.6-40.8c0,14.9,14.6,32.8,14.6,40.8S-565.4,478.9-573.4,478.9z"></path>
            </svg>
            IPhysResearch
            </a>
            <nav class="nav">
            <a href="/about/">About</a>
            <a href="/prize/">Prize</a>
            <a href="/journal/">Submit</a>
            </nav>
        </div>
    </distill-header>

    <d-front-matter>
    <script type="text/json">{
        "title": "Feature-wise transformations",
        "description": "A simple and surprisingly effective family of conditioning mechanisms.",
        "authors": [
        {
            "author": "Vincent Dumoulin",
            "authorURL": "https://vdumoulin.github.io",
            "affiliations": [{"name": "Google Brain", "url": "https://ai.google/research/teams/brain"}]
        },
        {
            "author": "Ethan Perez",
            "authorURL": "http://ethanperez.net/",
            "affiliations": [
            {"name": "Rice University", "url": "http://www.rice.edu/"},
            {"name": "MILA", "url": "https://mila.quebec/en/"}
            ]
        },
        ],
        "katex": {
        "delimiters": [
            {
            "left": "$",
            "right": "$",
            "display": false
            },
            {
            "left": "$$",
            "right": "$$",
            "display": true
            }
        ]
        }
    }</script>
    </d-front-matter>

    <d-title>
        <h1>Feature-wise transformations</h1>
        <p>A simple and surprisingly effective family of conditioning mechanisms.</p>
        <div class="l-page" id="vtoc"></div>
    </d-title>

    <d-byline>
        <div class="byline grid">
            <div class="authors-affiliations grid">
                <h3>Authors</h3>
                <h3>Affiliations</h3>
            
                <p class="author">
                    <a class="name" href="https://vdumoulin.github.io">Vincent Dumoulin</a>
                </p>
                <p class="affiliation">
                    <a class="affiliation" href="https://ai.google/research/teams/brain">Google Brain</a>
                </p>
            
                <p class="author">
                    <a class="name" href="http://ethanperez.net/">Ethan Perez</a>
                </p>
                <p class="affiliation">
                    <a class="affiliation" href="http://www.rice.edu/">Rice University</a>, <a class="affiliation" href="https://mila.quebec/en/">MILA</a>
                </p>
            </div>
            <div>
                <h3>Published</h3>
                <p>July 9, 2018</p> 
            </div>
            <div>
                <h3>DOI</h3>
                <p><a href="https://doi.org/10.23915/distill.00011">10.23915/distill.00011</a></p>
            </div>
        </div>
    </d-byline>

    <d-article>
        <p>
            Many real-world problems require integrating multiple sources of information.
            Sometimes these problems involve multiple, distinct modalities of
            information — vision, language, audio, etc. — as is required
            to understand a scene in a movie or answer a question about an image.
            Other times, these problems involve multiple sources of the same
            kind of input, i.e. when summarizing several documents or drawing one
            image in the style of another.
        </p>    
        <figure class="l-body-outset">
            <svg viewBox="0 0 888 280" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                <defs>
                    <filter x="0" y="0" width="1" height="1" id="zoolander-text-background">
                        <feflood flood-color="#fdf6f2"></feflood>
                        <fecomposite in="SourceGraphic"></fecomposite>
                    </filter>
                </defs>
                <g transform="translate(70, 0)">
                    <text x="0" y="210" class="figure-text">
                        <tspan> Video and audio must be understood in the context of each </tspan>
                        <tspan x="0" dy="1.5em"> other to understand the scene. </tspan>
                        <tspan x="0" dy="2em" style="font-style: italic;"> Credit: still frame from the movie Charade. </tspan>
                    </text>
                    <image x="0" y="0" width="364" height="182" style="clip-path: inset(0 0 0 0 round 6px);" xlink:href="https://distill.pub/2018/feature-wise-transformations/images/charade.png"></image>
                </g>
                <g transform="translate(530, 0)">
                    <text x="0" y="210" class="figure-text">
                        <tspan> An image needs to be processed in the context of </tspan>
                        <tspan x="0" dy="1.5em"> a question being asked. </tspan>
                        <tspan x="0" dy="2em" style="font-style: italic;"> Credit: image-question pair from the CLEVR dataset. </tspan>
                    </text>
                    <image x="0" y="0" width="297.18" height="182" style="clip-path: inset(0 0 0 0 round 6px);" xlink:href="https://distill.pub/2018/feature-wise-transformations/images/clevr_image.jpg"></image>
                </g>
            </svg>
        </figure>

        <p>
            When approaching such problems, it often makes sense to process one source
            of information <em>in the context of</em> another; for instance, in the
            right example above, one can extract meaning from the image in the context
            of the question. In machine learning, we often refer to this context-based
            processing as <em>conditioning</em>: the computation carried out by a model
            is conditioned or <em>modulated</em> by information extracted from an
            auxiliary input.
        </p>

        <p>
            Finding an effective way to condition on or fuse sources of information
            is an open research problem, and
            <!-- Introduction -->
            in this article, we concentrate on a specific family of approaches we call
            <em>feature-wise transformations</em>.
            <!-- Related Work -->
            We will examine the use of feature-wise transformations in many neural network
            architectures to solve a surprisingly large and diverse set of problems;
            <!-- Experiments -->
            their success, we will argue, is due to being flexible enough to learn an
            effective representation of the conditioning input in varied settings.
            In the language of multi-task learning, where the conditioning signal is
            taken to be a task description, feature-wise transformations
            learn a task representation which allows them to capture and leverage the
            relationship between multiple sources of information, even in remarkably
            different problem settings.
        </p>
            
        <hr>
        <h2 id="feature-wise-transformations">(h2) Feature-wise transformations</h2>

        <p>
            To motivate feature-wise transformations, we start with a basic example,
            where the two inputs are images and category labels, respectively. For the
            purpose of this example, we are interested in building a generative model of
            images of various classes (puppy, boat, airplane, etc.). The model takes as
            input a class and a source of random noise (e.g., a vector sampled from a
            normal distribution) and outputs an image sample for the requested class.
        </p>        
        <figure class="l-body">
            <svg viewBox="0 0 704 170" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                <!-- <defs>
                    <path id="arrow-right" d="M 0 0 C -2.779 1 -5.376 2.445 -7.69 4.28 L -6.14 0 L -7.69 -4.28 C -5.376 -2.445 -2.779 -1 0 0 Z"></path>
                    <path id="arrow-down" d="M 0 0 C 1 2.779 2.445 5.376 4.28 7.69 L 0 6.14 L -4.28 7.69 C -2.444 5.376 -1 2.770 0 0 Z" transform="rotate(180, 0, 0)"></path>
                    <filter x="0" y="0" width="1" height="1" id="zoolander-text-background">
                            <feflood flood-color="#fdf6f2"></feflood>
                            <fecomposite in="SourceGraphic"></fecomposite>
                    </filter>
                </defs> -->
                <g transform="translate(30, 0)">
                    <text x="0" y="210" class="figure-text">
                        <tspan> Video and audio must be understood in the context of each </tspan>
                        <tspan x="0" dy="1.5em"> other to understand the scene. </tspan>
                        <tspan x="0" dy="2em" style="font-style: italic;"> Credit: still frame from the movie Charade. </tspan>
                    </text>                    
                    <image x="305" y="0" width="150" height="150" style="clip-path: inset(0 0 0 0 round 6px);" xlink:href="https://distill.pub/2018/feature-wise-transformations/images/puppy.jpg"></image>
                </g>
            </svg>
        </figure>      
        <p>
            Because this operation is cheap, we might as well avoid making any such
            assumptions and concatenate the conditioning representation to the input of
            <em>all</em> layers in the network. Let’s call this approach
            <em>concatenation-based conditioning</em>.
        </p>       
        <p>
            Interestingly, conditional biasing can be thought of as another way to
            implement concatenation-based conditioning. Consider a fully-connected
            linear layer applied to the concatenation of an input
            <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">x</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.44444em;"></span><span class="strut bottom" style="height:0.44444em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathbf">x</span></span></span></span></span></span> and a conditioning representation
            <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">z</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.44444em;"></span><span class="strut bottom" style="height:0.44444em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathbf">z</span></span></span></span></span></span>:
            <d-footnote>
            The same argument applies to convolutional networks, provided we ignore
            the border effects due to zero-padding.
            </d-footnote>
        </p>       
        <p>
            Given that both additive and multiplicative interactions seem natural and
            intuitive, which approach should we pick? One argument in favor of
            <em>multiplicative</em> interactions is that they are useful in learning
            relationships between inputs, as these interactions naturally identify
            “matches”: multiplying elements that agree in sign yields larger values than
            multiplying elements that disagree. This property is why dot products are
            often used to determine how similar two vectors are.
            <d-footnote>
            Multiplicative interactions alone have had a history of success in various
            domains — see <a href="#bibliographic-notes">Bibliographic Notes</a>.
            </d-footnote>
            One argument in favor of <em>additive</em> interactions is that they are
            more natural for applications that are less strongly dependent on the
            joint values of two inputs, like feature aggregation or feature detection
            (i.e., checking if a feature is present in either of two inputs).
        </p>

        <h3 id="nomenclature">Nomenclature</h3>
        <p>
        To continue the discussion on feature-wise transformations we need to
        abstract away the distinction between multiplicative and additive
        interactions. Without losing generality, let’s focus on feature-wise affine
        transformations, and let’s adopt the nomenclature of Perez et al.
        <d-cite key="perez2018film"></d-cite>, which formalizes conditional affine
        transformations under the acronym <em>FiLM</em>, for Feature-wise Linear
        Modulation.
        <d-footnote>
            Strictly speaking, <em>linear</em> is a misnomer, as we allow biasing, but
            we hope the more rigorous-minded reader will forgive us for the sake of a
            better-sounding acronym.
        </d-footnote>
        </p>      
        
        <p>
        As the name implies, a FiLM layer applies a feature-wise affine
        transformation to its input. By <em>feature-wise</em>, we mean that scaling
        and shifting are applied element-wise, or in the case of convolutional
        networks, feature map -wise.
        <d-footnote>
        To expand a little more on the convolutional case, feature maps can be
        thought of as the same feature detector being evaluated at different
        spatial locations, in which case it makes sense to apply the same affine
        transformation to all spatial locations.
        </d-footnote>
        In other words, assuming <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">x</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.44444em;"></span><span class="strut bottom" style="height:0.44444em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathbf">x</span></span></span></span></span></span> is a FiLM layer’s
        input, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">z</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.44444em;"></span><span class="strut bottom" style="height:0.44444em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathbf">z</span></span></span></span></span></span> is a conditioning input, and
        <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.05556em;">γ</span></span></span></span></span> and <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.05278em;">β</span></span></span></span></span> are
        <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">z</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.44444em;"></span><span class="strut bottom" style="height:0.44444em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathbf">z</span></span></span></span></span></span>-dependent scaling and shifting vectors,

        <span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>FiLM</mtext><mo>(</mo><mrow><mi mathvariant="bold">x</mi></mrow><mo>)</mo><mo>=</mo><mi>γ</mi><mo>(</mo><mrow><mi mathvariant="bold">z</mi></mrow><mo>)</mo><mo>⊙</mo><mrow><mi mathvariant="bold">x</mi></mrow><mo>+</mo><mi>β</mi><mo>(</mo><mrow><mi mathvariant="bold">z</mi></mrow><mo>)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">
        \textrm{FiLM}(\mathbf{x}) = \gamma(\mathbf{z}) \odot \mathbf{x}
                                                    + \beta(\mathbf{z}).
        </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord text"><span class="mord mathrm">FiLM</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.05556em;">γ</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">z</span></span><span class="mclose">)</span><span class="mbin">⊙</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mbin">+</span><span class="mord mathit" style="margin-right:0.05278em;">β</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">z</span></span><span class="mclose">)</span><span class="mord mathrm">.</span></span></span></span></span></span>

        You can interact with the following fully-connected and convolutional FiLM
        layers to get an intuition of the sort of modulation they allow:
        </p>      
        
        <hr>

        <h2 id="in-literature">Feature-wise transformations in the literature</h2>
        <p>
            Feature-wise transformations find their way into methods applied to many
            problem settings, but because of their simplicity, their effectiveness is
            seldom highlighted in lieu of other novel research contributions.  Below are
            a few notable examples of feature-wise transformations in the literature,
            grouped by application domain. The diversity of these applications
            underscores the flexible, general-purpose ability of feature-wise
            interactions to learn effective task representations.
        </p>
        <div style="width: 100%">
            <button class="expand-collapse-button" content-type="literature">expand all</button>
        </div>
        <button class="collapsible" content-name="vqa" content-type="literature">Visual question-answering<span style="float: right;">+</span></button>
        <p class="content" content-name="vqa" content-type="literature">
            Perez et al. <d-cite key="perez2017learning,perez2018film"></d-cite> use
            FiLM layers to build a visual reasoning model
            trained on the CLEVR dataset <d-cite key="johnson2017clevr"></d-cite> to
            answer multi-step, compositional questions about synthetic images.
        </p>        
        <button class="collapsible" content-name="domain-adaptation" content-type="literature">Domain adaptation and few-shot learning<span style="float: right;">+</span></button>
        <p class="content" content-name="domain-adaptation" content-type="literature">
            For domain adaptation, Li et al. <d-cite key="li2018adaptive"></d-cite>
            find it effective to update the per-channel batch normalization
            statistics (mean and variance) of a network trained on one domain with that
            network’s statistics in a new, target domain. As discussed in the
            <em>Style transfer</em> subsection, this operation is akin to using the network as
            both the FiLM generator and the FiLM-ed network. Notably, this approach,
            along with Adaptive Instance Normalization, has the particular advantage of
            not requiring any extra trainable parameters.
        </p>
        <p class="content" content-name="domain-adaptation" content-type="literature">
            For few-shot learning, Oreshkin et al.
            <d-cite key="oreshkin2018tadam"></d-cite> explore the use of FiLM layers to
            provide more robustness to variations in the input distribution across
            few-shot learning episodes. The training set for a given episode is used to
            produce FiLM parameters which modulate the feature extractor used in a
            Prototypical Networks <d-cite key="snell2017prototypical"></d-cite>
            meta-training procedure.
        </p>

        <hr>
        <h2 id="related-ideas">Related ideas</h2>
        <p>
            Aside from methods which make direct use of feature-wise transformations,
            the FiLM framework connects more broadly with the following methods and
            concepts.
        </p>

        <hr>
        <h2 id="properties-of-learned-representation">Properties of the learned task representation</h2>
        <p>
            As hinted earlier, in adopting the FiLM perspective we implicitly introduce
            a notion of <em>task representation</em>: each task — be it a question
            about an image or a painting style to imitate — elicits a different
            set of FiLM parameters via the FiLM generator which can be understood as its
            representation in terms of how to modulate the FiLM-ed network. To help
            better understand the properties of this representation, let’s focus on two
            FiLM-ed models used in fairly different problem settings:
        </p>
        <ul>
            <li>
            The visual reasoning model of Perez et al.
            <d-cite key="perez2017learning,perez2018film"></d-cite>, which uses FiLM
            to modulate a visual processing pipeline based off an input question.
            </li>

            <hr>
        </ul>

        <h2 id="discussion">Discussion</h2>
        <p>
            Looking forward, there are still many unanswered questions.
            Do these experimental observations on FiLM-based architectures generalize to
            other related conditioning mechanisms, such as conditional biasing, sigmoidal
            gating, HyperNetworks, and bilinear transformations? When do feature-wise
            transformations outperform methods with stronger inductive biases and vice
            versa? Recent work combines feature-wise transformations with stronger
            inductive bias methods
            <d-cite key="strub2018visual,bahdanau2018learning,yang2018dataset"></d-cite>,
            which could be an optimal middle ground. Also, to what extent are FiLM’s
            task representation properties
            inherent to FiLM, and to what extent do they emerge from other features
            of neural networks (i.e. non-linearities, FiLM generator
            depth, etc.)? If you are interested in exploring these or other
            questions about FiLM, we recommend looking into the code bases for
            FiLM models for <a href="https://github.com/ethanjperez/film">visual reasoning</a>
            <d-cite key="perez2018film"></d-cite> and <a href="https://github.com/tensorflow/magenta/tree/master/magenta/models/arbitrary_image_stylization">style transfer</a>
            <d-cite key="ghiasi2017exploring"></d-cite> which we used as a
            starting point for our experiments here.
        </p>
        <p>
            Finally, the fact that changes on the feature level alone are able to
            compound into large and meaningful modulations of the FiLM-ed network is
            still very surprising to us, and hopefully future work will uncover deeper
            explanations. For now, though, it is a question that
            evokes the even grander mystery of how neural networks in general compound
            simple operations like matrix multiplications and element-wise
            non-linearities into semantically meaningful transformations.
        </p>
    </d-article>


    <d-appendix>
        <h3 id="bibliographic-notes">Bibliographic Notes</h3>
        <p>
            Multiplicative interactions have succeeded on various tasks, ever since
            they were introduced in vision as “mapping units” <d-cite key="hinton1981a"></d-cite>
            and “dynamic mappings” <d-cite key="vonderMalsburg1994the"></d-cite>
            around 40 years ago.  These tasks include Character-level Language
            Modeling<d-cite key="sutskever2011generating"></d-cite>,
            Image Denoising<d-cite key="tang2012boltzmann"></d-cite>,
            Pose Estimation<d-cite key="taylor2009factored"></d-cite>,
            Tracking<d-cite key="ross2006combining,denil2012learning"></d-cite>,
            Action Recognition<d-cite key="le2011learning,taylor2010convolutional"></d-cite>,
            and, more generally, tasks involving relating or matching inputs, such as
            from different modalities or points in time
            <d-cite key="memisevic2013learning"></d-cite>.
        </p>
        <p>
            Many models lie on the spectrum between FiLM and Hypernetworks:
        </p>
        <ul>
            <li>
            Adaptive CNN <d-cite key="kang2017incorporating"></d-cite> predicts the
            value of several of the model’s convolution filters as a function of
            auxiliary inputs like camera perspective, level of noise, etc. The
            resulting convolution filters turn out to be very effective in difficult
            vision tasks such as crowd counting or image deblurring.
            </li>
            <li>
            Residual Adapters <d-cite key="rebuffi2017residualadapters"></d-cite> also
            propose to predict entire convolutional filters conditioned on the visual
            recognition domain they are operating in.
            </li>
            <li>
            In zero-shot/one-shot learning, Ba et al.
            <d-cite key="lei2015predicting"></d-cite> propose a model that predicts
            convolutional filters and classifiers weights based on textual
            descriptions of object classes.
            </li>
            <li>
            In reinforcement learning, Oh et al. <d-cite key="oh2017zero"></d-cite>
            propose a model that computes the parameters of a
            convolutional policy network conditioned on the task description.
            </li>
        </ul>
        <p>
            Tenenbaum and Freeman <d-cite key="tenenbaum1997separating"></d-cite> first
            introduced bilinear models in the vision community to better disentangle
            latent perceptual factors. The authors wanted to separate an image’s style
            from its content, arguing that classic linear models were not rich enough to
            extract such complex interaction. They demonstrate the effectiveness of
            their approach by applying it to spoken vowel identification or zero-shot
            font classification. Notable applications include:
        </p>
        <ul>
            <li>
                Chuang et al. <d-cite key="chuang2002facial"></d-cite> perform facial
                animation using bilinear transformations by separating key facial features
                (the style) from visual emotions (the content). Their method can modify
                a speaking subject’s expression in recorded sequence from happy to angry
                or neutral.
            </li>
            <li>
                Chu and Park <d-cite key="chu2009personalized"></d-cite> and Yang et al.
                <d-cite key="yang2011like"></d-cite> apply bilinear models to
                recommendation systems by extracting user and item information in various
                settings. More generally, recommendation systems rely heavily on matrix
                factorization methods <d-cite key="koren2009matrix"></d-cite>, which can
                be viewed as a bilinear model where one of the latent vectors is
                fixed<d-cite key="tenenbaum1997separating"></d-cite>.
            </li>
            <li>
                More recently, bilinear models have inspired new neural architectures in
                visual recognition <d-cite key="lin2015bilinear"></d-cite>, video action
                recognition <d-cite key="feichtenhofer2016convolutional"></d-cite>, and
                visual question-answering<d-cite key="fukui2016multimodal"></d-cite>.
            </li>
        </ul>
        <h3 id="acknowledgements">Acknowledgements</h3>
        <p>
            This article would be nowhere near where it is today without the honest and
            constructive feedback we received from various people across several
            organizations. We would like to thank Chris Olah and Shan Carter from the
            Distill editorial team as well as Ludwig Schubert from the Google Brain team for being
            so generous with their time and advice. We would also like to thank Archy de
            Berker, Xavier Snelgrove, Pedro Oliveira Pinheiro, Alexei Nordell-Markovits,
            Masha Krol, and Minh Dao from Element AI; Roland Memisevic from TwentyBN;
            Dzmitry Bahdanau from MILA; Ameesh Shah and Will Levine from Rice
            University; Dhanush Radhakrishnan from Roivant Sciences; Raymond Cano from
            Plaid; Eleni Triantafillou from Toronto University; Olivier Pietquin and
            Jon Shlens from Google Brain; and Jérémie Mary from Criteo.
        </p>

        <h3 id="peer-reviews">Discussion and Review</h3>
        <p>
            <a href="https://github.com/distillpub/post--feature-wise-transformations/issues/156">Review 1 - Anonymous </a><br>
            <a href="https://github.com/distillpub/post--feature-wise-transformations/issues/158">Review 2 - Anonymous </a><br>
            <a href="https://github.com/distillpub/post--feature-wise-transformations/issues/159">Review 3 - Chris Olah</a><br>
        </p>
      
        <d-footnote-list></d-footnote-list>


        <d-citation-list distill-prerendered="true">
            <style>
                d-citation-list {
                  contain: style;
                }
                
                d-citation-list .references {
                  grid-column: text;
                }
                
                d-citation-list .references .title {
                  font-weight: 500;
                }
            </style>
            <h3 id="references">References</h3>
            <ol id="references-list" class="references"><li id="perez2018film"><span class="title">FiLM: Visual Reasoning with a General Conditioning Layer</span>   <a href="http://arxiv.org/pdf/1709.07871.pdf">[PDF]</a><br>Perez, E., Strub, F., Vries, H.d., Dumoulin, V. and Courville, A., 2018. AAAI. </li><li id="perez2017learning"><span class="title">Learning visual reasoning without strong priors</span>   <a href="https://arxiv.org/pdf/1707.03017.pdf">[PDF]</a><br>Perez, E., de Vries, H., Strub, F., Dumoulin, V. and Courville, A., 2017. ICML Workshop on Machine Learning in Speech and Language Processing. </li><li id="johnson2017clevr"><span class="title">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</span>   <a href="https://arxiv.org/pdf/1612.06890.pdf">[PDF]</a><br>Johnson, J., Li, F., Hariharan, B., Zitnick, L.C., van der Maaten, L. and Girshick, R., 2017. Proceedings of the Conference on Computer Vision and Pattern Recognition. </li><li id="strub2018visual"><span class="title">Visual Reasoning with Multi-hop Feature Modulation</span> <br>Strub, F., Seurin, M., Perez, E., de Vries, H., Mary, J., Preux, P., Courville, A. and Pietquin, O., 2018. ECCV. </li><li id="vries2017guesswhat"><span class="title">GuessWhat?! Visual object discovery through multi-modal dialogue</span>   <a href="http://arxiv.org/pdf/1611.08481.pdf">[PDF]</a><br>de Vries, H., Strub, F., Chandar, S., Pietquin, O., Larochelle, H. and Courville, A., 2017. Proceedings of the Conference on Computer Vision and Pattern Recognition. </li><li id="kazemzadeh2014referitgame"><span class="title">ReferItGame: Referring to objects in photographs of natural scenes</span>   <a href="http://tamaraberg.com/papers/referit.pdf">[PDF]</a><br>Kazemzadeh, S., Ordonez, V., Matten, M. and Berg, T., 2014. Conference on Empirical Methods in Natural Language Processing. </li><li id="vries2017modulating"><span class="title">Modulating early visual processing by language</span>   <a href="https://arxiv.org/pdf/1707.00683.pdf">[PDF]</a><br>de Vries, H., Strub, F., Mary, J., Larochelle, H., Pietquin, O. and Courville, A., 2017. Advances in Neural Information Processing Systems. </li><li id="agrawal2015vqa"><span class="title">VQA: visual question answering</span>   

            <a href="https://arxiv.org/pdf/1505.00468.pdf">[PDF]</a><br>Agrawal, A., Lu, J., Antol, S., Mitchell, M., Zitnick, C.L., Batra, D. and Parikh, D., 2015. Proceedings of the International Conference on Computer Vision. </li><li id="dumoulin2017learned"><span class="title">A learned representation for artistic style</span>   <a href="https://arxiv.org/pdf/1610.07629.pdf">[PDF]</a><br>Dumoulin, V., Shlens, J. and Kudlur, M., 2017. Proceedings of the International Conference on Learning Representations. </li><li id="ghiasi2017exploring"><span class="title">Exploring the structure of a real-time, arbitrary neural artistic stylization network</span>   <a href="https://arxiv.org/pdf/1705.06830.pdf">[PDF]</a><br>Ghiasi, G., Lee, H., Kudlur, M., Dumoulin, V. and Shlens, J., 2017. Proceedings of the British Machine Vision Conference. </li><li id="yang2018efficient"><span class="title">Efficient video object segmentation via network modulation</span>   <a href="http://arxiv.org/pdf/1802.01218.pdf">[PDF]</a><br>Yang, L., Wang, Y., Xiong, X., Yang, J. and Katsaggelos, A.K., 2018. arXiv. </li><li id="huang2017arbitrary"><span class="title">Arbitrary style transfer in real-time with adaptive instance normalization</span>   <a href="https://arxiv.org/pdf/1703.06868.pdf">[PDF]</a><br>Huang, X. and Belongie, S., 2017. Proceedings of the International Conference on Computer Vision. </li><li id="srivastava2015highway"><span class="title">Highway networks</span>   <a href="http://arxiv.org/pdf/1505.00387.pdf">[PDF]</a><br>Srivastava, R.K., Greff, K. and Schmidhuber, J., 2015. ICML Deep Learning Workshop. </li><li id="hochreiter1997long"><span class="title">Long short-term memory</span>   


            <a href="http://dx.doi.org/10.1162/neco.1997.9.8.1735">[link]</a><br>Hochreiter, S. and Schmidhuber, J., 1997. Neural Computation, Vol 9(8), pp. 1735--1780. MIT Press.</li><li id="hu2017squeeze"><span class="title">Squeeze-and-Excitation networks</span>   <a href="https://arxiv.org/pdf/1709.01507.pdf">[PDF]</a><br>Hu, J., Shen, L. and Sun, G., 2017. CVPR's ILSVRC 2017 Workshop. </li><li id="melis2017lstm"><span class="title">On the state of the art of evaluation in neural language models</span>   <a href="http://arxiv.org/pdf/1707.05589.pdf">[PDF]</a><br>Melis, G., Dyer, C. and Blunsom, P., 2017. Proceedings of the International Conference on Learning Representations. </li><li id="dauphin2017language"><span class="title">Language modeling with gated convolutional networks</span>   <a href="https://arxiv.org/pdf/1612.08083.pdf">[PDF]</a><br>Dauphin, Y.N., Fan, A., Auli, M. and Grangier, D., 2017. Proceedings of the International Conference on Machine Learning. </li><li id="gehring2017convolutional"><span class="title">Convolution sequence-to-sequence learning</span>   <a href="https://arxiv.org/pdf/1705.03122.pdf">[PDF]</a><br>Gehring, J., Auli, M., Grangier, D., Yarats, D. and Dauphin, Y.N., 2017. Proceedings of the International Conference on Machine Learning. </li><li id="dhingra2017gated"><span class="title">Gated-attention readers for text comprehension</span>   <a href="https://arxiv.org/pdf/1606.01549.pdf">[PDF]</a><br>Dhingra, B., Liu, H., Yang, Z., Cohen, W.W. and Salakhutdinov, R., 2017. Proceedings of the Annual Meeting of the Association for Computational Linguistics. </li><li id="chaplot2017gated"><span class="title">Gated-attention architectures for task-oriented language grounding</span>   <a href="https://arxiv.org/pdf/1706.07230.pdf">[PDF]</a><br>Chaplot, D.S., Sathyendra, K.M., Pasumarthi, R.K., Rajagopal, D. and Salakhutdinov, R., 2017. ACL Workshop on Language Grounding for Robotics. </li><li id="kempa2016vizdoom"><span class="title">Vizdoom: A doom-based AI research platform for visual reinforcement learning</span>   <a href="https://arxiv.org/pdf/1605.02097.pdf">[PDF]</a><br>Kempka, M., Wydmuch, M., Runc, G., Toczek, J. and Jaśkowski, W., 2016. IEEE Conference on Computational Intelligence and Games. </li><li id="bahdanau2018learning"><span class="title">Learning to follow language instructions with adversarial reward induction</span>   

            <a href="http://arxiv.org/pdf/1806.01946.pdf">[PDF]</a><br>Bahdanau, D., Hill, F., Leike, J., Hughes, E., Kohli, P. and Grefenstette, E., 2018. arXiv. </li><li id="andreas2016neural"><span class="title">Neural module networks</span>   <a href="http://arxiv.org/pdf/1511.02799.pdf">[PDF]</a><br>Andreas, J., Rohrbach, M., Darrell, T. and Klein, D., 2016. Proceedings of the Conference on Computer Vision and Pattern Recognition. </li><li id="kirkpatrick2017overcoming"><span class="title">Overcoming catastrophic forgetting in neural networks</span>   <a href="http://www.pnas.org/content/114/13/3521.abstract">[link]</a><br>Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A.A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D. and Hadsell, R., 2017. Proceedings of the National Academy of Sciences, Vol 114(13), pp. 3521--3526. </li><li id="radford2016unsupervised"><span class="title">Unsupervised representation learning with deep convolutional generative adversarial networks</span>   <a href="https://arxiv.org/pdf/1511.06434.pdf">[PDF]</a><br>Radford, A., Metz, L. and Chintala, S., 2016. Proceedings of the International Conference on Learning Representations. </li><li id="goodfellow2014generative"><span class="title">Generative adversarial nets</span>   <a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">[PDF]</a><br>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y., 2014. Advances in Neural Information Processing Systems. </li><li id="oord2016conditional"><span class="title">Conditional image generation with PixelCNN decoders</span>   <a href="https://arxiv.org/pdf/1606.05328.pdf">[PDF]</a><br>van den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals, O., Graves, A. and Kavukcuoglu, K., 2016. Advances in Neural Information Processing Systems. </li><li id="oord2016wavenet"><span class="title">WaveNet: A generative model for raw audio</span>   <a href="https://arxiv.org/pdf/1609.03499.pdf">[PDF]</a><br>van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A. and Kavukcuoglu, K., 2016. arXiv. </li><li id="kim2017dynamic"><span class="title">Dynamic layer normalization for adaptive neural acoustic modeling in speech recognition</span>   <a href="https://arxiv.org/pdf/1707.06065.pdf">[PDF]</a><br>Kim, T., Song, I. and Bengio, Y., 2017. Interspeech. </li><li id="li2018adaptive"><span class="title">Adaptive batch normalization for practical domain adaptation</span>   <a href="http://www.sciencedirect.com/science/article/pii/S003132031830092X">[link]</a><br>Li, Y., Wang, N., Shi, J., Hou, X. and Liu, J., 2018. Pattern Recognition, Vol 80, pp. 109 - 117. </li><li id="oreshkin2018tadam"><span class="title">TADAM: Task dependent adaptive metric for improved few-shot learning</span>   <a href="http://arxiv.org/pdf/1805.10123.pdf">[PDF]</a><br>Oreshkin, B.N., Rodriguez, P. and Lacoste, A., 2018. arXiv. </li><li id="snell2017prototypical"><span class="title">Prototypical networks for few-shot learning</span>   <a href="http://arxiv.org/pdf/1703.05175.pdf">[PDF]</a><br>Snell, J., Swersky, K. and Zemel, R., 2017. Advances in Neural Information Processing Systems. </li><li id="frome2013devise"><span class="title">Devise: A deep visual-semantic embedding model</span>   <a href="https://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model">[link]</a><br>Frome, A., Corrado, G.S., Shlens, J., Bengio, S., Dean, J. and Mikolov, T., 2013. Advances in Neural Information Processing Systems, pp. 2121--2129. </li><li id="socher2013zero"><span class="title">Zero-shot learning through cross-modal transfer</span>   <a href="http://arxiv.org/pdf/1301.3666.pdf">[PDF]</a><br>Socher, R., Ganjoo, M., Manning, C.D. and Ng, A., 2013. Advances in Neural Information Processing Systems, pp. 935--943. </li><li id="norouzi2014zero"><span class="title">Zero-shot learning by convex combination of semantic embeddings</span>   <a href="http://arxiv.org/pdf/1312.5650.pdf">[PDF]</a><br>Norouzi, M., Mikolov, T., Bengio, S., Singer, Y., Shlens, J., Frome, A., Corrado, G.S. and Dean, J., 2014. Proceedings of the International Conference on Learning Representations. </li><li id="ha2016hypernetworks"><span class="title">HyperNetworks</span>   <a href="https://arxiv.org/pdf/1609.09106.pdf">[PDF]</a><br>Ha, D., Dai, A. and Le, Q., 2016. Proceedings of the International Conference on Learning Representations. </li><li id="tenenbaum2000separating"><span class="title">Separating style and content with bilinear models</span>   <a href="http://dx.doi.org/10.1162/089976600300015349">[link]</a><br>Tenenbaum, J.B. and Freeman, W.T., 2000. Neural Computation, Vol 12(6), pp. 1247--1283. MIT Press.</li><li id="maaten2008visualizing"><span class="title">Visualizing data using t-SNE</span>   <a href="http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">[PDF]</a><br>Maaten, L.v.d. and Hinton, G., 2008. Journal of machine learning research, Vol 9(Nov), pp. 2579--2605. </li><li id="yang2018dataset"><span class="title">A dataset and architecture for visual reasoning with a working memory</span>   

            <a href="http://arxiv.org/pdf/1803.06092.pdf">[PDF]</a><br>Yang, G.R., Ganichev, I., Wang, X., Shlens, J. and Sussillo, D., 2018. arXiv. </li><li id="hinton1981a"><span class="title">A parallel computation that assigns canonical object-based frames of reference</span>   <a href="http://dl.acm.org/citation.cfm?id=1623264.1623282">[link]</a><br>Hinton, G.F., 1981. Proceedings of the International Joint Conference on Artificial Intelligence. </li><li id="vonderMalsburg1994the"><span class="title">The correlation theory of brain function</span>   <a href="https://doi.org/10.1007/978-1-4612-4320-5_2">[link]</a><br>von der Malsburg, C., 1994. Models of Neural Networks: Temporal Aspects of Coding and Information Processing in Biological Systems, pp. 95--119. </li><li id="sutskever2011generating"><span class="title">Generating text with recurrent neural networks</span>   <a href="http://dl.acm.org/citation.cfm?id=3104482.3104610">[link]</a><br>Sutskever, I., Martens, J. and Hinton, G., 2011. Proceedings of the International Conference on Machine Learning. </li><li id="tang2012boltzmann"><span class="title">Robust boltzmann machines for recognition and denoising</span>   

            <a href="http://www.cs.toronto.edu/~tang/papers/robm.pdf">[PDF]</a><br>Tang, Y., Salakhutdinov, R. and Hinton, G., 2012. IEEE Conference on Computer Vision and Pattern Recognition. </li><li id="taylor2009factored"><span class="title">Factored conditional restricted Boltzmann machines for modeling motion style</span>   <a href="http://doi.acm.org/10.1145/1553374.1553505">[link]</a><br>Taylor, G.W. and Hinton, G.E., 2009. Proceedings of the International Conference on Machine Learning. </li><li id="ross2006combining"><span class="title">Combining discriminative features to infer complex trajectories</span>   <a href="http://www.cs.toronto.edu/~osindero/PUBLICATIONS/RossOsinderoZemel_ICML06.pdf">[PDF]</a><br>Ross, D.A., Osindero, S. and Zemel, R.S., 2006. Proceedings of the International Conference on Machine Learning. </li><li id="denil2012learning"><span class="title">Learning where to attend with deep architectures for image tracking</span>   <a href="http://dx.doi.org/10.1162/NECO_a_00312">[link]</a><br>Denil, M., Bazzani, L., Larochelle, H. and de Freitas, N., 2012. Neural Comput., Vol 24(8), pp. 2151--2184. </li><li id="le2011learning"><span class="title">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</span>   <a href="https://ieeexplore.ieee.org/document/5995496/">[link]</a><br>Le, Q.V., Zou, W.Y., Yeung, S.Y. and Ng, A.Y., 2011. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. </li><li id="taylor2010convolutional"><span class="title">Convolutional learning of spatio-temporal features</span>   <a href="https://link.springer.com/chapter/10.1007/978-3-642-15567-3_11">[link]</a><br>Taylor, G.W., Fergus, R., LeCun, Y. and Bregler, C., 2010. ECCV. </li><li id="memisevic2013learning"><span class="title">Learning to relate images</span>   <a href="https://www.iro.umontreal.ca/~memisevr/pubs/pami_relational.pdf">[PDF]</a><br>Memisevic, R., 2013. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol 35(8), pp. 1829-1846. </li><li id="kang2017incorporating"><span class="title">Incorporating side information by adaptive convolution</span>   <a href="https://papers.nips.cc/paper/6976-incorporating-side-information-by-adaptive-convolution.pdf">[PDF]</a><br>Kang, D., Dhar, D. and Chan, A., 2017. Advances in Neural Information Processing Systems. </li><li id="rebuffi2017residualadapters"><span class="title">Learning multiple visual domains with residual adapters</span>   <a href="http://papers.nips.cc/paper/6654-learning-multiple-visual-domains-with-residual-adapters.pdf">[PDF]</a><br>Rebuffi, S., Bilen, H. and Vedaldi, A., 2017. Advances in Neural Information Processing Systems. </li><li id="lei2015predicting"><span class="title">Predicting deep zero-shot convolutional neural networks using textual descriptions</span>   <a href="https://arxiv.org/pdf/1506.00511.pdf">[PDF]</a><br>Ba, J., Swersky, K., Fidler, S. and Salakhutdinov, R., 2015. Proceedings of the IEEE International Conference on Computer Vision. </li><li id="oh2017zero"><span class="title">Zero-shot task generalization with multi-task deep reinforcement learning</span>   <a href="https://arxiv.org/pdf/1706.05064.pdf">[PDF]</a><br>Oh, J., Singh, S., Lee, H. and Kohli, P., 2017. Proceedings of the International Conference on Machine Learning. </li><li id="tenenbaum1997separating"><span class="title">Separating style and content</span>   <a href="https://papers.nips.cc/paper/1290-separating-style-and-content.pdf">[PDF]</a><br>Tenenbaum, J.B. and Freeman, W.T., 1997. Advances in neural information processing systems. </li><li id="chuang2002facial"><span class="title">Facial expression space learning</span>   <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.3338&amp;rep=rep1&amp;type=pdf">[link]</a><br>Chuang, E.S., Deshpande, F. and Bregler, C., 2002. Proceedings of the Pacific Conference on Computer Graphics and Applications. </li><li id="chu2009personalized"><span class="title">Personalized recommendation on dynamic content using predictive bilinear models</span>   <a href="http://www.wwwconference.org/www2009/proceedings/pdf/p691.pdf">[PDF]</a><br>Chu, W. and Park, S., 2009. Proceedings of the International Conference on World Wide Web. </li><li id="yang2011like"><span class="title">Like like alike: joint friendship and interest propagation in social networks</span>   <a href="http://wwwconference.org/proceedings/www2011/proceedings/p537.pdf">[PDF]</a><br>Yang, S., Long, B., Smola, A., Sadagopan, N., Zheng, Z. and Zha, H., 2011. Proceedings of the International Conference on World Wide Web. </li><li id="koren2009matrix"><span class="title">Matrix factorization techniques for recommender systems</span>   <a href="https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf">[PDF]</a><br>Koren, Y., Bell, R. and Volinsky, C., 2009. Computer, Vol 42(8). IEEE.</li><li id="lin2015bilinear"><span class="title">Bilinear CNN models for fine-grained visual recognition</span>   <a href="http://vis-www.cs.umass.edu/bcnn/docs/bcnn_iccv15.pdf">[PDF]</a><br>Lin, T., RoyChowdhury, A. and Maji, S., 2015. Proceedings of the IEEE International Conference on Computer Vision. </li><li id="feichtenhofer2016convolutional"><span class="title">Convolutional two-stream network fusion for video action recognition</span>   <a href="https://arxiv.org/pdf/1604.06573.pdf">[PDF]</a><br>Feichtenhofer, C., Pinz, A. and Zisserman, A., 2016. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. </li><li id="fukui2016multimodal"><span class="title">Multimodal compact bilinear pooling for visual question answering and visual grounding</span>   <a href="https://arxiv.org/pdf/1606.01847.pdf">[PDF]</a><br>Fukui, A., Park, D.H., Yang, D., Rohrbach, A., Darrell, T. and Rohrbach, M., 2016. Proceedings of the Conference on Empirical Methods in Natural Language Processing. </li></ol>
        </d-citation-list>

        <distill-appendix>
            <style>
            distill-appendix {
                contain: layout style;
            }

            distill-appendix .citation {
                font-size: 11px;
                line-height: 15px;
                border-left: 1px solid rgba(0, 0, 0, 0.1);
                padding-left: 18px;
                border: 1px solid rgba(0,0,0,0.1);
                background: rgba(0, 0, 0, 0.02);
                padding: 10px 18px;
                border-radius: 3px;
                color: rgba(150, 150, 150, 1);
                overflow: hidden;
                margin-top: -12px;
                white-space: pre-wrap;
                word-wrap: break-word;
            }

            distill-appendix > * {
                grid-column: text;
            }
            </style>
                    

            <h3 id="updates-and-corrections">Updates and Corrections</h3>
            <p>
                If you see mistakes or want to suggest changes, please <a href="https://github.com/distillpub/post--feature-wise-transformations/issues/new">create an issue on GitHub</a>. </p>

            <h3 id="reuse">Reuse</h3>
            <p>Diagrams and text are licensed under Creative Commons Attribution <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> with the <a class="github" href="https://github.com/distillpub/post--feature-wise-transformations">source available on GitHub</a>, unless noted otherwise. The figures that have been reused from other sources don’t fall under this license and can be recognized by a note in their caption: “Figure from …”.</p>

            <h3 id="citation">Citation</h3>
            <p>For attribution in academic contexts, please cite this work as</p>
            <pre class="citation short">Dumoulin, et al., "Feature-wise transformations", Distill, 2018.</pre>
            <p>BibTeX citation</p>
            <pre class="citation long">@article{dumoulin2018feature-wise,
                author = {Dumoulin, Vincent and Perez, Ethan and Schucher, Nathan and Strub, Florian and Vries, Harm de and Courville, Aaron and Bengio, Yoshua},
                title = {Feature-wise transformations},
                journal = {Distill},
                year = {2018},
                note = {https://distill.pub/2018/feature-wise-transformations},
                doi = {10.23915/distill.00011}
                }</pre>
                
        </distill-appendix>     
    </d-appendix>

    <d-bibliography>
        <script type="text/json">
            [["vries2017modulating",{"title":"Modulating early visual processing by language","author":"de Vries, Harm and Strub, Florian and Mary, J\\'{e}r\\'{e}mie and Larochelle, Hugo and Pietquin, Olivier and Courville, Aaron","booktitle":"Advances in Neural Information Processing Systems","year":"2017","url":"https://arxiv.org/pdf/1707.00683.pdf","type":"inproceedings"}],["vries2017guesswhat",{"title":"GuessWhat?! Visual object discovery through multi-modal dialogue","author":"de Vries, Harm and Strub, Florian and Chandar, Sarath and Pietquin, Olivier and Larochelle, Hugo and Courville, Aaron","booktitle":"Proceedings of the Conference on Computer Vision and Pattern Recognition","year":"2017","url":"https://arxiv.org/abs/1611.08481","type":"inproceedings"}],["perez2017learning",{"title":"Learning visual reasoning without strong priors","author":"Perez, Ethan and de Vries, Harm and Strub, Florian and Dumoulin, Vincent and Courville, Aaron","booktitle":"ICML Workshop on Machine Learning in Speech and Language Processing","year":"2017","url":"https://arxiv.org/pdf/1707.03017.pdf","type":"inproceedings"}],["perez2018film",{"title":"FiLM: Visual Reasoning with a General Conditioning Layer","author":"Ethan Perez and Florian Strub and Harm de Vries and Vincent Dumoulin and Aaron Courville","booktitle":"AAAI","year":"2018","url":"https://arxiv.org/abs/1709.07871","type":"inproceedings"}],["johnson2017clevr",{"title":"CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning","author":"Johnson, Justin and Li, Fei-Fei and Hariharan, Bharath and Zitnick, Lawrence C. and van der Maaten, Laurens and Girshick, Ross","booktitle":"Proceedings of the Conference on Computer Vision and Pattern Recognition","year":"2017","url":"https://arxiv.org/pdf/1612.06890.pdf","type":"inproceedings"}],["dumoulin2017learned",{"title":"A learned representation for artistic style","author":"Dumoulin, Vincent and Shlens, Jonathon and Kudlur, Manjunath","booktitle":"Proceedings of the International Conference on Learning Representations","year":"2017","url":"https://arxiv.org/pdf/1610.07629.pdf","type":"inproceedings"}],["ghiasi2017exploring",{"title":"Exploring the structure of a real-time, arbitrary neural artistic stylization network","author":"Ghiasi, Golnaz and Lee, Honglak and Kudlur, Manjunath and Dumoulin, Vincent and Shlens, Jonathon","booktitle":"Proceedings of the British Machine Vision Conference","year":"2017","url":"https://arxiv.org/pdf/1705.06830.pdf","type":"inproceedings"}],["huang2017arbitrary",{"title":"Arbitrary style transfer in real-time with adaptive instance normalization","author":"Huang, Xun and Belongie, Serge","booktitle":"Proceedings of the International Conference on Computer Vision","year":"2017","url":"https://arxiv.org/pdf/1703.06868.pdf","type":"inproceedings"}],["radford2016unsupervised",{"title":"Unsupervised representation learning with deep convolutional generative adversarial networks","author":"Radford, Alec and Metz, Luke and Chintala, Soumith","booktitle":"Proceedings of the International Conference on Learning Representations","year":"2016","url":"https://arxiv.org/pdf/1511.06434.pdf","type":"inproceedings"}],["oord2016conditional",{"title":"Conditional image generation with PixelCNN decoders","author":"van den Oord, Aaron and Kalchbrenner, Nal and Espeholt, Lasse and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray","booktitle":"Advances in Neural Information Processing Systems","year":"2016","url":"https://arxiv.org/pdf/1606.05328.pdf","type":"inproceedings"}],["oord2016wavenet",{"title":"WaveNet: A generative model for raw audio","author":"van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray","journal":"arXiv","year":"2016","url":"https://arxiv.org/pdf/1609.03499.pdf","type":"article"}],["ha2016hypernetworks",{"title":"HyperNetworks","author":"Ha, David and Dai, Andrew and Le, Quoc","booktitle":"Proceedings of the International Conference on Learning Representations","year":"2016","url":"https://arxiv.org/pdf/1609.09106.pdf","type":"inproceedings"}],["kim2017dynamic",{"title":"Dynamic layer normalization for adaptive neural acoustic modeling in speech recognition","author":"Kim, Taesup and Song, Inchul and Bengio, Yoshua","booktitle":"Interspeech","year":"2017","url":"https://arxiv.org/pdf/1707.06065.pdf","type":"inproceedings"}],["hu2017squeeze",{"title":"Squeeze-and-Excitation networks","author":"Hu, Jie and Shen, Li and Sun, Gang","booktitle":"CVPR's ILSVRC 2017 Workshop","year":"2017","url":"https://arxiv.org/pdf/1709.01507.pdf","type":"inproceedings"}],["dauphin2017language",{"title":"Language modeling with gated convolutional networks","author":"Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David","booktitle":"Proceedings of the International Conference on Machine Learning","year":"2017","url":"https://arxiv.org/pdf/1612.08083.pdf","type":"inproceedings"}],["gehring2017convolutional",{"title":"Convolution sequence-to-sequence learning","author":"Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.","booktitle":"Proceedings of the International Conference on Machine Learning","year":"2017","url":"https://arxiv.org/pdf/1705.03122.pdf","type":"inproceedings"}],["hochreiter1997long",{"title":"Long short-term memory","author":"Hochreiter, Sepp and Schmidhuber, Jurgen","journal":"Neural Computation","volume":"9","number":"8","pages":"1735--1780","year":"1997","publisher":"MIT Press","url":"http://dx.doi.org/10.1162/neco.1997.9.8.1735","type":"article"}],["tenenbaum2000separating",{"title":"Separating style and content with bilinear models","author":"Tenenbaum, Joshua B. and Freeman, William T.","journal":"Neural Computation","volume":"12","number":"6","pages":"1247--1283","year":"2000","publisher":"MIT Press","url":"http://dx.doi.org/10.1162/089976600300015349","type":"article"}],["dhingra2017gated",{"title":"Gated-attention readers for text comprehension","author":"Dhingra, Bhuwan and Liu, Hanxiao and Yang, Zhilin, and Cohen, William W and Salakhutdinov, Ruslan","booktitle":"Proceedings of the Annual Meeting of the Association for Computational Linguistics","year":"2017","url":"https://arxiv.org/pdf/1606.01549.pdf","type":"article"}],["chaplot2017gated",{"title":"Gated-attention architectures for task-oriented language grounding","author":"Chaplot, Devendra Singh and Sathyendra, Kanthashree Mysore and Pasumarthi, Rama Kumar and Rajagopal, Dheeraj and Salakhutdinov, Ruslan","booktitle":"ACL Workshop on Language Grounding for Robotics","year":"2017","url":"https://arxiv.org/pdf/1706.07230.pdf","type":"inproceedings"}],["shen2017disan",{"title":"DiSAN: Directional self-attention network for RNN/CNN-free language understanding","author":"Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Pan, Shirui and Zhang, Chengqi","booktitle":"arXiv","year":"2017","url":"https://arxiv.org/pdf/1709.04696.pdf","type":"inproceedings"}],["bowman2015large",{"title":"A large annotated corpus for learning natural language inference","author":"Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher, and Manning, Christopher D.","booktitle":"Proceedings of the Conference on Empirical Methods in Natural Language Processing","year":"2015","url":"https://nlp.stanford.edu/pubs/snli_paper.pdf","type":"inproceedings"}],["jacobs1991adaptive",{"title":"Adaptive mixtures of local experts","author":"Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.","journal":"Neural Computation","volume":"3","number":"1","pages":"79--87","year":"1991","publisher":"MIT Press","url":"http://dx.doi.org/10.1162/neco.1991.3.1.79","type":"article"}],["jordan1994hierarchical",{"title":"Hierarchical mixtures of experts and the EM algorithm","author":"Jordan, Michael I. and Jacobs, Robert A.","journal":"Neural Computation","volume":"6","number":"2","pages":"181--214","year":"1994","publisher":"MIT Press","url":"http://dx.doi.org/10.1162/neco.1994.6.2.181","type":"article"}],["eigen2014deep",{"title":"Learning factored representations in a deep mixture of experts","author":"Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya","booktitle":"Proceedings of the ICLR Workshops","year":"2014","url":"https://arxiv.org/pdf/1312.4314.pdf","type":"inproceedings"}],["kirkpatrick2017overcoming",{"title":"Overcoming catastrophic forgetting in neural networks","author":"Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia","journal":"Proceedings of the National Academy of Sciences","volume":"114","number":"13","pages":"3521--3526","year":"2017","url":"http://www.pnas.org/content/114/13/3521.abstract","type":"article"}],["mel1992clusteron",{"title":"The Clusteron: Toward a simple abstraction for a complex neuron","author":"Mel, Bartlett W","booktitle":"Advances in Neural Information Processing Systems","year":"1992","url":"http://papers.nips.cc/paper/450-the-clusteron-toward-a-simple-abstraction-for-a-complex-neuron.pdf","type":"inproceedings"}],["boutonnet2015words",{"title":"Words jump-start vision: A label advantage in object recognition","author":"B. Boutonnet and G. Lupyan","journal":"Journal of Neuroscience","volume":"35","number":"25","pages":"9329--9335","year":"2015","publisher":"Society for Neuroscience","url":"http://www.jneurosci.org/content/35/25/9329","type":"article"}],["agrawal2015vqa",{"title":"VQA: visual question answering","author":"Agrawal, Aishwarya and Lu, Jiasen and Antol, Stanislaw and Mitchell, Margaret and Zitnick, C. Lawrence and Batra, Dhruv and Parikh, Devi","booktitle":"Proceedings of the International Conference on Computer Vision","year":"2015","url":"https://arxiv.org/pdf/1505.00468.pdf","type":"inproceedings"}],["kempa2016vizdoom",{"title":"Vizdoom: A doom-based AI research platform for visual reinforcement learning","author":"Kempka, Michał and Wydmuch, Marek and Runc, Grzegorz and Toczek, Jakub and Jaśkowski, Wojciech","booktitle":"IEEE Conference on Computational Intelligence and Games","year":"2016","url":"https://arxiv.org/pdf/1605.02097.pdf","type":"inproceedings"}],["shazeer2017outrageously",{"title":"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer","author":"Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff","booktitle":"Proceedings of the International Conference on Learning Representations","year":"2017","url":"https://arxiv.org/pdf/1701.06538.pdf","type":"inproceedings"}],["bahdanau2014neural",{"title":"Neural machine translation by jointly learning to align and translate","author":"Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua","booktitle":"Proceedings of the International Conference on Learning Representations","year":"2015","url":"https://arxiv.org/pdf/1409.0473.pdf","type":"inproceedings"}],["yang2016stacked",{"title":"Stacked attention networks for image question answering","author":"Yang, Zichao and He, Xiaodong and Gao, Jianfeng and Deng, Li and Smola, Alex","booktitle":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","year":"2016","url":"https://arxiv.org/abs/1511.02274","type":"inproceedings"}],["kang2017incorporating",{"title":"Incorporating side information by adaptive convolution","author":"Kang, Di and Dhar, Debarun and Chan, Antoni","booktitle":"Advances in Neural Information Processing Systems","year":"2017","url":"https://papers.nips.cc/paper/6976-incorporating-side-information-by-adaptive-convolution.pdf","type":"inproceedings"}],["lei2015predicting",{"title":"Predicting deep zero-shot convolutional neural networks using textual descriptions","author":"Ba, Jimmy and Swersky, Kevin and Fidler, Sanja and Salakhutdinov, Ruslan","booktitle":"Proceedings of the IEEE International Conference on Computer Vision","year":"2015","url":"https://arxiv.org/pdf/1506.00511.pdf","type":"inproceedings"}],["oh2017zero",{"title":"Zero-shot task generalization with multi-task deep reinforcement learning","author":"Oh, Junhyuk and Singh, Satinder and Lee, Honglak and Kohli, Pushmeet","booktitle":"Proceedings of the International Conference on Machine Learning","year":"2017","url":"https://arxiv.org/pdf/1706.05064.pdf","type":"inproceedings"}],["goodfellow2014generative",{"title":"Generative adversarial nets","author":"Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua","booktitle":"Advances in Neural Information Processing Systems","year":"2014","url":"https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf","type":"inproceedings"}],["rebuffi2017residualadapters",{"title":"Learning multiple visual domains with residual adapters","author":"Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea","booktitle":"Advances in Neural Information Processing Systems","year":"2017","url":"http://papers.nips.cc/paper/6654-learning-multiple-visual-domains-with-residual-adapters.pdf","type":"incollection"}],["fukui2016compactbilinear",{"title":"Multimodal compact bilinear pooling for visual question answering and visual grounding","author":"Akira Fukui and Dong Huk Park and Daylen Yang and Anna Rohrbach and Trevor Darrell and Rohrbach, Marcus","booktitle":"Conference on Empirical Methods in Natural Language Processing","year":"2016","url":"https://arxiv.org/pdf/1606.01847.pdf","type":"inproceedings"}],["tenenbaum1997separating",{"title":"Separating style and content","author":"Tenenbaum, Joshua B and Freeman, William T","booktitle":"Advances in neural information processing systems","year":"1997","url":"https://papers.nips.cc/paper/1290-separating-style-and-content.pdf","type":"inproceedings"}],["chuang2002facial",{"title":"Facial expression space learning","author":"E. S. Chuang and F. Deshpande and C. Bregler","booktitle":"Proceedings of the Pacific Conference on Computer Graphics and Applications","year":"2002","url":"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.3338&rep=rep1&type=pdf","type":"inproceedings"}],["yang2011like",{"title":"Like like alike: joint friendship and interest propagation in social networks","author":"Yang, Shuang-Hong and Long, Bo and Smola, Alex and Sadagopan, Narayanan and Zheng, Zhaohui and Zha, Hongyuan","booktitle":"Proceedings of the International Conference on World Wide Web","year":"2011","url":"http://wwwconference.org/proceedings/www2011/proceedings/p537.pdf","type":"inproceedings"}],["chu2009personalized",{"title":"Personalized recommendation on dynamic content using predictive bilinear models","author":"Chu, Wei and Park, Seung-Taek","booktitle":"Proceedings of the International Conference on World Wide Web","year":"2009","url":"http://www.wwwconference.org/www2009/proceedings/pdf/p691.pdf","type":"inproceedings"}],["lin2015bilinear",{"title":"Bilinear CNN models for fine-grained visual recognition","author":"Lin, Tsung-Yu and RoyChowdhury, Aruni and Maji, Subhransu","booktitle":"Proceedings of the IEEE International Conference on Computer Vision","year":"2015","url":"http://vis-www.cs.umass.edu/bcnn/docs/bcnn_iccv15.pdf","type":"inproceedings"}],["feichtenhofer2016convolutional",{"title":"Convolutional two-stream network fusion for video action recognition","author":"Feichtenhofer, Christoph and Pinz, Axel and Zisserman, AP","booktitle":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","year":"2016","url":"https://arxiv.org/pdf/1604.06573.pdf","type":"inproceedings"}],["fukui2016multimodal",{"title":"Multimodal compact bilinear pooling for visual question answering and visual grounding","author":"Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus","booktitle":"Proceedings of the Conference on Empirical Methods in Natural Language Processing","year":"2016","url":"https://arxiv.org/pdf/1606.01847.pdf","type":"inproceedings"}],["koren2009matrix",{"title":"Matrix factorization techniques for recommender systems","author":"Koren, Yehuda and Bell, Robert and Volinsky, Chris","journal":"Computer","volume":"42","number":"8","year":"2009","publisher":"IEEE","url":"https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf","type":"article"}],["srivastava2015highway",{"title":"Highway networks","author":"Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, Jurgen","booktitle":"ICML Deep Learning Workshop","year":"2015","url":"https://arxiv.org/abs/1505.00387","type":"inproceedings"}],["memisevic2013learning",{"title":"Learning to relate images","author":"R. Memisevic","journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence","volume":"35","number":"8","pages":"1829-1846","year":"2013","url":"https://www.iro.umontreal.ca/~memisevr/pubs/pami_relational.pdf","type":"article"}],["hinton1981a",{"author":"Hinton, Geoffrey F.","title":"A parallel computation that assigns canonical object-based frames of reference","booktitle":"Proceedings of the International Joint Conference on Artificial Intelligence","year":"1981","url":"http://dl.acm.org/citation.cfm?id=1623264.1623282","type":"inproceedings"}],["vonderMalsburg1994the",{"title":"The correlation theory of brain function","author":"von der Malsburg, Christoph","booktitle":"Models of Neural Networks: Temporal Aspects of Coding and Information Processing in Biological Systems","year":"1994","pages":"95--119","url":"https://doi.org/10.1007/978-1-4612-4320-5_2","type":"inbook"}],["le2011learning",{"title":"Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis","author":"Le, Q. V. and Zou, W. Y. and Yeung, S. Y. and Ng, A. Y.","booktitle":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","year":"2011","url":"https://ieeexplore.ieee.org/document/5995496/","type":"inproceedings"}],["taylor2010convolutional",{"title":"Convolutional learning of spatio-temporal features","author":"Taylor, Graham W. and Fergus, Rob and LeCun, Yann and Bregler, Christoph","booktitle":"ECCV","year":"2010","url":"https://link.springer.com/chapter/10.1007/978-3-642-15567-3_11","type":"inproceedings"}],["ross2006combining",{"title":"Combining discriminative features to infer complex trajectories","author":"Ross, David A. and Osindero, Simon and Zemel, Richard S.","booktitle":"Proceedings of the International Conference on Machine Learning","year":"2006","url":"http://www.cs.toronto.edu/~osindero/PUBLICATIONS/RossOsinderoZemel_ICML06.pdf","type":"inproceedings"}],["denil2012learning",{"title":"Learning where to attend with deep architectures for image tracking","author":"Denil, Misha and Bazzani, Loris and Larochelle, Hugo and de Freitas, Nando","journal":"Neural Comput.","volume":"24","number":"8","year":"2012","pages":"2151--2184","url":"http://dx.doi.org/10.1162/NECO_a_00312","type":"article"}],["taylor2009factored",{"title":"Factored conditional restricted Boltzmann machines for modeling motion style","author":"Taylor, Graham W. and Hinton, Geoffrey E.","booktitle":"Proceedings of the International Conference on Machine Learning","year":"2009","url":"http://doi.acm.org/10.1145/1553374.1553505","type":"inproceedings"}],["tang2012boltzmann",{"title":"Robust boltzmann machines for recognition and denoising","author":"Y. Tang and R. Salakhutdinov and G. Hinton","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition","year":"2012","url":"http://www.cs.toronto.edu/~tang/papers/robm.pdf","type":"inproceedings"}],["sutskever2011generating",{"title":"Generating text with recurrent neural networks","author":"Sutskever, Ilya and Martens, James and Hinton, Geoffrey","booktitle":"Proceedings of the International Conference on Machine Learning","year":"2011","url":"http://dl.acm.org/citation.cfm?id=3104482.3104610","type":"inproceedings"}],["li2018adaptive",{"title":"Adaptive batch normalization for practical domain adaptation","author":"Yanghao Li and Naiyan Wang and Jianping Shi and Xiaodi Hou and Jiaying Liu","journal":"Pattern Recognition","volume":"80","pages":"109 - 117","year":"2018","url":"http://www.sciencedirect.com/science/article/pii/S003132031830092X","type":"article"}],["maaten2008visualizing",{"title":"Visualizing data using t-SNE","author":"Maaten, Laurens van der and Hinton, Geoffrey","journal":"Journal of machine learning research","volume":"9","number":"Nov","pages":"2579--2605","year":"2008","url":"http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf","type":"article"}],["yang2018efficient",{"title":"Efficient video object segmentation via network modulation","author":"Linjie Yang and Yanran Wang and Xuehan Xiong and Jianchao Yang and Aggelos K. Katsaggelos","journal":"arXiv","year":"2018","url":"https://arxiv.org/abs/1802.01218","type":"article"}],["frome2013devise",{"title":"Devise: A deep visual-semantic embedding model","author":"Frome, Andrea and Corrado, Greg S and Shlens, Jon and Bengio, Samy and Dean, Jeff and Mikolov, Tomas","booktitle":"Advances in Neural Information Processing Systems","pages":"2121--2129","year":"2013","url":"https://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model","type":"inproceedings"}],["socher2013zero",{"title":"Zero-shot learning through cross-modal transfer","author":"Socher, Richard and Ganjoo, Milind and Manning, Christopher D and Ng, Andrew","booktitle":"Advances in Neural Information Processing Systems","pages":"935--943","year":"2013","url":"https://arxiv.org/abs/1301.3666","type":"inproceedings"}],["norouzi2014zero",{"title":"Zero-shot learning by convex combination of semantic embeddings","author":"Norouzi, Mohammad and Mikolov, Tomas and Bengio, Samy and Singer, Yoram and Shlens, Jonathon and Frome, Andrea and Corrado, Greg S and Dean, Jeffrey","booktitle":"Proceedings of the International Conference on Learning Representations","year":"2014","url":"https://arxiv.org/abs/1312.5650","type":"inproceedings"}],["yu2018guided",{"title":"Guided feature transformation (GFT): A neural language grounding module for embodied agents","author":"Haonan Yu and Xiaochen Lian and Haichao Zhang and Wei Xu","journal":"arXiv","year":"2018","url":"https://arxiv.org/abs/1805.08329","type":"article"}],["arad2018compositional",{"title":"Compositional attention networks for machine reasoning","author":"Drew Arad Hudson and Christopher D. Manning","booktitle":"Proceedings of the International Conference on Learning Representations","year":"2018","url":"https://arxiv.org/abs/1803.03067","type":"inproceedings"}],["melis2017lstm",{"title":"On the state of the art of evaluation in neural language models","author":"G{\\'{a}}bor Melis and Chris Dyer and Phil Blunsom","journal":"Proceedings of the International Conference on Learning Representations","year":"2017","url":"http://arxiv.org/abs/1707.05589","type":"inproceedings"}],["bahdanau2018learning",{"author":"Dzmitry Bahdanau and Felix Hill and Jan Leike and Edward Hughes and Pushmeet Kohli and Edward Grefenstette","title":"Learning to follow language instructions with adversarial reward induction","journal":"arXiv","year":"2018","url":"https://arxiv.org/abs/1806.01946","type":"article"}],["andreas2016neural",{"title":"Neural module networks","author":"Jacob Andreas and Marcus Rohrbach and Trevor Darrell and Dan Klein","booktitle":"Proceedings of the Conference on Computer Vision and Pattern Recognition","year":"2016","url":"https://arxiv.org/abs/1511.02799","type":"inproceedings"}],["oreshkin2018tadam",{"title":"TADAM: Task dependent adaptive metric for improved few-shot learning","author":"Boris N. Oreshkin and Pau Rodriguez and Alexandre Lacoste","journal":"arXiv","year":"2018","url":"https://arxiv.org/abs/1805.10123","type":"article"}],["snell2017prototypical",{"title":"Prototypical networks for few-shot learning","author":"Snell, Jake and Swersky, Kevin and Zemel, Richard","booktitle":"Advances in Neural Information Processing Systems","year":"2017","url":"https://arxiv.org/abs/1703.05175","type":"inproceedings"}],["yang2018dataset",{"title":"A dataset and architecture for visual reasoning with a working memory","author":"Yang, Guangyu Robert and Ganichev, Igor and Wang, Xiao-Jing and Shlens, Jonathon and Sussillo, David","journal":"arXiv","year":"2018","url":"https://arxiv.org/abs/1803.06092","type":"article"}],["strub2018visual",{"author":"Strub, Florian and Seurin, Mathieu and Perez, Ethan and de Vries, Harm and Mary, J\\'{e}r\\'{e}mie and Preux, Philippe and Courville, Aaron and Pietquin, Olivier","title":"Visual Reasoning with Multi-hop Feature Modulation","booktitle":"ECCV","year":"2018","type":"inproceedings"}],["kazemzadeh2014referitgame",{"title":"ReferItGame: Referring to objects in photographs of natural scenes","author":"Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara","booktitle":"Conference on Empirical Methods in Natural Language Processing","year":"2014","url":"http://tamaraberg.com/papers/referit.pdf","type":"inproceedings"}]]
        </script>
    </d-bibliography> 

    <script type="text/javascript" src="static/js/index.bundle.js"></script>

    <distill-footer>
            <style>
            
            :host {
                color: rgba(255, 255, 255, 0.5);
                font-weight: 300;
                padding: 2rem 0;
                border-top: 1px solid rgba(0, 0, 0, 0.1);
                background-color: hsl(180, 5%, 15%); /*hsl(200, 60%, 15%);*/
                text-align: left;
                contain: content;
            }
            
            .footer-container .logo svg {
                width: 24px;
                position: relative;
                top: 4px;
                margin-right: 2px;
            }
            
            .footer-container .logo svg path {
                fill: none;
                stroke: rgba(255, 255, 255, 0.8);
                stroke-width: 3px;
            }
            
            .footer-container .logo {
                font-size: 17px;
                font-weight: 200;
                color: rgba(255, 255, 255, 0.8);
                text-decoration: none;
                margin-right: 6px;
            }
            
            .footer-container {
                grid-column: text;
            }
            
            .footer-container .nav {
                font-size: 0.9em;
                margin-top: 1.5em;
            }
            
            .footer-container .nav a {
                color: rgba(255, 255, 255, 0.8);
                margin-right: 6px;
                text-decoration: none;
            }
            
            </style>
            
        <div class="footer-container">
            <a href="/" class="logo">
                <svg viewBox="-607 419 64 64">
                <path d="M-573.4,478.9c-8,0-14.6-6.4-14.6-14.5s14.6-25.9,14.6-40.8c0,14.9,14.6,32.8,14.6,40.8S-565.4,478.9-573.4,478.9z"></path>
                </svg>
                Distill
            </a> is dedicated to clear explanations of machine learning
            
            <div class="nav">
                <a href="https://distill.pub/about/">About</a>
                <a href="https://distill.pub/journal/">Submit</a>
                <a href="https://distill.pub/prize/">Prize</a>
                <a href="https://distill.pub/archive/">Archive</a>
                <a href="https://distill.pub/rss.xml">RSS</a>
                <a href="https://github.com/distillpub">GitHub</a>
                <a href="https://twitter.com/distillpub">Twitter</a>
                &nbsp;&nbsp;&nbsp;&nbsp; ISSN 2476-0757
            </div>
        </div>
    </distill-footer>

    <!-- <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-83741880-1', 'auto');
        ga('send', 'pageview');
    </script> -->

    <footer>
        <small>&copy; 2018 <a href="http://helloflask.com/tutorial">HelloFlask</a></small>
    </footer>
</body>
</html>