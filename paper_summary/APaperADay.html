<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>A Paper A Day</title><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color: #ffffff; --text-color: #333333; --select-text-bg-color: #B5D6FC; --select-text-font-color: auto; --monospace: "Lucida Console",Consolas,"Courier",monospace; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; bottom: 0px; top: 0px; left: 0px; right: 0px; font-size: 1rem; line-height: 1.42857143; overflow-x: hidden; background-image: inherit; background-size: inherit; background-attachment: inherit; background-origin: inherit; background-clip: inherit; background-color: inherit; background-position: inherit inherit; background-repeat: inherit inherit; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; word-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export p { white-space: pre-wrap; }
@media screen and (max-width: 500px) { 
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
#write li > figure:first-child { margin-top: -20px; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; }
button, input, select, textarea { color: inherit; font-family: inherit; font-size: inherit; font-style: inherit; font-variant-caps: inherit; font-weight: inherit; font-stretch: inherit; line-height: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 2; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.701961); color: rgb(85, 85, 85); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px !important; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 80px; }
.CodeMirror-gutters { border-right-width: 0px; background-color: inherit; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background-image: inherit; background-size: inherit; background-attachment: inherit; background-origin: inherit; background-clip: inherit; background-color: inherit; position: relative !important; background-position: inherit inherit; background-repeat: inherit inherit; }
.md-diagram-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; background-position: 0px 0px; background-repeat: initial initial; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print { 
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid-page; break-before: avoid-page; }
  #write { margin-top: 0px; border-color: transparent !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  html.blink-to-pdf { font-size: 13px; }
  .typora-export #write { padding-left: 1cm; padding-right: 1cm; padding-bottom: 0px; break-after: avoid-page; }
  .typora-export #write::after { height: 0px; }
  @page { margin: 20mm 0px; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background-color: rgb(204, 204, 204); display: block; overflow-x: hidden; background-position: initial initial; background-repeat: initial initial; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
[contenteditable="true"]:active, [contenteditable="true"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-top-left-radius: 10px; border-top-right-radius: 10px; border-bottom-right-radius: 10px; border-bottom-left-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) { 
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background-color: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-top-left-radius: 3px; border-top-right-radius: 3px; border-bottom-right-radius: 3px; border-bottom-left-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; background-position: initial initial; background-repeat: initial initial; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.8; font-family: var(--monospace); }
code { text-align: left; }
a.md-print-anchor { white-space: pre !important; border: none !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; text-shadow: initial !important; background-position: 0px 0px !important; background-repeat: initial initial !important; }
.md-inline-math .MathJax_SVG .noError { display: none !important; }
.md-math-block .MathJax_SVG_Display { text-align: center; margin: 0px; position: relative; text-indent: 0px; max-width: none; max-height: none; min-height: 0px; min-width: 100%; width: auto; overflow-y: hidden; display: block !important; }
.MathJax_SVG_Display, .md-inline-math .MathJax_SVG_Display { width: auto; margin: inherit; display: inline-block !important; }
.MathJax_SVG .MJX-monospace { font-family: var(--monospace); }
.MathJax_SVG .MJX-sans-serif { font-family: sans-serif; }
.MathJax_SVG { display: inline; font-style: normal; font-weight: 400; line-height: normal; zoom: 90%; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; }
.MathJax_SVG * { transition: none; }
.MathJax_SVG_Display svg { vertical-align: middle !important; margin-bottom: 0px !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="mermaid"] svg, [lang="flow"] svg { max-width: 100%; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom-width: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }


:root {
    --side-bar-bg-color: #fff;
    --control-text-color: #777;
}

/* cyrillic-ext */
@font-face {
    font-family: 'Roboto Mono';
    font-style: normal;
    font-weight: 400;
    src: local('Roboto Mono'), local('RobotoMono-Regular'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/L0x5DF4xlVMF-BfR8bXMIjhGq3-cXbKDO1w.woff2') format('woff2');
    unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
}

/* cyrillic */
@font-face {
    font-family: 'Roboto Mono';
    font-style: normal;
    font-weight: 400;
    src: local('Roboto Mono'), local('RobotoMono-Regular'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/L0x5DF4xlVMF-BfR8bXMIjhPq3-cXbKDO1w.woff2') format('woff2');
    unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
}

/* greek-ext */
@font-face {
    font-family: 'Roboto Mono';
    font-style: normal;
    font-weight: 400;
    src: local('Roboto Mono'), local('RobotoMono-Regular'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/L0x5DF4xlVMF-BfR8bXMIjhHq3-cXbKDO1w.woff2') format('woff2');
    unicode-range: U+1F00-1FFF;
}

/* greek */
@font-face {
    font-family: 'Roboto Mono';
    font-style: normal;
    font-weight: 400;
    src: local('Roboto Mono'), local('RobotoMono-Regular'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/L0x5DF4xlVMF-BfR8bXMIjhIq3-cXbKDO1w.woff2') format('woff2');
    unicode-range: U+0370-03FF;
}

/* vietnamese */
@font-face {
    font-family: 'Roboto Mono';
    font-style: normal;
    font-weight: 400;
    src: local('Roboto Mono'), local('RobotoMono-Regular'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/L0x5DF4xlVMF-BfR8bXMIjhEq3-cXbKDO1w.woff2') format('woff2');
    unicode-range: U+0102-0103, U+0110-0111, U+1EA0-1EF9, U+20AB;
}

/* latin-ext */
@font-face {
    font-family: 'Roboto Mono';
    font-style: normal;
    font-weight: 400;
    src: local('Roboto Mono'), local('RobotoMono-Regular'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/L0x5DF4xlVMF-BfR8bXMIjhFq3-cXbKDO1w.woff2') format('woff2');
    unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}

/* latin */
@font-face {
    font-family: 'Roboto Mono';
    font-style: normal;
    font-weight: 400;
    src: local('Roboto Mono'), local('RobotoMono-Regular'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/L0x5DF4xlVMF-BfR8bXMIjhLq3-cXbKD.woff2') format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

/* cyrillic-ext */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 300;
    src: local('Source Sans Pro Light'), local('SourceSansPro-Light'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xKydSBYKcSV-LCoeQqfX1RYOo3ik4zwmhduz8A.woff2') format('woff2');
    unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
}

/* cyrillic */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 300;
    src: local('Source Sans Pro Light'), local('SourceSansPro-Light'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xKydSBYKcSV-LCoeQqfX1RYOo3ik4zwkxduz8A.woff2') format('woff2');
    unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
}

/* greek-ext */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 300;
    src: local('Source Sans Pro Light'), local('SourceSansPro-Light'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xKydSBYKcSV-LCoeQqfX1RYOo3ik4zwmxduz8A.woff2') format('woff2');
    unicode-range: U+1F00-1FFF;
}

/* greek */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 300;
    src: local('Source Sans Pro Light'), local('SourceSansPro-Light'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xKydSBYKcSV-LCoeQqfX1RYOo3ik4zwlBduz8A.woff2') format('woff2');
    unicode-range: U+0370-03FF;
}

/* vietnamese */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 300;
    src: local('Source Sans Pro Light'), local('SourceSansPro-Light'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xKydSBYKcSV-LCoeQqfX1RYOo3ik4zwmBduz8A.woff2') format('woff2');
    unicode-range: U+0102-0103, U+0110-0111, U+1EA0-1EF9, U+20AB;
}

/* latin-ext */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 300;
    src: local('Source Sans Pro Light'), local('SourceSansPro-Light'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xKydSBYKcSV-LCoeQqfX1RYOo3ik4zwmRduz8A.woff2') format('woff2');
    unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}

/* latin */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 300;
    src: local('Source Sans Pro Light'), local('SourceSansPro-Light'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xKydSBYKcSV-LCoeQqfX1RYOo3ik4zwlxdu.woff2') format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

/* cyrillic-ext */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 400;
    src: local('Source Sans Pro Regular'), local('SourceSansPro-Regular'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xK3dSBYKcSV-LCoeQqfX1RYOo3qNa7lqDY.woff2') format('woff2');
    unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
}

/* cyrillic */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 400;
    src: local('Source Sans Pro Regular'), local('SourceSansPro-Regular'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xK3dSBYKcSV-LCoeQqfX1RYOo3qPK7lqDY.woff2') format('woff2');
    unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
}

/* greek-ext */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 400;
    src: local('Source Sans Pro Regular'), local('SourceSansPro-Regular'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xK3dSBYKcSV-LCoeQqfX1RYOo3qNK7lqDY.woff2') format('woff2');
    unicode-range: U+1F00-1FFF;
}

/* greek */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 400;
    src: local('Source Sans Pro Regular'), local('SourceSansPro-Regular'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xK3dSBYKcSV-LCoeQqfX1RYOo3qO67lqDY.woff2') format('woff2');
    unicode-range: U+0370-03FF;
}

/* vietnamese */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 400;
    src: local('Source Sans Pro Regular'), local('SourceSansPro-Regular'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xK3dSBYKcSV-LCoeQqfX1RYOo3qN67lqDY.woff2') format('woff2');
    unicode-range: U+0102-0103, U+0110-0111, U+1EA0-1EF9, U+20AB;
}

/* latin-ext */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 400;
    src: local('Source Sans Pro Regular'), local('SourceSansPro-Regular'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xK3dSBYKcSV-LCoeQqfX1RYOo3qNq7lqDY.woff2') format('woff2');
    unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}

/* latin */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 400;
    src: local('Source Sans Pro Regular'), local('SourceSansPro-Regular'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xK3dSBYKcSV-LCoeQqfX1RYOo3qOK7l.woff2') format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

/* cyrillic-ext */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 600;
    src: local('Source Sans Pro SemiBold'), local('SourceSansPro-SemiBold'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xKydSBYKcSV-LCoeQqfX1RYOo3i54rwmhduz8A.woff2') format('woff2');
    unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
}

/* cyrillic */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 600;
    src: local('Source Sans Pro SemiBold'), local('SourceSansPro-SemiBold'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xKydSBYKcSV-LCoeQqfX1RYOo3i54rwkxduz8A.woff2') format('woff2');
    unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
}

/* greek-ext */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 600;
    src: local('Source Sans Pro SemiBold'), local('SourceSansPro-SemiBold'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xKydSBYKcSV-LCoeQqfX1RYOo3i54rwmxduz8A.woff2') format('woff2');
    unicode-range: U+1F00-1FFF;
}

/* greek */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 600;
    src: local('Source Sans Pro SemiBold'), local('SourceSansPro-SemiBold'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xKydSBYKcSV-LCoeQqfX1RYOo3i54rwlBduz8A.woff2') format('woff2');
    unicode-range: U+0370-03FF;
}

/* vietnamese */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 600;
    src: local('Source Sans Pro SemiBold'), local('SourceSansPro-SemiBold'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xKydSBYKcSV-LCoeQqfX1RYOo3i54rwmBduz8A.woff2') format('woff2');
    unicode-range: U+0102-0103, U+0110-0111, U+1EA0-1EF9, U+20AB;
}

/* latin-ext */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 600;
    src: local('Source Sans Pro SemiBold'), local('SourceSansPro-SemiBold'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xKydSBYKcSV-LCoeQqfX1RYOo3i54rwmRduz8A.woff2') format('woff2');
    unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}

/* latin */
@font-face {
    font-family: 'Source Sans Pro';
    font-style: normal;
    font-weight: 600;
    src: local('Source Sans Pro SemiBold'), local('SourceSansPro-SemiBold'), url('file:///Users/Herb/Library/Application%20Support/abnerworks.Typora/themes/vue/6xKydSBYKcSV-LCoeQqfX1RYOo3i54rwlxdu.woff2') format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

html {
    font-size: 16px;
}

body {
    font-family: Source Sans Pro, Helvetica Neue, Arial, sans-serif !important;
    color: #34495e;
    -webkit-font-smoothing: antialiased;
    line-height: 1.6rem;
    letter-spacing: 0;
    margin: 0;
    overflow-x: hidden;
}

#write {
    max-width: 860px;
    margin: 0 auto;
    padding: 20px 30px 40px 30px;
    padding-top: 20px;
    padding-bottom: 100px;
}

#write p {
    /* text-indent: 2rem; */
    line-height: 1.6rem;
    word-spacing: .05rem;
}

#write ol li {
    padding-left: 0.5rem;
}

#write>ul:first-child,
#write>ol:first-child {
    margin-top: 30px;
}

body>*:first-child {
    margin-top: 0 !important;
}

body>*:last-child {
    margin-bottom: 0 !important;
}

a {
    color: #42b983;
    font-weight: 600;
    padding: 0px 2px;
}

h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 1rem;
    font-weight: bold;
    line-height: 1.4;
    cursor: text;
}

h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}

h1 tt,
h1 code {
    font-size: inherit;
}

h2 tt,
h2 code {
    font-size: inherit;
}

h3 tt,
h3 code {
    font-size: inherit;
}

h4 tt,
h4 code {
    font-size: inherit;
}

h5 tt,
h5 code {
    font-size: inherit;
}

h6 tt,
h6 code {
    font-size: inherit;
}

h1 {
    padding-bottom: .4rem;
    font-size: 2.2rem;
    line-height: 1.3;
}

h2 {
    font-size: 1.75rem;
    line-height: 1.225;
    margin: 35px 0px 15px 0px;
}

h3 {
    font-size: 1.4rem;
    line-height: 1.43;
    margin: 20px 0px 7px 0px;
}

h4 {
    font-size: 1.2rem;
}

h5 {
    font-size: 1rem;
}

h6 {
    font-size: 1rem;
    color: #777;
}

p,
blockquote,
ul,
ol,
dl,
table {
    margin: 0.8em 0;
}

li>ol,
li>ul {
    margin: 0 0;
}

hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

body>h2:first-child {
    margin-top: 0;
    padding-top: 0;
}

body>h1:first-child {
    margin-top: 0;
    padding-top: 0;
}

body>h1:first-child+h2 {
    margin-top: 0;
    padding-top: 0;
}

body>h3:first-child,
body>h4:first-child,
body>h5:first-child,
body>h6:first-child {
    margin-top: 0;
    padding-top: 0;
}

a:first-child h1,
a:first-child h2,
a:first-child h3,
a:first-child h4,
a:first-child h5,
a:first-child h6 {
    margin-top: 0;
    padding-top: 0;
}

h1 p,
h2 p,
h3 p,
h4 p,
h5 p,
h6 p {
    margin-top: 0;
}

li p.first {
    display: inline-block;
}

ul,
ol {
    padding-left: 30px;
}

ul:first-child,
ol:first-child {
    margin-top: 0;
}

ul:last-child,
ol:last-child {
    margin-bottom: 0;
}

blockquote {
    border-left: 4px solid #42b983;
    padding: 10px 0px 10px 15px;
    color: #777;
    background-color: rgba(66, 185, 131, .1);
}

table {
    padding: 0;
    word-break: initial;
}

table tr {
    border-top: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}

table tr:nth-child(2n),
thead {
    background-color: #fafafa;
}

table tr th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}

table tr td {
    border: 1px solid #dfe2e5;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}

table tr th:first-child,
table tr td:first-child {
    margin-top: 0;
}

table tr th:last-child,
table tr td:last-child {
    margin-bottom: 0;
}

#write strong {
    padding: 0px 1px 0 1px;
}

#write em {
    padding: 0px 5px 0 2px;
}

#write table thead th {
    background-color: #f2f2f2;
}

#write .CodeMirror-gutters {
    border-right: none;
}

#write .md-fences {
    border: 1px solid #F4F4F4;
    -webkit-font-smoothing: initial;
    margin: 0.8rem 0 !important;
    padding: 0.3rem 0rem !important;
    line-height: 1.43rem;
    background-color: #F8F8F8 !important;
    border-radius: 2px;
    font-family: Roboto Mono, Source Sans Pro, Monaco, courier, monospace !important;
    font-size: 0.85rem;
    word-wrap: normal;
}

#write .CodeMirror-wrap .CodeMirror-code pre {
    padding-left: 12px;
}

#write code, tt {
    margin: 0 2px;
    padding: 2px 4px;
    border-radius: 2px;
    font-family: Source Sans Pro, Roboto Mono, Monaco, courier, monospace !important;
    font-size: 0.92rem;
    color: #e96900;
    background-color: #f8f8f8;
}

#write .md-footnote {
    background-color: #f8f8f8;
    color: #e96900;
}

/* heighlight. */
#write mark {
    background-color:#EBFFEB;
    border-radius: 2px;
    padding: 2px 4px;
    margin: 0 2px;
    color: #222;
    font-weight: 500;
}

#write del {
    padding: 1px 2px;
}

.cm-s-inner .cm-link,
.cm-s-inner.cm-link {
    color: #22a2c9;
}

.cm-s-inner .cm-string {
    color: #22a2c9;
}

.md-task-list-item>input {
    margin-left: -1.3em;
}

@media screen and (min-width: 914px) {
    /*body {
        width: 854px;
        margin: 0 auto;
    }*/
}

@media print {
    html {
        font-size: 13px;
    }
    table,
    pre {
        page-break-inside: avoid;
    }
    pre {
        word-wrap: break-word;
    }
}

.md-fences {
    background-color: #f8f8f8;
}

#write pre.md-meta-block {
    padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block>.code-tooltip {
    bottom: .375rem;
}

#write>h3.md-focus:before {
    left: -1.5625rem;
    top: .375rem;
}

#write>h4.md-focus:before {
    left: -1.5625rem;
    top: .285714286rem;
}

#write>h5.md-focus:before {
    left: -1.5625rem;
    top: .285714286rem;
}

#write>h6.md-focus:before {
    left: -1.5625rem;
    top: .285714286rem;
}

.md-image>.md-meta {
    /*border: 1px solid #ddd;*/
    border-radius: 3px;
    font-family: Consolas, "Liberation Mono", Courier, monospace;
    padding: 2px 0px 0px 4px;
    font-size: 0.9em;
    color: inherit;
}

.md-tag {
    color: inherit;
}

.md-toc {
    margin-top: 20px;
    padding-bottom: 20px;
}

.sidebar-tabs {
    border-bottom: none;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

#md-notification:before {
    top: 10px;
}

/** focus mode */

.on-focus-mode blockquote {
    border-left-color: rgba(85, 85, 85, 0.12);
}

header,
.context-menu,
.megamenu-content,
footer {
    font-family: "Segoe UI", "Arial", sans-serif;
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state {
    visibility: visible;
}

.mac-seamless-mode #typora-sidebar {
    background-color: #fafafa;
    background-color: var(--side-bar-bg-color);
}

.md-lang {
    color: #b4654d;
}

.html-for-mac .context-menu {
    --item-hover-bg-color: #E6F0FE;
}


</style>
</head>
<body class='typora-export' >
<div  id='write'  class = 'is-mac'><p><a href='../index.html'>返回到首页</a> | <a href='./index.html'>返回到 Paper Summary</a></p><hr /><p><img src='https://i.loli.net/2018/09/28/5bad80bf9e4bf.png' alt='' referrerPolicy='no-referrer' /></p><hr /><p>&nbsp;</p><h1><a name='header-n7' class='md-header-anchor '></a>✏️ A Paper A Day</h1><blockquote><p>Felt like I wasn’t reading enough – and what I was reading wasn’t sinking in enough. I also wanted to keep track of my sources in a more controlled manner. As a part of adding everything to my JabRef (maybe…), I figured I would write up my comments on papers. </p><p>The goal is to read and comment once a day and this <a href='./APaperADay.html'>post</a> will be updated day by day according to the reading process.</p></blockquote><div class='md-toc' mdtype='toc'><p class="md-toc-content"><span class="md-toc-item md-toc-h1" data-ref="n7"><a class="md-toc-inner" style="" href="#header-n7">✏️ A Paper A Day</a></span><span class="md-toc-item md-toc-h2" data-ref="n12"><a class="md-toc-inner" style="" href="#header-n12">🔁 Generative Models</a></span><span class="md-toc-item md-toc-h2" data-ref="n44"><a class="md-toc-inner" style="" href="#header-n44">👊 Adversarial Examples/Attacks</a></span><span class="md-toc-item md-toc-h2" data-ref="n53"><a class="md-toc-inner" style="" href="#header-n53">🎵 Sound &amp; Signal Processing</a></span><span class="md-toc-item md-toc-h2" data-ref="n71"><a class="md-toc-inner" style="" href="#header-n71">📉 Optimization &amp; Generalization</a></span><span class="md-toc-item md-toc-h2" data-ref="n84"><a class="md-toc-inner" style="" href="#header-n84">👍 Model Evaluation &amp; Performance &amp; Interpretion &amp; Visualization</a></span><span class="md-toc-item md-toc-h2" data-ref="n116"><a class="md-toc-inner" style="" href="#header-n116">🎛 Model Configuration</a></span><span class="md-toc-item md-toc-h2" data-ref="n138"><a class="md-toc-inner" style="" href="#header-n138">〽️ ODE &amp; PDE</a></span><span class="md-toc-item md-toc-h2" data-ref="n147"><a class="md-toc-inner" style="" href="#header-n147">⚛️ Physics Related</a></span><span class="md-toc-item md-toc-h2" data-ref="n155"><a class="md-toc-inner" style="" href="#header-n155">📚 Review</a></span><span class="md-toc-item md-toc-h2" data-ref="n182"><a class="md-toc-inner" style="" href="#header-n182">🖼 Figure Design &amp; Dimension Reduction</a></span></p></div><h2><a name='header-n12' class='md-header-anchor '></a>🔁 Generative Models</h2><p><strong>On Finding Local Nash Equilibria (and Only Local Nash Equilibria) in Zero-Sum Games</strong>. E V. Mazumdar, M I. Jordan, S. S Sastry [UC Berkeley] (2019) <a href='https://arxiv.org/abs/1901.00838'>arXiv:1901.00838</a></p><p><strong>Evaluating Generative Adversarial Networks on Explicitly Parameterized Distributions</strong>. S O&#39;Brien, M Groh, A Dubey [MIT] (2018) <a href='https://arxiv.org/abs/1812.10782'>arXiv:1812.10782</a> <a href='https://github.com/shayneobrien/explicit-gan-eval'>Github</a></p><p><strong>InstaGAN: Instance-aware Image-to-Image Translation</strong>. S Mo, M Cho, J Shin [Korea Advanced Institute of Science and Technology (KAIST) &amp; Pohang University of Science and Technology (POSTECH)] (2018) <a href='https://arxiv.org/abs/1812.10889'>arXiv:1812.10889</a> <a href='https://openreview.net/forum?id=ryxwJhC9YX'>OpenReview.net</a> <a href='https://github.com/sangwoomo/instagan'>Github</a>  <a href='https://www.jiqizhixin.com/articles/2019-01-02-17'>机器之心</a></p><p><strong>Improving Generalization and Stability of Generative Adversarial Networks</strong>. H Thanh-Tung, T Tran, S Venkatesh [Deakin University] (ICLR 2018) <a href='https://openreview.net/forum?id=ByxPYjC5KQ'>OpenReview.net</a> <a href='https://github.com/LMescheder/GAN_stability'>Github</a></p><p><strong>Finger-GAN: Generating Realistic Fingerprint Images Using Connectivity Imposed GAN</strong>. S Minaee, A Abdolrashidi [New York University &amp; University of California, Riverside] (2018) <a href='https://arxiv.org/abs/1812.10482'>arXiv:1812.10482</a> </p><p><strong>Generative Models from the perspective of Continual Learning</strong>. T Lesort, H Caselles-Dupré, M Garcia-Ortiz, A Stoian, D Filliat [Flowers Team (ENSTA ParisTech &amp; INRIA)] (2018) <a href='https://arxiv.org/abs/1812.09111'>arXiv:1812.09111</a> <a href='https://github.com/TLESORT/Generative_Continual_Learning'>Github</a> <a href='https://openreview.net/forum?id=S1eFtj0cKQ'>OpenReview.net</a></p><p><strong>A Probe into Understanding GAN and VAE models</strong>. J Zhang, L Mi, M Shen [MIT] (2018) <a href='https://arxiv.org/abs/1812.05676'>arXiv:1812.05676</a></p><p><strong>A Style-Based Generator Architecture for Generative Adversarial Networks</strong>. T Karras, S Laine, T Aila [NVIDIA] (2018) <a href='https://arxiv.org/abs/1812.04948'>arXiv:1812.04948</a> <a href='https://docs.google.com/document/d/1SDbnM1nxLZNuwD8fQkIigUve_SlihgoCmvjN3e388Us/edit'>Code</a> <a href='https://www.youtube.com/watch?v=kSLJriaOumA'>YouTube</a> <a href='https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650753854&idx=4&sn=683d862b174cb26c01c1e4d7541498d7'>机器之心</a></p><p><strong>Intra-class Variation Isolation in Conditional GANs</strong>. R T. Marriott, S Romdhani, L Chen [Ecole Centrale de Lyon &amp; IDEMIA] (2018) <a href='https://arxiv.org/abs/1811.11296'>arXiv:1811.11296</a></p><p><strong>Metropolis-Hastings Generative Adversarial Networks</strong>. R Turner, J Hung, Y Saatci, J Yosinski [Uber AI Labs] (2018) <a href='https://arxiv.org/abs/1811.11357'>arXiv:1811.11357</a> <a href='https://github.com/uber-research/metropolis-hastings-gans'>Github</a> <a href='https://eng.uber.com/mh-gan/'>Blog</a></p><p><strong>Label-Noise Robust Generative Adversarial Networks</strong>. Takuhiro Kaneko, Yoshitaka Ushiku, Tatsuya Harada [The University of Tokyo &amp; RIKEN] (2018) <a href='https://arxiv.org/abs/1811.11165'>arXiv:1811.11165</a></p><p><strong>Do GAN Loss Functions Really Matter?</strong>. Y Qin, N Mitra, P Wonka [KAUST &amp; UCL] (2018) <a href='https://arxiv.org/abs/1811.09567'>arXiv:1811.09567</a> <a href='https://www.reddit.com/r/MachineLearning/comments/a0p9wg/r_do_gan_loss_functions_really_matter/'>Reddit</a></p><p><strong>Copy the Old or Paint Anew? An Adversarial Framework for (non-) Parametric Image Stylization</strong>. N Jetchev, U Bergmann, G Yildirim [Zalando Research] (2018) <a href='https://arxiv.org/abs/1811.09236'>arXiv:1811.09236</a> <a href='https://github.com/zalandoresearch/famos'>GitHub</a></p><p><strong>Guiding the One-to-one Mapping in CycleGAN via Optimal Transport</strong>. G Lu, Z Zhou, Y Song, K Ren, Y Yu [Shanghai Jiao Tong University] (2018) <a href='https://arxiv.org/abs/1811.06284'>arXiv:1811.06284</a></p><p><strong>NEMGAN: Noise Engineered Mode-matching GAN</strong>. D Mishra, P AP, A J, P Pandey, S Chaudhury [Indian Institute of Technology Delhi] (2018) <a href='https://arxiv.org/abs/1811.03692'>arXiv:1811.03692</a> <a href='https://github.com/NEMGAN/NEMGAN'>GitHub</a></p><p><strong>Bias and Generalization in Deep Generative Models: An Empirical Study</strong>. S Zhao, H Ren, A Yuan, J Song, N Goodman, S Ermon [Stanford University] (<a href='https://sites.google.com/view/tadgm/home?authuser=0'>ICML2018</a>) <a href='https://arxiv.org/abs/1811.03259'>arXiv:1811.03259</a> <a href='https://github.com/ermongroup/BiasAndGeneralization'>GitHub</a></p><p><strong>Language GANs Falling Short</strong>. Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, Laurent Charlin [MILA, Université de Montréal &amp; MILA, McGill University &amp; MILA, HEC Montréal &amp; Google Brain &amp; Facebook AI Research] (2018) <a href='https://arxiv.org/abs/1811.02549v3'>arXiv:1811.02549</a></p><p><strong>CariGANs: Unpaired Photo-to-Caricature Translation</strong>. K Cao, J Liao, L Yuan [Tsinghua University &amp; Microsoft Research] (2018) <a href='https://arxiv.org/abs/1811.00222'>arXiv:1811.00222</a> <a href='https://cari-gan.github.io'>Blog</a> <a href='https://ai.stanford.edu/~kaidicao/cari-gan/index.html'>App</a> <a href='https://www.youtube.com/watch?v=V6G717ewUuw'>YouTube</a></p><p><strong>Large Scale GAN Training for High Fidelity Natural Image Synthesis</strong> | <a href='https://openreview.net/forum?id=B1xsqj09Fm'>OpenReview</a>. (2018) </p><p><strong>Generative adversarial networks and adversarial methods in biomedical image analysis</strong>. J M. Wolterink, K Kamnitsas, C Ledig, I Išgum [University Medical Center Utrecht &amp; Imperial College London &amp; Imagen Technologies] (2018) <a href='https://arxiv.org/abs/1810.10352'>arXiv:1810.10352</a></p><p><strong>Do Deep Generative Models Know What They Don&#39;t Know?</strong>. E Nalisnick, A Matsukawa, Y W Teh, D Gorur, B Lakshminarayanan [DeepMind] (2018) <a href='https://arxiv.org/abs/1810.09136'>arXiv:1810.09136</a></p><p><strong>Discriminator Rejection Sampling</strong>. Samaneh Azadi, Catherine Olsson, Trevor Darrell, Ian Goodfellow, Augustus Odena   [UC Berkeley &amp; Google Brain]. <a href='https://arxiv.org/abs/1810.06758'>arXiv:1810.06758</a></p><p><strong>Refacing: reconstructing anonymized facial features using GANs</strong>. D Abramian, A Eklund [Linkoping University] (2018) <a href='https://arxiv.org/abs/1810.06455'>arXiv:1810.06455</a></p><p><strong>ClusterGAN : Latent Space Clustering in Generative Adversarial Networks</strong>. S Mukherjee, H Asnani, E Lin, S Kannan [University of Washington] (2018) <a href='https://arxiv.org/abs/1809.03627'>arXiv:1809.03627</a></p><p><strong>Whispered-to-voiced Alaryngeal Speech Conversion with Generative Adversarial Networks</strong>. Santiago Pascual, Antonio Bonafonte, Joan Serrà, Jose A. Gonzalez [Universitat Polite`cnica de Catalunya &amp; Telefo ́nica Research &amp; Universidad de Ma ́laga, Spain] (2018) <a href='https://arxiv.org/abs/1808.10687'>arXiv:1808.10687</a></p><p><strong>The relativistic discriminator: a key element missing from standard GAN</strong>. Alexia Jolicoeur-Martineau [Lady Davis Institute Montreal, Canada] (2018) <a href='https://arxiv.org/abs/1807.00734'>arXiv:1807.00734</a></p><p><strong>Do GANs learn the distribution? Some Theory and Empirics</strong>. Sanjeev Arora, Andrej Risteski, Yi Zhang [Princeton University &amp; MIT] (ICLR 2018) <a href='https://openreview.net/forum?id=BJehNfW0-'>OpenReview.net</a></p><p><strong>Do GANs actually learn the distribution? An empirical study</strong>. Sanjeev Arora, Yi Zhang [] (2017) <a href='https://arxiv.org/abs/1706.08224'>arXiv:1706.08224</a> <a href='https://www.reddit.com/r/MachineLearning/comments/6jxes2/r_170608224_do_gans_actually_learn_the/'>Reddit</a> <a href='http://www.offconvex.org/2017/07/06/GANs3/'>Blog</a></p><p><strong>Generalization and Equilibrium in Generative Adversarial Nets (GANs)</strong>. S Arora, R Ge, Y Liang, T Ma, Y Zhang [Princeton University &amp; Duke University] (2017) <a href='https://arxiv.org/abs/1703.00573'>arXiv:1703.00573</a> <a href='http://www.offconvex.org/2017/03/30/GANs2/'>Blog</a> <a href='https://www.reddit.com/r/MachineLearning/comments/637u1l/r_generalization_and_equilibrium_in_generative/'>Reddit</a></p><p><strong>Improved Techniques for Training GANs</strong>. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen [OpenAI] (2016) <a href='https://arxiv.org/abs/1606.03498'>arXiv:1606.03498</a> <a href='https://github.com/openai/improved-gan'>Github</a> <a href='https://www.reddit.com/r/MachineLearning/comments/4o0aj9/160603498_improved_techniques_for_training_gans/'>Reddit</a></p><p>&nbsp;</p><h2><a name='header-n44' class='md-header-anchor '></a>👊 Adversarial Examples/Attacks</h2><p><strong>Multi-Label Adversarial Perturbations</strong>. Q Song, H Jin, X Huang, X Hu [Texas A&amp;M University] (2019) <a href='https://arxiv.org/abs/1901.00546'>arXiv:1901.00546</a></p><p><strong>Adversarial Transfer Learning</strong>. G Wilson, D J. Cook [Washington State University] (2018) <a href='https://arxiv.org/abs/1812.02849'>arXiv:1812.02849</a></p><p><strong>Defensive Dropout for Hardening Deep Neural Networks under Adversarial Attacks</strong>. S Wang, X Wang, P Zhao, W Wen, D Kaeli, P Chin, X Lin [Northeastern University &amp; Boston university &amp; Florida International University] (2018) <a href='https://arxiv.org/abs/1809.05165'>arXiv:1809.05165</a></p><p><strong>Are adversarial examples inevitable?</strong>. A Shafahi, W. R Huang, C Studer, S Feizi, T Goldstein (2018) <a href='https://arxiv.org/abs/1809.02104'>arXiv:1809.02104</a></p><p><strong>Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples</strong>. Anish Athalye, Nicholas Carlini, David Wagner [MIT &amp; Berkeley] (2018) <a href='https://arxiv.org/abs/1802.00420'>arXiv:1802.00420</a> <a href='https://github.com/anishathalye/obfuscated-gradients'>Github</a></p><p><strong>Generating Natural Adversarial Examples</strong>. Z Zhao, D Dua, S Singh [University of California, Irvine] (2017) <a href='https://arxiv.org/abs/1710.11342'>arXiv:1710.11342</a> <a href='https://github.com/zhengliz/natural-adversary'>Github</a> [comment]</p><p>&nbsp;</p><p>&nbsp;</p><h2><a name='header-n53' class='md-header-anchor '></a>🎵 Sound &amp; Signal Processing</h2><p><strong>Kymatio: Scattering Transforms in Python</strong>. M Andreux, T Angles, G Exarchakis... [PSL Research University &amp; Universite de Montreal &amp; New York University] (2018) <a href='https://arxiv.org/abs/1812.11214'>arXiv:1812.11214</a> <a href='https://www.kymat.io'>Home</a></p><p><strong>Deep Neural Networks for Automatic Classification of Anesthetic-Induced Unconsciousness</strong>. Konstantinos Patlatzoglou, etc. [etc.] (2018) <a href='./2018Patlatzoglou-DeepNeuralNetworks.pdf'>PDF</a></p><p><strong>Using Convolutional Neural Networks to Classify Audio Signal in Noisy Sound Scenes</strong>. M.V. Gubin [South Ural State University] (2018 GloSIC) <a href='https://ieeexplore.ieee.org/abstract/document/8570117'>PDF</a> <a href='https://github.com/gubinmv/cnn_in_noisy_scenes'>Github</a></p><p><strong>Interpretable Convolutional Filters with SincNet</strong>. M Ravanelli, Y Bengio [Université de Montréal] (2018) <a href='https://arxiv.org/abs/1811.09725'>arXiv:1811.09725</a>  <a href='https://github.com/mravanelli/SincNet/'>GitHub</a></p><p><strong>A Deep Neural Network for Unsupervised Anomaly Detection and Diagnosis in Multivariate Time Series Data</strong>. C Zhang, D Song, Y Chen, X Feng, C Lumezanu, W Cheng, J Ni, B Zong, H Chen, N V. Chawla [University of Notre Dame &amp; NEC Laboratories America &amp; Columbia University] (2018) <a href='https://arxiv.org/abs/1811.08055'>arXiv:1811.08055</a></p><p><strong>Stochastic Adaptive Neural Architecture Search for Keyword Spotting</strong>. T Véniat, O Schwander, L Denoyer [Sorbonne Université &amp; Facebook AI Research] (2018) <a href='https://arxiv.org/abs/1811.06753'>arXiv:1811.06753</a> <a href='https://github.com/TomVeniat/SANAS'>GitHub</a></p><p><strong>Unifying Probabilistic Models for Time-Frequency Analysis</strong>. W J. Wilkinson, M R Andersen, J D. Reiss, D Stowell, A Solin [Queen Mary University of London &amp; Aalto University] (2018) <a href='https://arxiv.org/abs/1811.02489'>arXiv:1811.02489</a> <a href='https://github.com/wil-j-wil/unifying-prob-time-freq'>GitHub</a></p><p><strong>WaveGlow: A Flow-based Generative Network for Speech Synthesis</strong>. Ryan Prenger, Rafael Valle, Bryan Catanzaro [NVIDIA Corporation] (2018) <a href='https://arxiv.org/abs/1811.00002'>arXiv:1811.00002</a> <a href='https://github.com/NVIDIA/waveglow'>Github</a></p><p><strong>Training neural audio classifiers with few data</strong>. J Pons, J Serrà, X Serra [Telefonica Research &amp; Universitat Pompeu Fabra] (2018) <a href='https://arxiv.org/abs/1810.10274'>arXiv:1810.10274</a> <a href='https://github.com/jordipons/neural-classifiers-with-few-audio/'>Github</a></p><p><strong>End-to-end music source separation: is it possible in the waveform domain?</strong>. F Lluís, J Pons, X Serra [Universitat Pompeu Fabra] (2018) <a href='https://arxiv.org/abs/1810.12187'>arXiv:1810.12187</a></p><p><strong>Multilevel Wavelet Decomposition Network for Interpretable Time Series Analysis</strong>. Jingyuan Wang, Ze Wang, Jianfeng Li, Junjie Wu.[Beihang University] (2018) <a href='https://arxiv.org/abs/1806.08946'>arXiv:1806.08946</a></p><p><strong>Deep Convolutional Neural Networks On Multichannel Time Series For Human Activity Recognition</strong>. Jian Bo Yang, Minh Nhut Nguyen, Phyo Phyo San, Xiao Li Li, Shonali Krishnaswamy (2015) <a href='http://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/download/10710/11297'>IJCAI2015</a></p><p><strong>Towards a universal neural network encoder for time series</strong>. J Serrà, S Pascual, A Karatzoglou [Telefonica Research &amp; Universitat Politecnica de Catalunya] (2018)</p><p><strong>Sound Event Detection Using Spatial Features and Convolutional Recurrent Neural Network</strong>. Sharath Adavanne, Pasi Pertilä, Tuomas Virtanen [Tampere University of Technology] (DCASE 2017) <a href='https://arxiv.org/abs/1706.02291'>arXiv:1706.02291</a> <a href='https://github.com/sharathadavanne/multichannel-sed-crnn'>Github</a></p><p><strong>Time Series Classification Using Multi-Channels Deep Convolutional Neural Networks</strong>. Yi Zheng, Qi Liu, Enhong Chen, Yong Ge, and J. Leon Zhao [USTC, et al.] (2014) <a href='http://staff.ustc.edu.cn/~cheneh/paper_pdf/2014/Yi-Zheng-WAIM2014.pdf'>WAIM2014</a></p><p>&nbsp;</p><p>&nbsp;</p><h2><a name='header-n71' class='md-header-anchor '></a>📉 Optimization &amp; Generalization</h2><p><strong>Elimination of All Bad Local Minima in Deep Learning</strong>. K Kawaguchi, L P Kaelbling [MIT] (2019) <a href='https://arxiv.org/abs/1901.00279'>arXiv:1901.00279</a></p><p><strong>Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks</strong>. B Neyshabur, Z Li... [Princeton &amp; Toyota Technological Institute at Chicago &amp; Facebook AI Research] (2018) <a href='https://arxiv.org/abs/1805.12076'>arXiv:1805.12076</a> <a href='https://github.com/bneyshabur/over-parametrization'>Github</a> <a href='https://www.reddit.com/r/MLEVN/comments/92u58v/towards_understanding_the_role_of/'>Reddit</a></p><p><strong>Visualizing the Loss Landscape of Neural Nets</strong>. H Li, Z Xu, G Taylor, T Goldstein [University of Maryland &amp; United States Naval Academy] (NIPS 2018) <a href='https://arxiv.org/abs/1712.09913'>arXiv:1712.09913</a> <a href='https://github.com/tomgoldstein/loss-landscape'>Github</a> <a href='https://www.reddit.com/r/MachineLearning/comments/7mr7j5/r_171209913_visualizing_the_loss_landscape_of/'>Reddit</a></p><p><strong>Gradient Descent Happens in a Tiny Subspace</strong>. G Gur-Ari, D A. Roberts, E Dyer [Institute for Advanced Study &amp; Facebook AI Research &amp; Johns Hopkins University] (2018) <a href='https://arxiv.org/abs/1812.04754'>arXiv:1812.04754</a></p><p><strong>A Sufficient Condition for Convergences of Adam and RMSProp</strong>. F Zou, L Shen, Z Jie, W Zhang, W Liu [Stony Brook University &amp; Tencent AI Lab] (2018) <a href='https://arxiv.org/abs/1811.09358'>arXiv:1811.09358</a></p><p><strong>Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks</strong>. D Zou, Y Cao, D Zhou, Q Gu [University of California, Los Angeles] (2018) <a href='https://arxiv.org/abs/1811.08888'>arXiv:1811.08888</a></p><p><strong>Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers</strong>. Z Allen-Zhu, Y Li, Y Liang [Microsoft Research AI &amp; Stanford University &amp; University of Wisconsin-Madison] (2018) <a href='https://arxiv.org/abs/1811.04918'>arXiv:1811.04918</a></p><p><strong>A Convergence Theory for Deep Learning via Over-Parameterization</strong>. Z Allen-Zhu, Y Li, Z Song [Microsoft Research AI &amp; Stanford University &amp; UT-Austin] (2018) <a href='https://arxiv.org/abs/1811.03962'>arXiv:1811.03962</a></p><p><strong>Gradient Descent Finds Global Minima of Deep Neural Networks</strong>. S S. Du, J D. Lee, H Li, L Wang, X Zhai [CMU &amp; University of Southern California &amp; Peking University &amp; MIT] (2018) <a href='https://arxiv.org/abs/1811.03804'>arXiv:1811.03804</a></p><p><strong>Identifying Generalization Properties in Neural Networks</strong>. H Wang, N S Keskar, C Xiong, R Socher [Salesforce Research] (2018) <a href='https://arxiv.org/abs/1809.07402'>arXiv:1809.07402</a></p><p><strong>Accelerating Natural Gradient with Higher-Order Invariance</strong>. by Yang Song <a href='https://ermongroup.github.io/blog/geo/'>Post</a> <a href='http://proceedings.mlr.press/v80/song18a/song18a.pdf'>paper</a></p><p>&nbsp;</p><h2><a name='header-n84' class='md-header-anchor '></a>👍 Model Evaluation &amp; Performance &amp; Interpretion &amp; Visualization</h2><p><strong>Attribute-Aware Attention Model for Fine-grained Representation Learning</strong>. K Han, J Guo, C Zhang, M Zhu [Peking University] (2019) <a href='https://arxiv.org/abs/1901.00392'>arXiv:1901.00392</a></p><p><strong>Multi-class Classification without Multi-class Labels</strong>. Y Hsu, Z Lv, J Schlosser, P Odom, Z Kira [Georgia Institute of Technology &amp; Georgia Tech Research Institute] (ICLR 2019) <a href='https://arxiv.org/abs/1901.00544'>arXiv:1901.00544</a></p><p><strong>Are All Training Examples Created Equal? An Empirical Study</strong>. K Vodrahalli, K Li, J Malik [UC Berkeley] (2018) <a href='https://arxiv.org/abs/1811.12569'>arXiv:1811.12569</a> <a href='https://zhuanlan.zhihu.com/p/52458512'>知乎</a></p><p><strong>Rethinking ImageNet Pre-training</strong>. K He, R Girshick, P Dollár [Facebook AI Research (FAIR)] (2018) <a href='https://arxiv.org/abs/1811.08883'>arXiv:1811.08883</a></p><p><strong>Efficient Identification of Approximate Best Configuration of Training in Large Datasets</strong>. S Huang, C Wang, B Ding, S Chaudhuri [University of Illinois &amp; Microsoft Research &amp; Alibaba Group] (2018) <a href='https://arxiv.org/abs/1811.03250'>arXiv:1811.03250</a></p><p><strong>Explaining Deep Learning Models - A Bayesian Non-parametric Approach</strong>. W Guo, S Huang, Y Tao, X Xing, L Lin [The Pennsylvania State University &amp; Netflix Inc &amp; Columbia University] (2018) <a href='https://arxiv.org/abs/1811.03422'>arXiv:1811.03422</a></p><p><strong>How deep is deep enough? - Optimizing deep neural network architecture</strong>. A Schilling, J Rietsch, R Gerum, H Schulze, C Metzner, P Krauss [University Hospital Erlangen &amp; Friedrich-Alexander University Erlangen-N¨urnberg (FAU)] (2018) <a href='https://arxiv.org/abs/1811.01753'>arXiv:1811.01753</a></p><p><strong>Approximate Fisher Information Matrix to Characterise the Training of Deep Neural Networks</strong>. Z Liao, T Drummond, I Reid, G Carneiro [University of Adelaide &amp; Monash University] (2018) <a href='https://arxiv.org/abs/1810.06767'>arXiv:1810.06767</a>  <a href='https://github.com/zhibinliao89/fisher.info.mat.torch'>GitHub</a> </p><p><strong>A Performance Evaluation of Convolutional Neural Networks for Face Anti Spoofing</strong>. Chaitanya Nagpal, Shiv Ram Dubey (2018) <a href='https://arxiv.org/abs/1805.04176'>arXiv:1805.04176</a></p><p><strong>An Information-Theoretic View for Deep Learning</strong>. J Zhang, T Liu, D Tao [UBTECH Sydney AI Centre] (2018) <a href='https://arxiv.org/abs/1804.09060'>arXiv:1804.09060</a></p><p><strong>Understanding Individual Neuron Importance Using Information Theory</strong>. K Liu, R A Amjad, B C. Geiger [Technical University of Munich &amp; Graz University of Technology] (2018) <a href='https://arxiv.org/abs/1804.06679'>arXiv:1804.06679</a></p><p><strong>Understanding Convolutional Neural Network Training with Information Theory</strong>. S Yu, R Jenssen, J C. Principe [University of Florida &amp; University of Tromsø] (2018) <a href='https://arxiv.org/abs/1804.06537'>arXiv:1804.06537</a></p><p><strong>A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay</strong>. Leslie N. Smith. <a href='https://arxiv.org/abs/1803.09820'>arXiv:1803.09820</a></p><p><strong>Focal Loss for Dense Object Detection</strong>. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár [Facebook AI Research] (2017) <a href='https://arxiv.org/abs/1708.02002'>arXiv:1708.02002</a> <a href='https://github.com/facebookresearch/Detectron'>Github</a></p><ul><li><p>Samples &amp; Datasets</p><p><strong>Image Score: How to Select Useful Samples</strong>. Simiao Zuo, Jialin Wu [University of Texas at Austin] (2018) <a href='https://arxiv.org/abs/1812.00334'>arXiv:1812.00334</a> <a href='https://www.reddit.com/r/MachineLearning/comments/a30cuw/181200334_image_score_how_to_select_useful_samples/'>Reddit</a></p><p><strong>Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift</strong>. S Rabanser, S Günnemann, Z C. Lipton [CMU &amp; Technical University of Munich] (2018) <a href='https://arxiv.org/abs/1810.11953'>arXiv:1810.11953</a></p><p><strong>How Many Samples are Needed to Learn a Convolutional Neural Network?</strong>. S S. Du, Y Wang, X Zhai, S Balakrishnan, R Salakhutdinov, A Singh [CMU &amp; University of Cambridge] (2018) <a href='https://arxiv.org/abs/1805.07883'>arXiv:1805.07883</a></p></li><li><p>Batch-size</p><p><strong>An Empirical Model of Large-Batch Training</strong>. Sam McCandlish, Jared Kaplan, Dario Amodei [OpenAI] (DECEMBER 14, 2018) <a href='https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/science-of-ai/An+Empirical+Model+of+Large-Batch+Training.pdf'>PDF</a> <a href='https://blog.openai.com/science-of-ai/'>BLOG</a></p><p><strong>Don&#39;t Use Large Mini-Batches, Use Local SGD</strong>. T Lin, S U. Stich, M Jaggi [EPFL] (2018) <a href='https://arxiv.org/abs/1808.07217'>arXiv:1808.07217</a></p><p><strong>Revisiting Small Batch Training for Deep Neural Networks</strong>. Dominic Masters, Carlo Luschi. (2018) <a href='https://arxiv.org/abs/1804.07612'>arXiv:1804.07612</a></p></li><li><p>Saliency</p><p><strong>Visualizing Deep Similarity Networks</strong>. A Stylianou, R Souvenir, R Pless [George Washington University &amp; Temple University] (2019) <a href='https://arxiv.org/abs/1901.00536'>arXiv:1901.00536</a> <a href='https://github.com/GWUvision/Similarity-Visualization'>Github</a></p><p><strong>Understanding Individual Decisions of CNNs via Contrastive Backpropagation</strong>. J Gu, Y Yang, V Tresp [the University of Munich &amp; Siemens AG] (2018) <a href='https://arxiv.org/abs/1812.02100'>arXiv:1812.02100</a></p><p><strong>Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values</strong>. J Adebayo, J Gilmer, I Goodfellow, B Kim [Google Brain] (2018) <a href='https://arxiv.org/abs/1810.03307'>arXiv:1810.03307</a>  <a href='https://openreview.net/forum?id=SJOYTK1vM'>OpenReview</a></p><p><strong>Sanity Checks for Saliency Maps</strong>. J Adebayo, J Gilmer, M Muelly, I Goodfellow, M Hardt, B Kim [Google Brain] (2018) <a href='https://arxiv.org/abs/1810.03292'>arXiv:1810.03292</a></p></li></ul><h2><a name='header-n116' class='md-header-anchor '></a>🎛 Model Configuration</h2><p><strong>Flow Based Self-supervised Pixel Embedding for Image Segmentation</strong>. B Ma, S Liu, Y Zhi, Q Song [CuraCloud] (2019) <a href='https://arxiv.org/abs/1901.00520'>arXiv:1901.00520</a></p><p><strong>Bag of Tricks for Image Classification with Convolutional Neural Networks</strong>. Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, Mu Li [AWS] (2018) <a href='https://arxiv.org/abs/1812.01187'>arXiv:1812.01187</a> <a href='https://www.reddit.com/r/MachineLearning/comments/a4dxna/r_bag_of_tricks_for_image_classification_with/'>Reddit</a> <a href='https://www.reddit.com/r/MachineLearning/comments/a5s8pv/r_a_bags_of_tricks_which_may_improve_deep/'>Slides[Reddit]</a></p><p><strong>Linear Backprop in non-linear networks</strong>. Mehrdad Yazdani [University of California San Diego] (NIPS 2018) <a href='https://openreview.net/forum?id=ByfPDyrYim'>OpenReview</a> <a href='https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650753228&idx=2&sn=fad16dfb7e96c4301ac3838f058ecaae'>机器之心</a></p><p><strong>Seeing in the dark with recurrent convolutional neural networks</strong>. T S. Hartmann [Harvard Medical School] (2018) <a href='https://arxiv.org/abs/1811.08537'>arXiv:1811.08537</a></p><p><strong>Dataset Distillation</strong>. T Wang, J Zhu, A Torralba, A A. Efros [Facebook AI Research &amp; MIT &amp; UC Berkeley] (2018) <a href='https://arxiv.org/abs/1811.10959'>arXiv:1811.10959</a></p><p><strong>Retina U-Net: Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection</strong>. P F. Jaeger, S A. A. Kohl, S Bickelhaupt, F Isensee, T A Kuder, H Schlemmer, K H. Maier-Hein [German Cancer Research Center] (2018) <a href='https://arxiv.org/abs/1811.08661'>arXiv:1811.08661</a> <a href='https://github.com/pfjaeger/medicaldetectiontoolkit'>GitHub</a></p><p><strong>Rethinking floating point for deep learning</strong>. Jeff Johnson [Facebook AI Research] (2018) <a href='https://arxiv.org/abs/1811.01721'>arXiv:1811.01721</a> <a href='https://github.com/facebookresearch/deepfloat'>Github</a> <a href='https://code.fb.com/ai-research/floating-point-math/'>Blog</a></p><p><strong>Quaternion Convolutional Neural Networks</strong>. Xuanyu Zhu / Yi Xu / Hongteng Xu / Changjian Chen. [Shanghai Jiao Tong University &amp; Duke University] (ECCV2018) (<a href='http://openaccess.thecvf.com/content_ECCV_2018/html/Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper.html'>pdf</a>)</p><p><strong>Why scatter plots suggest causality, and what we can do about it</strong>. C T. Bergstrom, J D. West [University of Washington] (2018) <a href='https://arxiv.org/abs/1809.09328'>arXiv:1809.09328</a></p><p><strong>Human activity recognition based on time series analysis using U-Net</strong>. Y Zhang, Y Zhang, Z Zhang, J Bao, Y Song [Beijing University of Posts and Telecommunications &amp; AIdong Super AI] (2018). <a href='https://arxiv.org/abs/1809.08113'>arXiv:1809.08113</a></p><p><strong>Backprop Evolution</strong>. M Alber, I Bello, B Zoph, P Kindermans, P Ramachandran, Q Le [TU Berlin &amp; Google Brain] (2018) <a href='https://arxiv.org/abs/1808.01974'>arXiv:1808.01974</a></p><p><strong>Smooth Loss Functions for Deep Top-k Classification</strong>. L Berrada, A Zisserman, M. P Kumar [University of Oxford] (2018) <a href='https://arxiv.org/abs/1802.07595'>arXiv:1802.07595</a> <a href='https://github.com/oval-group/smooth-topk'>Github</a></p><p><strong>Learning Confidence for Out-of-Distribution Detection in Neural Networks</strong>. T DeVries, G W. Taylor [University of Guelph &amp; Vector Institute] (2018) <a href='https://arxiv.org/abs/1802.04865'>arXiv:1802.04865</a></p><ul><li><p>Batch Normalization</p><ul><li><strong>Generalized Batch Normalization: Towards Accelerating Deep Neural Networks</strong>. X Yuan, Z Feng, M Norton, X Li [University of Florida &amp; Naval Postgraduate School] (2018) <a href='https://arxiv.org/abs/1812.03271'>arXiv:1812.03271</a></li></ul><ul><li><strong>How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)</strong>. S Santurkar, D Tsipras, A Ilyas, A Madry [MIT] (2018) <a href='https://arxiv.org/abs/1805.11604'>arXiv:1805.11604</a> <a href='https://www.youtube.com/watch?v=ZOabsYbmBRM'>YouTube</a> <a href='https://www.reddit.com/r/MachineLearning/comments/8n4eot/r_how_does_batch_normalization_help_optimization/'>Reddit</a> <a href='https://shaoanlu.wordpress.com/2018/07/12/notes-for-paper-how-does-batch-normalization-help-optimization-no-it-is-not-about-internal-covariate-shift/'>Notes from SALU</a></li></ul></li><li><p>Fenchel-Young Losses</p><ul><li><p><strong>Learning with Fenchel-Young Losses</strong>. M Blondel, A F. T. Martins, V Niculae [NTT Communication Science Laboratories &amp; Unbabel &amp; Instituto de Telecomunica¸coes &amp; Instituto de Telecomunica¸coes] (2019) <a href='https://arxiv.org/abs/1901.02324'>arXiv:1901.02324</a></p></li><li><p><strong>Learning Classifiers with Fenchel-Young Losses: Generalized Entropies, Margins, and Algorithms</strong>. M Blondel, A F. T. Martins, V Niculae [NTT Communication Science Laboratories &amp; Instituto de Telecomunicacoes] (2018) <a href='https://arxiv.org/abs/1805.09717'>arXiv:1805.09717</a> <a href='https://github.com/mblondel/fenchel-young-losses'>Github</a></p><p>&nbsp;</p></li></ul></li></ul><p>&nbsp;</p><h2><a name='header-n138' class='md-header-anchor '></a>〽️ ODE &amp; PDE</h2><p><strong>Data Driven Governing Equations Approximation Using Deep Neural Networks</strong>. T Qin, K Wu, D Xiu [The Ohio State University] (2018) <a href='https://arxiv.org/abs/1811.05537'>arXiv:1811.05537</a></p><p><strong>Neural Ordinary Differential Equations</strong>. Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud [University of Toronto, Canada] (NeurIPS 2018 | best paper) <a href='https://arxiv.org/abs/1806.07366'>arXiv:1806.07366</a> <a href='https://github.com/rtqichen/torchdiffeq'>Github</a> <a href='https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/'>Blog</a> <a href='https://mp.weixin.qq.com/s/ZEIsyV-0aTvYn6K8GyANPA'>机器之心</a>(【硬核NeruIPS 2018最佳论文，一个神经了的常微分方程】这是一篇神奇的论文，以前一层一层叠加的神经网络似乎突然变得连续了，反向传播也似乎不再需要一点一点往前传、一层一层更新参数了。 )</p><p>【《Neural Ordinary Differential Equations》论文解读】《Paper Summary: Neural Ordinary Differential Equations》by Branislav Holländer <a href='http://t.cn/EGPEh8y' target='_blank' class='url'>http://t.cn/EGPEh8y</a> </p><p><strong>Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations</strong>. M Raissi [Brown University] (2018) <a href='https://arxiv.org/abs/1804.07010'>arXiv:1804.07010</a></p><p>&nbsp;</p><p>&nbsp;</p><hr /><p>&nbsp;</p><h2><a name='header-n147' class='md-header-anchor '></a>⚛️ Physics Related</h2><p><strong>The Calabi-Yau Landscape: from Geometry, to Physics, to Machine-Learning</strong>. Y He (2018) <a href='https://arxiv.org/abs/1812.02893'>arXiv:1812.02893</a></p><p><strong>DeepSphere: Efficient spherical Convolutional Neural Network with HEALPix sampling for cosmological applications</strong>. N Perraudin, M Defferrard, T Kacprzak, R Sgier [aSwiss Data Science Center (SDSC) &amp; EPFL &amp; ETH Zurich] (2018) <a href='https://arxiv.org/abs/1810.12186'>arXiv:1810.12186</a></p><p><strong>Toward an AI Physicist for Unsupervised Learning</strong>. T Wu, M Tegmark [MIT] (2018) <a href='https://arxiv.org/abs/1810.10525'>arXiv:1810.10525</a></p><p><strong>Using Machine Learning to Predict the Evolution of Physics Research</strong>. W Liu, S Saganowski, P Kazienko, S A Cheong [Nanyang Technological University &amp; Wrocław University of Science and Technology] (2018) <a href='https://arxiv.org/abs/1810.12116'>arXiv:1810.12116</a></p><p><strong>hep-th</strong>. Y He, V Jejjala, B D. Nelson [University of London &amp; Nankai University &amp; Northeastern University] (2018) <a href='https://arxiv.org/abs/1807.00735'>arXiv:1807.00735</a> [comment]</p><p><strong>Physics-guided Neural Networks (PGNNs)</strong>. Anuj Karpatne, William Watkins, Jordan Read, Vipin Kumar [University of Minnesota] (2017) <a href='https://arxiv.org/abs/1710.11431'>arXiv:1710.11431</a></p><p>&nbsp;</p><h2><a name='header-n155' class='md-header-anchor '></a>📚 Review</h2><p><strong>How Generative Adversarial Networks and Their Variants Work: An Overview</strong>. Yongjun Hong, Uiwon Hwang, Jaeyoon Yoo, Sungroh Yoon [Seoul National University] (2017, 2018v9) <a href='https://arxiv.org/abs/1711.05914'>arXiv:1711.05914</a></p><p><strong>A Survey on Multi-output Learning</strong>. D Xu, Y Shi, I W. Tsang, Y Ong, C Gong, X Shen [ University of Technology Sydney &amp; Nanyang Technological University] (2019) <a href='https://arxiv.org/abs/1901.00248'>arXiv:1901.00248</a></p><p><strong>Analysis Methods in Neural Language Processing: A Survey</strong>. Y Belinkov, J Glass [MIT] (2018) <a href='https://arxiv.org/abs/1812.08951'>arXiv:1812.08951</a> <a href='https://github.com/boknilev/nlp-analysis-methods'>Github</a> <a href='https://boknilev.github.io/nlp-analysis-methods/'>Blog</a></p><p><strong>Neural Approaches to Conversational AI</strong>. J Gao, M Galley, L Li [Microsoft Research &amp; Google Brain] (2018) <a href='https://arxiv.org/abs/1809.08267'>arXiv:1809.08267</a></p><p><strong>Recent Advances in Autoencoder-Based Representation Learning</strong>. M Tschannen, O Bachem, M Lucic [ETH Zurich &amp; Google AI] (2018) <a href='https://arxiv.org/abs/1812.05069'>arXiv:1812.05069</a></p><p><strong>Learning From Positive and Unlabeled Data: A Survey</strong>. J Bekker, J Davis [KU Leuven] (2018) <a href='https://arxiv.org/abs/1811.04820'>arXiv:1811.04820</a></p><p><strong>Analyzing biological and artificial neural networks: challenges with opportunities for synergy?</strong>. David G.T. Barrett, Ari S. Morcos, Jakob H. Macke [DeepMind; Technical University of Munich, Germany] (2018) <a href='https://arxiv.org/abs/1810.13373'>arXiv:1810.13373</a></p><p><strong>Model Selection Techniques -- An Overview</strong>. J Ding, V Tarokh, Y Yang [University of Minnesota &amp; Duke University] (2018) <a href='https://arxiv.org/abs/1810.09583'>arXiv:1810.09583</a></p><p><strong>Deep Learning with the Random Neural Network and its Applications</strong>. Y Yin [Imperial College] (2018) <a href='https://arxiv.org/abs/1810.08653'>arXiv:1810.08653</a></p><p><strong>The Frontiers of Fairness in Machine Learning</strong>. A Chouldechova, A Roth [CMU &amp; University of Pennsylvania] (2018) <a href='https://arxiv.org/abs/1810.08810'>arXiv:1810.08810</a></p><p><strong>Applications of Deep Reinforcement Learning in Communications and Networking: A Survey</strong>. N C Luong, D T Hoang, S Gong, D Niyato, P Wang, Y Liang, D I Kim [Nanyang Technological University &amp; University of Technology Sydney &amp; Chinese Academy of Sciences] (2018) <a href='https://arxiv.org/abs/1810.07862'>arXiv:1810.07862</a></p><p><strong>A Survey on Deep Learning: Algorithms, Techniques, and Applications</strong>. Pouyanfar S, Sadiq S, Yan Y, et al [ACM Computing Surveys (CSUR)] (2018) (<a href='https://dl.acm.org/citation.cfm?id=3234150'>pdf</a>) (<a href='https://mp.weixin.qq.com/s/AQrgvjFPXUpqfqQQgOFN9A'>专知</a>)</p><p><strong>A Tale of Three Probabilistic Families: Discriminative, Descriptive and Generative Models</strong>. Y N Wu, R Gao, T Han, S Zhu [UCLA] (2018) <a href='https://arxiv.org/abs/1810.04261'>arXiv:1810.04261</a></p><p><strong>Deep learning for time series classification: a review</strong>. H I Fawaz, G Forestier, J Weber, L Idoumghar, P Muller [Université Haute Alsace] (2018) <a href='https://arxiv.org/abs/1809.04356'>arXiv:1809.04356</a></p><p><strong>A Survey on Deep Transfer Learning</strong>. C Tan, F Sun, T Kong, W Zhang, C Yang, C Liu [Tsinghua University] (2018) <a href='https://arxiv.org/abs/1808.01974'>arXiv:1808.01974</a></p><p><strong>Generalization Error in Deep Learning</strong>. D Jakubovitz, R Giryes, M R. D. Rodrigues [Tel Aviv University &amp; University College London] (2018) <a href='https://arxiv.org/abs/1808.01174'>arXiv:1808.01174</a></p><p><strong>How convolutional neural network see the world - A survey of convolutional neural network visualization methods</strong>. Z Qin, F Yu, C Liu, X Chen [George Mason University &amp; Clarkson University] (2018) <a href='https://arxiv.org/abs/1804.11191'>arXiv:1804.11191</a></p><p><strong>Deep Learning for Time-Series Analysis</strong>. John Gamboa [University of Kaiserslautern, Germany] (2017) <a href='https://arxiv.org/abs/1701.01887'>arXiv:1701.01887</a></p><p><strong>Deep Learning in Neural Networks: An Overview</strong>. Ju ̈rgen Schmidhuber [University of Lugano &amp; SUPSI, Switzerland] (2014) <a href='https://arxiv.org/abs/1404.7828'>arXiv:1404.7828</a></p><ul><li><p>Graph Neural Networks</p><p><strong>A Comprehensive Survey on Graph Neural Networks</strong>. Z Wu, S Pan, F Chen, G Long, C Zhang, P S. Yu [University of Technology Sydney &amp; Monash University &amp; University of Illinois at Chicago] (2019) <a href='https://arxiv.org/abs/1901.00596'>arXiv:1901.00596</a></p><p><strong>Graph Neural Networks: A Review of Methods and Applications</strong>. J Zhou, G Cui, Z Zhang, C Yang, Z Liu, M Sun [Tsinghua University] (2018) <a href='https://arxiv.org/abs/1812.08434'>arXiv:1812.08434</a></p><p><strong>Deep Learning on Graphs: A Survey</strong>. Z Zhang, P Cui, W Zhu [Tsinghua University] (2018) <a href='https://arxiv.org/abs/1812.04202'>arXiv:1812.04202</a></p></li></ul><p>&nbsp;</p><h2><a name='header-n182' class='md-header-anchor '></a>🖼 Figure Design &amp; Dimension Reduction</h2><p><strong>CFUN: Combining Faster R-CNN and U-net Network for Efficient Whole Heart Segmentation</strong>. Z Xu, Z Wu, J Feng [Tsinghua University] (2018) <a href='https://arxiv.org/abs/1812.04914'>arXiv:1812.04914</a> <a href='https://github.com/Wuziyi616/CFUN'>GitHub</a></p><p><strong>Deep Paper Gestalt</strong>. J Huang [Virginia Tech] (2018) <a href='https://arxiv.org/abs/1812.08775'>arXiv:1812.08775</a> <a href='https://github.com/vt-vl-lab/paper-gestalt'>GitHub</a> <a href='https://www.youtube.com/watch?v=yQLsZLf02yg'>YouTube</a> <a href='https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650754320&idx=1&sn=d6691ecd6adb98fa3c29986cf1efeebc&chksm=871a896eb06d00780f96e27b3ce3ee321c45ec21d30107f3e3778b5f674e172855a29fbcc70f&token=576114296&lang=zh_CN#rd'>机器之心</a></p><p><strong>A Tutorial on Distance Metric Learning: Mathematical Foundations, Algorithms and Software</strong>. J L Suárez, S García, F Herrera [University of Granada] (2018) <a href='https://arxiv.org/abs/1812.05944'>arXiv:1812.05944</a></p><p><strong>Improving Generalization for Abstract Reasoning Tasks Using Disentangled Feature Representations</strong>. X Steenbrugge, S Leroux, T Verbelen, B Dhoedt [Ghent University] (2018) <a href='https://arxiv.org/abs/1811.04784'>arXiv:1811.04784</a></p><p><strong>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</strong>. Leland McInnes and John Healy [Tutte Institute for Mathematics and Computing] (2018) <a href='https://arxiv.org/abs/1802.03426'>arXiv:1802.03426</a> <a href='https://github.com/lmcinnes/umap'>Github</a></p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><hr /><blockquote><p>Need to be reviewed.....</p></blockquote><p><strong>Entropic GANs meet VAEs: A Statistical Approach to Compute Sample Likelihoods in GANs</strong>. Y Balaji, H Hassani, R Chellappa, S Feizi [University of Maryland &amp; University of Pennsylvania] (2018) <a href='https://arxiv.org/abs/1810.04147'>arXiv:1810.04147</a> [comment]</p><p><strong>Analyzing the Noise Robustness of Deep Neural Networks</strong>. M Liu, S Liu, H Su, K Cao, J Zhu [Tsinghua University] (2018) <a href='https://arxiv.org/abs/1810.03913'>arXiv:1810.03913</a> [comment]</p><p><strong>Deep convolutional Gaussian processes</strong>. K Blomqvist, S Kaski, M Heinonen [Aalto university] (2018) <a href='https://arxiv.org/abs/1810.03052'>arXiv:1810.03052</a> <a href='https://github.com/kekeblom/DeepCGP'>GitHub</a> [comment]</p><p><strong>Learning Confidence Sets using Support Vector Machines</strong>. W Wang, X Qiao [Binghamton University] (2018) <a href='https://arxiv.org/abs/1809.10818'>arXiv:1809.10818</a> [comment]</p><p><strong>Learning with Random Learning Rates</strong>. L Blier, P Wolinski, Y Ollivier [Facebook AI Research &amp; Universite Paris Sud] (2018) <a href='https://arxiv.org/abs/1810.01322'>arXiv:1810.01322</a> <a href='https://github.com/leonardblier/alrao'>Github</a> <a href='https://leonardblier.github.io/alrao/'>Blog</a> [comment]</p><p><strong>Interpreting Adversarial Robustness: A View from Decision Surface in Input Space</strong>. F Yu, C Liu, Y Wang, X Chen [George Mason University &amp; Clarkson University &amp; Northeastern University] (2018) <a href='https://arxiv.org/abs/1810.00144'>arXiv:1810.00144</a> [comment]</p><p><strong>Spurious samples in deep generative models: bug or feature?</strong>. B Kégl, M Cherti, A Kazakçı [CNRS/Universite Paris-Saclay &amp; PSL Research University] (2018) <a href='https://arxiv.org/abs/1810.01876'>arXiv:1810.01876</a> [comment]</p><p><strong>Inhibited Softmax for Uncertainty Estimation in Neural Networks</strong>. M Możejko, M Susik, R Karczewski [Sigmoidal] (2018)  <a href='https://arxiv.org/abs/1810.01861'>arXiv:1810.01861</a> <a href='https://github.com/MSusik/Inhibited-softmax'>GitHub</a> [comment]</p><p><strong>Deep processing of structured data</strong>. Ł Maziarka, M Śmieja, A Nowak, J Tabor, Ł Struski, P Spurek [Jagiellonian University] (2018) <a href='https://arxiv.org/abs/1810.01989'>arXiv:1810.01989</a> [comment]</p><p><strong>Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow</strong>. Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, Sergey Levine [University of California, Berkeley] (2018) <a href='https://arxiv.org/abs/1810.00821'>arXiv:1810.00821</a></p><p><strong>Taming VAEs</strong>. D J Rezende, F Viola [DeepMind] (2018) <a href='https://arxiv.org/abs/1810.00597'>arXiv:1810.00597</a> [comment]</p><p><strong>Adversarial Attacks and Defences: A Survey</strong>. A Chakraborty, M Alam, V Dey, A Chattopadhyay, D Mukhopadhyay [Indian Institute of Technology &amp; The Ohio State University &amp; Nanyang Technological University] (2018) <a href='https://arxiv.org/abs/1810.00069'>arXiv:1810.00069</a> [comment]</p><p><strong>Over-Optimization of Academic Publishing Metrics: Observing Goodhart&#39;s Law in Action</strong>. M Fire, C Guestrin [University of Washington] (2018) <a href='https://arxiv.org/abs/1809.07841'>arXiv:1809.07841</a> [comment]</p><p><strong>On the loss landscape of a class of deep neural networks with no bad local valleys</strong>. Q Nguyen, M C Mukkamala, M Hein [Saarland University &amp; University of Tübingen] (2018) <a href='https://arxiv.org/abs/1809.10749'>arXiv:1809.10749</a> [comment]</p><p><strong>Conditional WaveGAN</strong>. Chae Young Lee, Anoop Toffy, Gue Jun Jung, Woo-Jin Han (2018) <a href='https://github.com/acheketa/cwavegan'>GitHub</a> <a href='https://arxiv.org/abs/1809.10636'>arXiv:1809.10636</a></p><p><strong>An analytic theory of generalization dynamics and transfer learning in deep linear networks</strong>. A K. Lampinen, S Ganguli [Stanford University] (2018) <a href='https://arxiv.org/abs/1809.10374'>arXiv:1809.10374</a> [comment]</p><p><strong>Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning</strong>. N Frazier-Logue, S J Hanson [Rutgers University] (2018) <a href='https://arxiv.org/abs/1808.03578'>arXiv:1808.03578</a> [comment]</p><p><strong>Grassmannian Learning: Embedding Geometry Awareness in Shallow and Deep Learning</strong>. J Zhang, G Zhu, R W. H Jr., a K Huang [The University of Hong Kong] (2018) <a href='https://arxiv.org/abs/1808.02229'>arXiv:1808.02229</a> [comment]</p><p><strong>Is Robustness the Cost of Accuracy? -- A Comprehensive Study on the Robustness of 18 Deep Image Classification Models</strong>. D Su, H Zhang... [IBM Research &amp; University of California, Davis &amp; MIT] (2018) <a href='https://arxiv.org/abs/1808.01688'>arXiv:1808.01688</a> <a href='https://github.com/huanzhang12/Adversarial_Survey'>GitHub</a> [comment]</p><p><strong>Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining</strong>. Xia Li, Jianlong Wu, Zhouchen Lin, Hong Liu, Hongbin Zha. [Peking University] (2018) <a href='https://arxiv.org/abs/1807.05698'>arXiv:1807.05698</a> <a href='https://github.com/XiaLiPKU/RESCAN'>GitHub</a> [comment]</p><p><strong>Toward Convolutional Blind Denoising of Real Photographs</strong>. S Guo, Z Yan, K Zhang, W Zuo, L Zhang [Harbin Institute of Technology &amp; The Hong Kong Polytechnic University] (2018) <a href='https://arxiv.org/abs/1807.04686'>arXiv:1807.04686</a> <a href='https://github.com/GuoShi28/CBDNet'>Github</a> [comment]</p><p><strong>Seamless Nudity Censorship: an Image-to-Image Translation Approach based on Adversarial Training</strong>. MD More, DM Souza, J Wehrmann, RC Barros (2018) <a href='https://www.researchgate.net/profile/Jonatas_Wehrmann/publication/325746502_Seamless_Nudity_Censorship_an_Image-to-Image_Translation_Approach_based_on_Adversarial_Training/links/5b2c7950aca2720785d66732/Seamless-Nudity-Censorship-an-Image-to-Image-Translation-Approach-based-on-Adversarial-Training.pdf'>ResearchGate</a> [comment]</p><p><strong>Classification and Geometry of General Perceptual Manifolds</strong>. SY Chung, DD Lee, H Sompolinsky [Harvard University] (2018) <a href='https://journals.aps.org/prx/abstract/10.1103/PhysRevX.8.031003'>PRX</a> [comment]</p><p><strong>The GAN Landscape: Losses, Architectures, Regularization, and Normalization</strong>. K Kurach, M Lucic, X Zhai, M Michalski, S Gelly [Google Brain] (2018) <a href='https://arxiv.org/abs/1807.04720'>arXiv:1807.04720</a> <a href='https://github.com/google/compare_gan'>Github</a> [comment]</p><p><strong>Troubling Trends in Machine Learning Scholarship</strong>. Z C. Lipton, J Steinhardt [Stanford University] (2018) <a href='https://arxiv.org/abs/1807.03341'>arXiv:1807.03341</a> [comment]</p><p><strong>On the Spectral Bias of Deep Neural Networks</strong>. N Rahaman, D Arpit, A Baratin, F Draxler, M Lin, F A. Hamprecht, Y Bengio, A Courville [Heidelberg University &amp; MILA] (2018) <a href='https://arxiv.org/abs/1806.08734'>arXiv:1806.08734</a> [comment]</p><p><strong>Opening the black box of deep learning</strong>. D Lei, X Chen, J Zhao [Shanghai University] (2018) <a href='https://arxiv.org/abs/1805.08355'>arXiv:1805.08355</a> [comment]</p><p><strong>Foundations of Sequence-to-Sequence Modeling for Time Series</strong>. V Kuznetsov, Z Mariet [Google Research &amp; MIT] (2018) <a href='https://arxiv.org/abs/1805.03714'>arXiv:1805.03714</a></p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><hr /><p><a href='../index.html'>返回到首页</a> | <a href='./APaperADay.html'>返回到顶部</a></p><div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://iphysresearch.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><p><br>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
<br></p><script type="application/json" class="js-hypothesis-config">
  {
    "openSidebar": false,
    "showHighlights": true,
    "theme": classic,
    "enableExperimentalNewNoteButton": true
  }
</script>
<script async="" src="https://hypothes.is/embed.js"></script><p>&nbsp;</p><p>&nbsp;</p></div>
</body>
</html>