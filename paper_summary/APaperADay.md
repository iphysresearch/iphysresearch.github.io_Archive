---
title: A Paper A Day
date: 2018-09-28
---

[ËøîÂõûÂà∞È¶ñÈ°µ](../index.html) | [ËøîÂõûÂà∞ Paper Summary](./index.html)

---

![](https://i.loli.net/2018/09/28/5bad80bf9e4bf.png)

---



# :pencil2: A Paper A Day

> Felt like I wasn‚Äôt reading enough ‚Äì and what I was reading wasn‚Äôt sinking in enough. I also wanted to keep track of my sources in a more controlled manner. As a part of adding everything to my JabRef (maybe‚Ä¶), I figured I would write up my comments on papers. 
>
> The goal is to read and comment once a day and this [post](./APaperADay.html) will be updated day by day according to the reading process.

[TOC]

## :repeat: Generative Models

**A Three-Player GAN: Generating Hard Samples To Improve Classification Networks**. S Vandenhende, B D Brabandere, D Neven, L V Gool [KU Leuven] (2019) [arXiv:1903.03496](https://arxiv.org/abs/1903.03496)

**O-GAN: Extremely Concise Approach for Auto-Encoding Generative Adversarial Networks**. Jianlin Su [Sun Yat-sen University] (2019) [arXiv:1903.01931](https://arxiv.org/abs/1903.01931) [Reddit](https://www.reddit.com/r/MachineLearning/comments/axw4aw/ogan_extremely_concise_approach_for_autoencoding/) [PaperWeekly](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247495491&idx=1&sn=978f0afeb0b38affe54fc9e6d6086e3c)

**AVP: Physics-informed Data Generation for Small-data Learning**. J Chen, Y Xie, K Wang, C Zhang, M A. Vannan, B Wang, Z Qian [Georgia Institute of Technology] (2019) [arXiv:1902.01522](https://arxiv.org/abs/1902.01522)

**A Layer-Based Sequential Framework for Scene Generation with GANs**. M O Turkoglu, W Thong, L Spreeuwers, B Kicanaoglu [University of Twente & University of Amsterdam] (2019) [arXiv:1902.00671](https://arxiv.org/abs/1902.00671) [Github](https://github.com/0zgur0/Seq_Scene_Gen)

**Perturbative GAN: GAN with Perturbation Layers**. Y Kishi, T Ikegami, S O'uchi, R Takano, W Nogami, T Kudoh [National Institute of Advanced Industrial Science and Technology, Tsukuba, Japan & The University of Tokyo] (2019) [arXiv:1902.01514](https://arxiv.org/abs/1902.01514)

**HyperGAN: A Generative Model for Diverse, Performant Neural Networks**. N Ratzlaff, L Fuxin [Oregon State University] (2019) [arXiv:1901.11058](https://arxiv.org/abs/1901.11058)

**Lie Group Auto-Encoder**. L Gong, Q Cheng [University of Kentucky] (2019) [arXiv:1901.09970](https://arxiv.org/abs/1901.09970)

**Maximum Entropy Generators for Energy-Based Models**. Rithesh Kumar, Anirudh Goyal, Aaron Courville, Yoshua Bengio [Mila & CIFAR & IVADO] (2019) [arXiv:1901.08508](https://arxiv.org/abs/1901.08508) [Reddit](https://www.reddit.com/r/MachineLearning/comments/ak1if1/r_maximum_entropy_generators_for_energybased/) [PaperWeekly](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247494846&idx=1&sn=0b1bdd770672f038d79b76cc31c1cbb4) [PaperWeekly](https://mp.weixin.qq.com/s/uGuywTY33SrYERDO522N1Q)

**MAD-GAN: Multivariate Anomaly Detection for Time Series Data with Generative Adversarial Networks**. D Li, D Chen, L Shi, B Jin, J Goh, S Ng [National University of Singapore & UC Berkeley & ST Electronics (Info Security) Pte Ltd] (2019) [arXiv:1901.04997](https://arxiv.org/abs/1901.04997)

**On Finding Local Nash Equilibria (and Only Local Nash Equilibria) in Zero-Sum Games**. E V. Mazumdar, M I. Jordan, S. S Sastry [UC Berkeley] (2019) [arXiv:1901.00838](https://arxiv.org/abs/1901.00838)

**Evaluating Generative Adversarial Networks on Explicitly Parameterized Distributions**. S O'Brien, M Groh, A Dubey [MIT] (2018) [arXiv:1812.10782](https://arxiv.org/abs/1812.10782) [Github](https://github.com/shayneobrien/explicit-gan-eval)

**InstaGAN: Instance-aware Image-to-Image Translation**. S Mo, M Cho, J Shin [Korea Advanced Institute of Science and Technology (KAIST) & Pohang University of Science and Technology (POSTECH)] (2018) [arXiv:1812.10889](https://arxiv.org/abs/1812.10889) [OpenReview.net](https://openreview.net/forum?id=ryxwJhC9YX) [Github](https://github.com/sangwoomo/instagan)  [Êú∫Âô®‰πãÂøÉ](https://www.jiqizhixin.com/articles/2019-01-02-17)

**Improving Generalization and Stability of Generative Adversarial Networks**. H Thanh-Tung, T Tran, S Venkatesh [Deakin University] (ICLR 2018) [OpenReview.net](https://openreview.net/forum?id=ByxPYjC5KQ) [Github](https://github.com/LMescheder/GAN_stability)

**Finger-GAN: Generating Realistic Fingerprint Images Using Connectivity Imposed GAN**. S Minaee, A Abdolrashidi [New York University & University of California, Riverside] (2018) [arXiv:1812.10482](https://arxiv.org/abs/1812.10482) 

**Generative Models from the perspective of Continual Learning**. T Lesort, H Caselles-Dupr√©, M Garcia-Ortiz, A Stoian, D Filliat [Flowers Team (ENSTA ParisTech & INRIA)] (2018) [arXiv:1812.09111](https://arxiv.org/abs/1812.09111) [Github](https://github.com/TLESORT/Generative_Continual_Learning) [OpenReview.net](https://openreview.net/forum?id=S1eFtj0cKQ)

**A Probe into Understanding GAN and VAE models**. J Zhang, L Mi, M Shen [MIT] (2018) [arXiv:1812.05676](https://arxiv.org/abs/1812.05676)

**A Style-Based Generator Architecture for Generative Adversarial Networks**. T Karras, S Laine, T Aila [NVIDIA] (2018) [arXiv:1812.04948](https://arxiv.org/abs/1812.04948) [Code](https://docs.google.com/document/d/1SDbnM1nxLZNuwD8fQkIigUve_SlihgoCmvjN3e388Us/edit) [YouTube](https://www.youtube.com/watch?v=kSLJriaOumA) [Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650753854&idx=4&sn=683d862b174cb26c01c1e4d7541498d7)

**Intra-class Variation Isolation in Conditional GANs**. R T. Marriott, S Romdhani, L Chen [Ecole Centrale de Lyon & IDEMIA] (2018) [arXiv:1811.11296](https://arxiv.org/abs/1811.11296)

**Metropolis-Hastings Generative Adversarial Networks**. R Turner, J Hung, Y Saatci, J Yosinski [Uber AI Labs] (2018) [arXiv:1811.11357](https://arxiv.org/abs/1811.11357) [Github](https://github.com/uber-research/metropolis-hastings-gans) [Blog](https://eng.uber.com/mh-gan/)

**Label-Noise Robust Generative Adversarial Networks**. Takuhiro Kaneko, Yoshitaka Ushiku, Tatsuya Harada [The University of Tokyo & RIKEN] (2018) [arXiv:1811.11165](https://arxiv.org/abs/1811.11165)

**Do GAN Loss Functions Really Matter?**. Y Qin, N Mitra, P Wonka [KAUST & UCL] (2018) [arXiv:1811.09567](https://arxiv.org/abs/1811.09567) [Reddit](https://www.reddit.com/r/MachineLearning/comments/a0p9wg/r_do_gan_loss_functions_really_matter/)

**Copy the Old or Paint Anew? An Adversarial Framework for (non-) Parametric Image Stylization**. N Jetchev, U Bergmann, G Yildirim [Zalando Research] (2018) [arXiv:1811.09236](https://arxiv.org/abs/1811.09236) [GitHub](https://github.com/zalandoresearch/famos)

**Guiding the One-to-one Mapping in CycleGAN via Optimal Transport**. G Lu, Z Zhou, Y Song, K Ren, Y Yu [Shanghai Jiao Tong University] (2018) [arXiv:1811.06284](https://arxiv.org/abs/1811.06284)

**NEMGAN: Noise Engineered Mode-matching GAN**. D Mishra, P AP, A J, P Pandey, S Chaudhury [Indian Institute of Technology Delhi] (2018) [arXiv:1811.03692](https://arxiv.org/abs/1811.03692) [GitHub](https://github.com/NEMGAN/NEMGAN)

**Bias and Generalization in Deep Generative Models: An Empirical Study**. S Zhao, H Ren, A Yuan, J Song, N Goodman, S Ermon [Stanford University] ([ICML2018](https://sites.google.com/view/tadgm/home?authuser=0)) [arXiv:1811.03259](https://arxiv.org/abs/1811.03259) [GitHub](https://github.com/ermongroup/BiasAndGeneralization)

**Language GANs Falling Short**. Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, Laurent Charlin [MILA, UniversiteÃÅ de MontreÃÅal & MILA, McGill University & MILA, HEC MontreÃÅal & Google Brain & Facebook AI Research] (2018) [arXiv:1811.02549](https://arxiv.org/abs/1811.02549v3)

**CariGANs: Unpaired Photo-to-Caricature Translation**. K Cao, J Liao, L Yuan [Tsinghua University & Microsoft Research] (2018) [arXiv:1811.00222](https://arxiv.org/abs/1811.00222) [Blog](https://cari-gan.github.io) [App](https://ai.stanford.edu/~kaidicao/cari-gan/index.html) [YouTube](https://www.youtube.com/watch?v=V6G717ewUuw)

**Large Scale GAN Training for High Fidelity Natural Image Synthesis** | [OpenReview](https://openreview.net/forum?id=B1xsqj09Fm). (2018) [More examples](https://drive.google.com/drive/folders/1lWC6XEPD0LT5KUnPXeve_kWeY-FxH002) [BigGAN Demo(Colab Notebook)](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb#scrollTo=Cd1dhL4Ykbm7) 

**Generative adversarial networks and adversarial methods in biomedical image analysis**. J M. Wolterink, K Kamnitsas, C Ledig, I I≈°gum [University Medical Center Utrecht & Imperial College London & Imagen Technologies] (2018) [arXiv:1810.10352](https://arxiv.org/abs/1810.10352)

**Do Deep Generative Models Know What They Don't Know?**. E Nalisnick, A Matsukawa, Y W Teh, D Gorur, B Lakshminarayanan [DeepMind] (2018) [arXiv:1810.09136](https://arxiv.org/abs/1810.09136)

**Discriminator Rejection Sampling**. Samaneh Azadi, Catherine Olsson, Trevor Darrell, Ian Goodfellow, Augustus Odena   [UC Berkeley & Google Brain]. [arXiv:1810.06758](https://arxiv.org/abs/1810.06758)

**Refacing: reconstructing anonymized facial features using GANs**. D Abramian, A Eklund [Linkoping University] (2018) [arXiv:1810.06455](https://arxiv.org/abs/1810.06455)

**ClusterGAN : Latent Space Clustering in Generative Adversarial Networks**. S Mukherjee, H Asnani, E Lin, S Kannan [University of Washington] (2018) [arXiv:1809.03627](https://arxiv.org/abs/1809.03627)

**Whispered-to-voiced Alaryngeal Speech Conversion with Generative Adversarial Networks**. Santiago Pascual, Antonio Bonafonte, Joan Serr√†, Jose A. Gonzalez [Universitat Polite`cnica de Catalunya & Telefo ÃÅnica Research & Universidad de Ma ÃÅlaga, Spain] (2018) [arXiv:1808.10687](https://arxiv.org/abs/1808.10687)

**The relativistic discriminator: a key element missing from standard GAN**. Alexia Jolicoeur-Martineau [Lady Davis Institute Montreal, Canada] (2018) [arXiv:1807.00734](https://arxiv.org/abs/1807.00734)

**Exploring Disentangled Feature Representation Beyond Face Identification**. Yu Liu, Fangyin Wei, Jing Shao, Lu Sheng, Junjie Yan, Xiaogang Wang [The Chinese University of Hong Kong, SenseTime Group Limited, Peking University] (2018) [arXiv:1804.03487](https://arxiv.org/abs/1804.03487)

**Do GANs learn the distribution? Some Theory and Empirics**. Sanjeev Arora, Andrej Risteski, Yi Zhang [Princeton University & MIT] (ICLR 2018) [OpenReview.net](https://openreview.net/forum?id=BJehNfW0-)

**Do GANs actually learn the distribution? An empirical study**. Sanjeev Arora, Yi Zhang [] (2017) [arXiv:1706.08224](https://arxiv.org/abs/1706.08224) [Reddit](https://www.reddit.com/r/MachineLearning/comments/6jxes2/r_170608224_do_gans_actually_learn_the/) [Blog](http://www.offconvex.org/2017/07/06/GANs3/)

**Generalization and Equilibrium in Generative Adversarial Nets (GANs)**. S Arora, R Ge, Y Liang, T Ma, Y Zhang [Princeton University & Duke University] (2017) [arXiv:1703.00573](https://arxiv.org/abs/1703.00573) [Blog](http://www.offconvex.org/2017/03/30/GANs2/) [Reddit](https://www.reddit.com/r/MachineLearning/comments/637u1l/r_generalization_and_equilibrium_in_generative/)

**InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets**. X Chen, Y Duan, R Houthooft, J Schulman, I Sutskever, P Abbeel [UC Berkeley & OpenAI] (2016) [arXiv:1606.03657](https://arxiv.org/abs/1606.03657) [Reddit](https://www.reddit.com/r/MachineLearning/comments/4o4shk/160603657_infogan_interpretable_representation/) 

**Improved Techniques for Training GANs**. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen [OpenAI] (2016) [arXiv:1606.03498](https://arxiv.org/abs/1606.03498) [Github](https://github.com/openai/improved-gan) [Reddit](https://www.reddit.com/r/MachineLearning/comments/4o0aj9/160603498_improved_techniques_for_training_gans/) [Github](https://github.com/openai/InfoGAN) [PyTorch](https://github.com/Natsu6767/InfoGAN-PyTorch)



## :facepunch: Adversarial Examples/Attacks

**GanDef: A GAN based Adversarial Training Defense for Neural Network Classifier**. G Liu, I Khalil, A Khreishah [New Jersey Institute of Technology & Qatar Computing Research Institute] (2019) https://arxiv.org/abs/1903.02585

**Adversarial Attacks on Time Series**. Fazle Karim, Somshubra Majumdar, Houshang Darabi [‚Ä¶] (2019) [arXiv:1902.10755](https://arxiv.org/abs/1902.10755)

**On Evaluating Adversarial Robustness**. N Carlini, A Athalye, N Papernot, W Brendel, J Rauber, D Tsipras, I Goodfellow, A Madry [Google Brain & MIT & University of T√ºbingen] (2019) [arXiv:1902.06705](https://arxiv.org/abs/1902.06705) [Github](https://github.com/evaluating-adversarial-robustness/adv-eval-paper) [Reddit](https://www.reddit.com/r/MachineLearning/comments/as2yhj/r_on_evaluating_adversarial_robustness/)

**Adversarial Examples Are a Natural Consequence of Test Error in Noise**. Nic Ford, Justin Gilmer, Nicolas Carlini, Dogus Cubuk [Google AI Residency] (2019) [arXiv:1901.10513](https://arxiv.org/abs/1901.10513) [Reddit](https://www.reddit.com/r/MachineLearning/comments/alppkp/190110513_adversarial_examples_are_a_natural/)

**Towards a Deeper Understanding of Adversarial Losses**. H Dong, Y Yang [Academia Sinica] (2019) [arXiv:1901.08753](https://arxiv.org/abs/1901.08753) [Github](https://github.com/salu133445/dan) [Blog](https://salu133445.github.io/dan/)

**Image Transformation can make Neural Networks more robust against Adversarial Examples**. D D Thang, T Matsui [Institute of Information Security] (2019) [arXiv:1901.03037](https://arxiv.org/abs/1901.03037)

**Multi-Label Adversarial Perturbations**. Q Song, H Jin, X Huang, X Hu [Texas A&M University] (2019) [arXiv:1901.00546](https://arxiv.org/abs/1901.00546)

**Adversarial Transfer Learning**. G Wilson, D J. Cook [Washington State University] (2018) [arXiv:1812.02849](https://arxiv.org/abs/1812.02849)

**Defensive Dropout for Hardening Deep Neural Networks under Adversarial Attacks**. S Wang, X Wang, P Zhao, W Wen, D Kaeli, P Chin, X Lin [Northeastern University & Boston university & Florida International University] (2018) [arXiv:1809.05165](https://arxiv.org/abs/1809.05165)

**Are adversarial examples inevitable?**. A Shafahi, W. R Huang, C Studer, S Feizi, T Goldstein (2018) [arXiv:1809.02104](https://arxiv.org/abs/1809.02104)

**Evolutionary Generative Adversarial Networks**. Chaoyue Wang, Chang Xu, Xin Yao, Dacheng Tao [2018] [[arXiv:1803.0065](https://arxiv.org/abs/1803.00657)7](https://arxiv.org/abs/1803.00657) [PaperWeekly](https://www.paperweekly.site/papers/2904)

**Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples**. Anish Athalye, Nicholas Carlini, David Wagner [MIT & Berkeley] (2018) [arXiv:1802.00420](https://arxiv.org/abs/1802.00420) [Github](https://github.com/anishathalye/obfuscated-gradients)

**Generating Natural Adversarial Examples**. Z Zhao, D Dua, S Singh [University of California, Irvine] (2017) [arXiv:1710.11342](https://arxiv.org/abs/1710.11342) [Github](https://github.com/zhengliz/natural-adversary) [comment]





## :musical_note: Sound & Signal Processing

**VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking**. Q Wang, H Muckenhirn, K Wilson, P Sridhar, Z Wu, J Hershey, R A. Saurous, R J. Weiss, Y Jia, I L Moreno [Google & Idiap Research Institute] (2018)  [arXiv:2018:04826](https://arxiv.org/abs/1810.04826) [Home](https://google.github.io/speaker-id/publications/VoiceFilter/) [VentureBeat](https://venturebeat.com/2018/10/12/google-researchers-use-ai-to-pick-out-voices-in-a-crowd/) [Tproger](https://tproger.ru/news/google-pick-out-voice/) [Êú∫Âô®‰πãÂøÉ](https://www.jiqizhixin.com/articles/2018-10-17-8) [Êñ∞Êô∫ÂÖÉ](https://wallstreetcn.com/articles/3419688)

**Phase-aware Speech Enhancement with Deep Complex U-Net**. H Choi, J Kim, J Huh, A Kim, J Ha, K Lee [Seoul National University & NAVER Corp] (2019) [arXiv:1903.03107](https://arxiv.org/abs/1903.03107) [Home](http://www.deepcomplexunet.tk) [OpenReview.net](https://openreview.net/forum?id=SkeRTsAcYm)

**A Deep Generative Model of Speech Complex Spectrograms**. A A Nugraha, K Sekiguchi, K Yoshii [RIKEN Center for Advanced Intelligence Project (AIP) & Kyoto University] (2019)  [arXiv:1903.03269](https://arxiv.org/abs/1903.03269)

**Utterance-level Aggregation For Speaker Recognition In The Wild**. W Xie, A Nagrani, J S Chung, A Zisserman [University of Oxford] (2019) [arXiv:1902.10107](https://arxiv.org/abs/1902.10107) [Blog](http://www.robots.ox.ac.uk/~vgg/research/speakerID/)

**catch22: CAnonical Time-series CHaracteristics**. C H Lubba, S S Sethi, P Knaute, S R Schultz, B D Fulcher, N S Jones [Imperial College London] (2019) [arXiv:1901.10200](https://arxiv.org/abs/1901.10200) [Github](https://github.com/chlubba/op_importance)

**End-to-End Probabilistic Inference for Nonstationary Audio Analysis**. W J. Wilkinson, M R Andersen, J D. Reiss, D Stowell, A Solin [Queen Mary University of London & Aalto University] (2019) [arXiv:1901.11436](https://arxiv.org/abs/1901.11436)

**Unsupervised speech representation learning using WaveNet autoencoders**. J Chorowski, R J. Weiss, S Bengio, A v d Oord [University of Wroc≈Çaw & Google Research & DeepMind] (2019) [arXiv:1901.08810](https://arxiv.org/abs/1901.08810)

**Kymatio: Scattering Transforms in Python**. M Andreux, T Angles, G Exarchakis... [PSL Research University & Universite de Montreal & New York University] (2018) [arXiv:1812.11214](https://arxiv.org/abs/1812.11214) [Home](https://www.kymat.io)

**Deep Neural Networks for Automatic Classification of Anesthetic-Induced Unconsciousness**. Konstantinos Patlatzoglou, etc. [etc.] (2018) [PDF](./2018Patlatzoglou-DeepNeuralNetworks.pdf)

**Using Convolutional Neural Networks to Classify Audio Signal in Noisy Sound Scenes**. M.V. Gubin [South Ural State University] (2018 GloSIC) [PDF](https://ieeexplore.ieee.org/abstract/document/8570117) [Github](https://github.com/gubinmv/cnn_in_noisy_scenes)

**Interpretable Convolutional Filters with SincNet**. M Ravanelli, Y Bengio [Universit√© de Montr√©al] (2018) [arXiv:1811.09725](https://arxiv.org/abs/1811.09725)  [GitHub](https://github.com/mravanelli/SincNet/)

**A Deep Neural Network for Unsupervised Anomaly Detection and Diagnosis in Multivariate Time Series Data**. C Zhang, D Song, Y Chen, X Feng, C Lumezanu, W Cheng, J Ni, B Zong, H Chen, N V. Chawla [University of Notre Dame & NEC Laboratories America & Columbia University] (2018) [arXiv:1811.08055](https://arxiv.org/abs/1811.08055)

**Stochastic Adaptive Neural Architecture Search for Keyword Spotting**. T V√©niat, O Schwander, L Denoyer [Sorbonne Universit√© & Facebook AI Research] (2018) [arXiv:1811.06753](https://arxiv.org/abs/1811.06753) [GitHub](https://github.com/TomVeniat/SANAS)

**Unifying Probabilistic Models for Time-Frequency Analysis**. W J. Wilkinson, M R Andersen, J D. Reiss, D Stowell, A Solin [Queen Mary University of London & Aalto University] (2018) [arXiv:1811.02489](https://arxiv.org/abs/1811.02489) [GitHub](https://github.com/wil-j-wil/unifying-prob-time-freq)

**WaveGlow: A Flow-based Generative Network for Speech Synthesis**. Ryan Prenger, Rafael Valle, Bryan Catanzaro [NVIDIA Corporation] (2018) [arXiv:1811.00002](https://arxiv.org/abs/1811.00002) [Github](https://github.com/NVIDIA/waveglow)

**Training neural audio classifiers with few data**. J Pons, J Serr√†, X Serra [Telefonica Research & Universitat Pompeu Fabra] (2018) [arXiv:1810.10274](https://arxiv.org/abs/1810.10274) [Github](https://github.com/jordipons/neural-classifiers-with-few-audio/)

**End-to-end music source separation: is it possible in the waveform domain?**. F Llu√≠s, J Pons, X Serra [Universitat Pompeu Fabra] (2018) [arXiv:1810.12187](https://arxiv.org/abs/1810.12187)

**Multilevel Wavelet Decomposition Network for Interpretable Time Series Analysis**. Jingyuan Wang, Ze Wang, Jianfeng Li, Junjie Wu.[Beihang University] (2018) [arXiv:1806.08946](https://arxiv.org/abs/1806.08946)

**Deep Convolutional Neural Networks On Multichannel Time Series For Human Activity Recognition**. Jian Bo Yang, Minh Nhut Nguyen, Phyo Phyo San, Xiao Li Li, Shonali Krishnaswamy (2015) [IJCAI2015](http://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/download/10710/11297)

**Towards a universal neural network encoder for time series**. J Serr√†, S Pascual, A Karatzoglou [Telefonica Research & Universitat Politecnica de Catalunya] (2018)

**Sound Event Detection Using Spatial Features and Convolutional Recurrent Neural Network**. Sharath Adavanne, Pasi Pertil√§, Tuomas Virtanen [Tampere University of Technology] (DCASE 2017) [arXiv:1706.02291](https://arxiv.org/abs/1706.02291) [Github](https://github.com/sharathadavanne/multichannel-sed-crnn)

**Time Series Classification Using Multi-Channels Deep Convolutional Neural Networks**. Yi Zheng, Qi Liu, Enhong Chen, Yong Ge, and J. Leon Zhao [USTC, et al.] (2014) [WAIM2014](http://staff.ustc.edu.cn/~cheneh/paper_pdf/2014/Yi-Zheng-WAIM2014.pdf)



## üìç Anomaly Detection

**Deep CNN-based Multi-task Learning for Open-Set Recognition**. P Oza, V M. Patel [Johns Hopkins University] (2019) [arXiv:1903.03161](https://arxiv.org/abs/1903.03161) [Github](https://github.com/otkupjnoz/mlosr)

**Learning Confidence Sets using Support Vector Machines**. W Wang, X Qiao [Binghamton University] (2018) [arXiv:1809.10818](https://arxiv.org/abs/1809.10818)

**Novelty Detection with GAN**, Mark Kliger, Shachar Fleishman [Amazon] [arXiv:1802.10560](https://arxiv.org/abs/1802.10560)

**Learning Confidence for Out-of-Distribution Detection in Neural Networks**. T DeVries, G W. Taylor [University of Guelph & Vector Institute] (2018) [arXiv:1802.04865](https://arxiv.org/abs/1802.04865)

**Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples**, Kimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin [Korea Advanced Institute of Science and Technology, University of Michigan, Google Brain] (ICLR 2018) [arXiv:1711.09325](https://arxiv.org/abs/1711.09325)



## Unsupervised / Domain Adaptation

**Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation**. C Lee, T Batra, M H Baig, D Ulbricht [Apple Inc] (2019) [arXiv:1903.04064](https://arxiv.org/abs/1903.04064)







## :chart_with_downwards_trend: Optimization & Generalization

**Transfusion: Understanding Transfer Learning with Applications to Medical Imaging**. M Raghu, C Zhang, J Kleinberg, S Bengio [Cornell University & Google Brain] (2019) [arXiv:1902.07208](https://arxiv.org/abs/1902.07208)

**Decoupled Greedy Learning of CNNs**. Eugene Belilovsky, Michael Eickenberg, Edouard Oyallon [University of Montreal , University of California Berkeley, CentraleSupelec and INRIA] (2019) [arXiv:1901.08164](https://arxiv.org/abs/1901.08164v1) [Github](https://github.com/eugenium/DGL)

**Eliminating all bad Local Minima from Loss Landscapes without even adding an Extra Unit**. J Sohl-Dickstein, K Kawaguchi [Google & MIT] (2019) [arXiv:1901.03909](https://arxiv.org/abs/1901.03909)

**The Benefits of Over-parameterization at Initialization in Deep ReLU Networks**. D Arpit, Y Bengio [Montreal Institute for Learning Algorithms] (2019) [arXiv:1901.03611](https://arxiv.org/abs/1901.03611)

**Generalization in Deep Networks: The Role of Distance from Initialization**. Vaishnavh Nagarajan, J. Zico Kolter [Carnegie-Mellon University] (NeurlPS 2017, 2019) [arXiv:1901.01672](https://arxiv.org/abs/1901.01672)

**Elimination of All Bad Local Minima in Deep Learning**. K Kawaguchi, L P Kaelbling [MIT] (2019) [arXiv:1901.00279](https://arxiv.org/abs/1901.00279)

**Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks**. B Neyshabur, Z Li... [Princeton & Toyota Technological Institute at Chicago & Facebook AI Research] (2018) [arXiv:1805.12076](https://arxiv.org/abs/1805.12076) [Github](https://github.com/bneyshabur/over-parametrization) [Reddit](https://www.reddit.com/r/MLEVN/comments/92u58v/towards_understanding_the_role_of/)

**Visualizing the Loss Landscape of Neural Nets**. H Li, Z Xu, G Taylor, T Goldstein [University of Maryland & United States Naval Academy] (NIPS 2018) [arXiv:1712.09913](https://arxiv.org/abs/1712.09913) [Github](https://github.com/tomgoldstein/loss-landscape) [Reddit](https://www.reddit.com/r/MachineLearning/comments/7mr7j5/r_171209913_visualizing_the_loss_landscape_of/)

**Gradient Descent Happens in a Tiny Subspace**. G Gur-Ari, D A. Roberts, E Dyer [Institute for Advanced Study & Facebook AI Research & Johns Hopkins University] (2018) [arXiv:1812.04754](https://arxiv.org/abs/1812.04754)

**A Sufficient Condition for Convergences of Adam and RMSProp**. F Zou, L Shen, Z Jie, W Zhang, W Liu [Stony Brook University & Tencent AI Lab] (2018) [arXiv:1811.09358](https://arxiv.org/abs/1811.09358)

**Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks**. D Zou, Y Cao, D Zhou, Q Gu [University of California, Los Angeles] (2018) [arXiv:1811.08888](https://arxiv.org/abs/1811.08888)

**Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers**. Z Allen-Zhu, Y Li, Y Liang [Microsoft Research AI & Stanford University & University of Wisconsin-Madison] (2018) [arXiv:1811.04918](https://arxiv.org/abs/1811.04918)

**A Convergence Theory for Deep Learning via Over-Parameterization**. Z Allen-Zhu, Y Li, Z Song [Microsoft Research AI & Stanford University & UT-Austin] (2018) [arXiv:1811.03962](https://arxiv.org/abs/1811.03962)

**Gradient Descent Finds Global Minima of Deep Neural Networks**. S S. Du, J D. Lee, H Li, L Wang, X Zhai [CMU & University of Southern California & Peking University & MIT] (2018) [arXiv:1811.03804](https://arxiv.org/abs/1811.03804)

**Identifying Generalization Properties in Neural Networks**. H Wang, N S Keskar, C Xiong, R Socher [Salesforce Research] (2018) [arXiv:1809.07402](https://arxiv.org/abs/1809.07402)

**Accelerating Natural Gradient with Higher-Order Invariance**. by Yang Song [Post](https://ermongroup.github.io/blog/geo/) [paper](http://proceedings.mlr.press/v80/song18a/song18a.pdf)



## :+1: Model Evaluation & Performance & Interpretion & Visualization

**Unmasking Clever Hans Predictors and Assessing What Machines Really Learn**. S Lapuschkin, S W√§ldchen, A Binder, G Montavon, W Samek, K M√ºller [Fraunhofer Heinrich Hertz Institute & Technische Universitat Berlin & Singapore University of Technology and Design] (2019) [arXiv:1902.10178](https://arxiv.org/abs/1902.10178)

**Seven Myths in Machine Learning Research**. O Chang, H Lipson [Columbia University] (2019) [arXiv:1902.06789](https://arxiv.org/abs/1902.06789) [Blog](https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/)

**Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent**. J Lee, L Xiao, S S. Schoenholz, Y Bahri, J Sohl-Dickstein, J Pennington [Google Brain] (2019) [arXiv:1902.06720](https://arxiv.org/abs/1902.06720) [Reddit](https://www.reddit.com/r/MachineLearning/comments/asifrt/190206720_wide_neural_networks_of_any_depth/)

**Impact of Fully Connected Layers on Performance of Convolutional Neural Networks for Image Classification**. S H S Basha, S R Dubey, V Pulabaigari, S Mukherjee [Indian Institute of Information Technology Sri City] [arXiv:1902.02771](https://arxiv.org/abs/1902.02771) [Github](https://github.com/shabbeersh/Impact-of-FC-layers)

**CHIP: Channel-wise Disentangled Interpretation of Deep Convolutional Neural Networks**. X Cui, D Wang, Z. J Wang [University of British Columbia] (2019) [arXiv:1902.02497](https://arxiv.org/abs/1902.02497)

**Are All Layers Created Equal?**. C Zhang, S Bengio, Y Singer [Google] (2019) [arXiv:1902.01996](https://arxiv.org/abs/1902.01996) [Êñ∞Êô∫ÂÖÉ](https://www.wxnmh.com/thread-4769772.htm)

**Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet**. W Brendel, M Bethge [Eberhard Karls University of T√ºbingen] (2019) [OpenReview.net](https://openreview.net/forum?id=SkfMWhAqYQ) [Reddit](https://www.reddit.com/r/MachineLearning/comments/amj0vv/r_approximating_cnns_with_bagoflocalfeatures/?ref=readnext) [Blogcomment](https://blog.evjang.com/2019/02/bagnet.html) [Blogcomment](https://medium.com/bethgelab/neural-networks-seem-to-follow-a-puzzlingly-simple-strategy-to-classify-images-f4229317261f) [ËøáÂæÄNetÔºåÁöÜ‰∏∫Ë∞ÉÂèÇÔºü‰∏ÄÁØáBagNetËÆ∫ÊñáÂºïÂèëÂ≠¶ÁïåÈúáÂä®](https://mp.weixin.qq.com/s/VmEc-OK60c6sZHYx4JzltA)

**See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification**. T Hu, H Qi [University of Chinese Academy of Sciences] (2019) [arXiv:1901.09891](https://arxiv.org/abs/1901.09891)

**Deep Learning on Small Datasets without Pre-Training using Cosine Loss**. B Barz, J Denzler [Friedrich Schiller University Jena] (2019) [arXiv:1901.09054](https://arxiv.org/abs/1901.09054) [Github](https://github.com/cvjena/semantic-embeddings/blob/v1.1.0/CosineLoss.md)

**Using Pre-Training Can Improve Model Robustness and Uncertainty**. D Hendrycks, K Lee, M Mazeika (2019) [arXiv:1901.09960](https://arxiv.org/abs/1901.09960)

**Understanding Geometry of Encoder-Decoder CNNs**. J C Ye, W K Sung [KAIST] (2019) [arXiv:1901.07647](https://arxiv.org/abs/1901.07647)

**Attribute-Aware Attention Model for Fine-grained Representation Learning**. K Han, J Guo, C Zhang, M Zhu [Peking University] (2019) [arXiv:1901.00392](https://arxiv.org/abs/1901.00392)

**Multi-class Classification without Multi-class Labels**. Y Hsu, Z Lv, J Schlosser, P Odom, Z Kira [Georgia Institute of Technology & Georgia Tech Research Institute] (ICLR 2019) [arXiv:1901.00544](https://arxiv.org/abs/1901.00544)

**Are All Training Examples Created Equal? An Empirical Study**. K Vodrahalli, K Li, J Malik [UC Berkeley] (2018) [arXiv:1811.12569](https://arxiv.org/abs/1811.12569) [Áü•‰πé](https://zhuanlan.zhihu.com/p/52458512)

**Rethinking ImageNet Pre-training**. K He, R Girshick, P Doll√°r [Facebook AI Research (FAIR)] (2018) [arXiv:1811.08883](https://arxiv.org/abs/1811.08883)

**Efficient Identification of Approximate Best Configuration of Training in Large Datasets**. S Huang, C Wang, B Ding, S Chaudhuri [University of Illinois & Microsoft Research & Alibaba Group] (2018) [arXiv:1811.03250](https://arxiv.org/abs/1811.03250)

**Explaining Deep Learning Models - A Bayesian Non-parametric Approach**. W Guo, S Huang, Y Tao, X Xing, L Lin [The Pennsylvania State University & Netflix Inc & Columbia University] (2018) [arXiv:1811.03422](https://arxiv.org/abs/1811.03422)

**How deep is deep enough? - Optimizing deep neural network architecture**. A Schilling, J Rietsch, R Gerum, H Schulze, C Metzner, P Krauss [University Hospital Erlangen & Friedrich-Alexander University Erlangen-N¬®urnberg (FAU)] (2018) [arXiv:1811.01753](https://arxiv.org/abs/1811.01753)

**Approximate Fisher Information Matrix to Characterise the Training of Deep Neural Networks**. Z Liao, T Drummond, I Reid, G Carneiro [University of Adelaide & Monash University] (2018) [arXiv:1810.06767](https://arxiv.org/abs/1810.06767)  [GitHub](https://github.com/zhibinliao89/fisher.info.mat.torch) 

**A Performance Evaluation of Convolutional Neural Networks for Face Anti Spoofing**. Chaitanya Nagpal, Shiv Ram Dubey (2018) [arXiv:1805.04176](https://arxiv.org/abs/1805.04176)

**An Information-Theoretic View for Deep Learning**. J Zhang, T Liu, D Tao [UBTECH Sydney AI Centre] (2018) [arXiv:1804.09060](https://arxiv.org/abs/1804.09060)

**Understanding Individual Neuron Importance Using Information Theory**. K Liu, R A Amjad, B C. Geiger [Technical University of Munich & Graz University of Technology] (2018) [arXiv:1804.06679](https://arxiv.org/abs/1804.06679)

**Understanding Convolutional Neural Network Training with Information Theory**. S Yu, R Jenssen, J C. Principe [University of Florida & University of Troms√∏] (2018) [arXiv:1804.06537](https://arxiv.org/abs/1804.06537)

**A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay**. Leslie N. Smith. [arXiv:1803.09820](https://arxiv.org/abs/1803.09820)

**Focal Loss for Dense Object Detection**. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Doll√°r [Facebook AI Research] (2017) [arXiv:1708.02002](https://arxiv.org/abs/1708.02002) [Github](https://github.com/facebookresearch/Detectron)

**How transferable are features in deep neural networks?**. J Yosinski, J Clune, Y Bengio, H Lipson [Cornell University, University of Wyoming, University of Montreal] (2014) [(NIPS 2014)](http://papers.nips.cc/book/advances-in-neural-information-processing-systems-27-2014) 

- Samples & Datasets

  **Do we train on test data? Purging CIFAR of near-duplicates**. B Barz, J Denzler [Friedrich Schiller University Jena] (2019) [arXiv:1902.00423](https://arxiv.org/abs/1902.00423) [Blog](https://cvjena.github.io/cifair/)

  **Semantic Redundancies in Image-Classification Datasets: The 10% You Don't Need**. V Birodkar, H Mobahi, S Bengio [Google Research] (2019) [arXiv:1901.11409](https://arxiv.org/abs/1901.11409)

  **Image Score: How to Select Useful Samples**. Simiao Zuo, Jialin Wu [University of Texas at Austin] (2018) [arXiv:1812.00334](https://arxiv.org/abs/1812.00334) [Reddit](https://www.reddit.com/r/MachineLearning/comments/a30cuw/181200334_image_score_how_to_select_useful_samples/)

  **Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift**. S Rabanser, S G√ºnnemann, Z C. Lipton [CMU & Technical University of Munich] (2018) [arXiv:1810.11953](https://arxiv.org/abs/1810.11953)

  **How Many Samples are Needed to Learn a Convolutional Neural Network?**. S S. Du, Y Wang, X Zhai, S Balakrishnan, R Salakhutdinov, A Singh [CMU & University of Cambridge] (2018) [arXiv:1805.07883](https://arxiv.org/abs/1805.07883)

- Batch-size

  **Interplay Between Optimization and Generalization of Stochastic Gradient Descent with Covariance Noise**. Y Wen, K Luk, M Gazeau, G Zhang, H Chan, J Ba [Borealis AI & University of Toronto] (2019) [arXiv:1902.08234](https://arxiv.org/abs/1902.08234)

  **An Empirical Model of Large-Batch Training**. Sam McCandlish, Jared Kaplan, Dario Amodei [OpenAI] (DECEMBER 14, 2018) [PDF](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/science-of-ai/An+Empirical+Model+of+Large-Batch+Training.pdf) [BLOG](https://blog.openai.com/science-of-ai/)

  **Don't Use Large Mini-Batches, Use Local SGD**. T Lin, S U. Stich, M Jaggi [EPFL] (2018) [arXiv:1808.07217](https://arxiv.org/abs/1808.07217)

  **Revisiting Small Batch Training for Deep Neural Networks**. Dominic Masters, Carlo Luschi. (2018) [arXiv:1804.07612](https://arxiv.org/abs/1804.07612)

- Saliency

  **Why are Saliency Maps Noisy? Cause of and Solution to Noisy Saliency Maps**. B Kim, J Seo, S Jeon, J Koo, J Choe, T Jeon [Korea Advanced Institute of Science and Technology & Daejeon] (2019) [arXiv:1902.04893](https://arxiv.org/abs/1902.04893) [Github](https://github.com/1202kbs/Rectified-Gradient)

  **Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation**. S Singla, E Wallace, S Feng, S Feizi [University of Maryland] (2019) [arXiv:1902.00407](https://arxiv.org/abs/1902.00407) [Reddit](https://www.reddit.com/r/MachineLearning/comments/an1g14/r_understanding_impacts_of_highorder_loss/)

  **Visualizing Deep Similarity Networks**. A Stylianou, R Souvenir, R Pless [George Washington University & Temple University] (2019) [arXiv:1901.00536](https://arxiv.org/abs/1901.00536) [Github](https://github.com/GWUvision/Similarity-Visualization)

  **Understanding Individual Decisions of CNNs via Contrastive Backpropagation**. J Gu, Y Yang, V Tresp [the University of Munich & Siemens AG] (2018) [arXiv:1812.02100](https://arxiv.org/abs/1812.02100)

  **Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values**. J Adebayo, J Gilmer, I Goodfellow, B Kim [Google Brain] (2018) [arXiv:1810.03307](https://arxiv.org/abs/1810.03307)  [OpenReview](https://openreview.net/forum?id=SJOYTK1vM)

  **Sanity Checks for Saliency Maps**. J Adebayo, J Gilmer, M Muelly, I Goodfellow, M Hardt, B Kim [Google Brain] (2018) [arXiv:1810.03292](https://arxiv.org/abs/1810.03292)

- Explanatory Graphs

  **Explanatory Graphs for CNNs**. Q Zhang, X Wang, R Cao, Y N Wu, F Shi, S Zhu [Shanghai Jiao Tong University & University of California, Los Angeles] (2018) [arXiv:1812.07997](https://arxiv.org/abs/1812.07997)





## :control_knobs: Model Configuration

**Ising-Dropout: A Regularization Method for Training and Compression of Deep Neural Networks**. H Salehinejad, S Valaee [University of Toronto] (2019) [arXiv:1902.08673](https://arxiv.org/abs/1902.08673)

**LocalNorm: Robust Image Classification through Dynamically Regularized Normalization**. B Yin, S Schaafsma, H Corporaal, H. S Scholte, S M. Bohte [Centrum Wiskunde & Informatica (CWI) & Holst Centre / IMEC] (2019) [arXiv:1902.06550](https://arxiv.org/abs/1902.06550)

**On the Impact of the Activation Function on Deep Neural Networks Training**. S Hayou, A Doucet, J Rousseau [Universiy of Oxford] (2019) [arXiv:1902.06853](https://arxiv.org/abs/1902.06853) [Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650757440&idx=1&sn=4164f2d1c88df07ec80b7d24e8fc3c8f&chksm=871a9d3eb06d14289928961cc6dcca5d9eb54d47374b4417dcda48724d2857d24b2ed3bad29c&token=667209389&lang=zh_CN#rd)

**TUNet: Incorporating segmentation maps to improve classification**. Y Tian [New York University] (2019)  [arXiv:1901.11379](https://arxiv.org/abs/1901.11379)

**Weighted Channel Dropout for Regularization of Deep Convolutional Neural Network**. Saihui Hou, Zilei Wang [USTC] (AAAI 2019) [Paper](http://home.ustc.edu.cn/~saihui/papers/aaai2019_weighted.pdf) [Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650756694&idx=3&sn=007bc096a14d2c0d5ace149322953dc6)

**Fixup Initialization: Residual Learning Without Normalization**. H Zhang, Y N. Dauphin, T Ma [MIT & Google Brain & Stanford University] (2019) [arXiv:1901.09321](https://arxiv.org/abs/1901.09321) [Reddit](https://www.reddit.com/r/MachineLearning/comments/aler62/d_l2_regularization_and_batch_norm/) [Github](https://github.com/ajbrock/BoilerPlate/blob/master/Models/fixup.py)

**Training Neural Networks with Local Error Signals**. A N√∏kland, L H Eidnes [Trondheim] (2019) [arXiv:1901.06656](https://arxiv.org/abs/1901.06656) [GitHub](https://github.com/anokland/local-loss)

**Flow Based Self-supervised Pixel Embedding for Image Segmentation**. B Ma, S Liu, Y Zhi, Q Song [CuraCloud] (2019) [arXiv:1901.00520](https://arxiv.org/abs/1901.00520)

**Bag of Tricks for Image Classification with Convolutional Neural Networks**. Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, Mu Li [AWS] (2018) [arXiv:1812.01187](https://arxiv.org/abs/1812.01187) [Reddit](https://www.reddit.com/r/MachineLearning/comments/a4dxna/r_bag_of_tricks_for_image_classification_with/) [Slides[Reddit]](https://www.reddit.com/r/MachineLearning/comments/a5s8pv/r_a_bags_of_tricks_which_may_improve_deep/)

**Linear Backprop in non-linear networks**. Mehrdad Yazdani [University of California San Diego] (NIPS 2018) [OpenReview](https://openreview.net/forum?id=ByfPDyrYim) [Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650753228&idx=2&sn=fad16dfb7e96c4301ac3838f058ecaae)

**Seeing in the dark with recurrent convolutional neural networks**. T S. Hartmann [Harvard Medical School] (2018) [arXiv:1811.08537](https://arxiv.org/abs/1811.08537)

**Dataset Distillation**. T Wang, J Zhu, A Torralba, A A. Efros [Facebook AI Research & MIT & UC Berkeley] (2018) [arXiv:1811.10959](https://arxiv.org/abs/1811.10959)

**Retina U-Net: Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection**. P F. Jaeger, S A. A. Kohl, S Bickelhaupt, F Isensee, T A Kuder, H Schlemmer, K H. Maier-Hein [German Cancer Research Center] (2018) [arXiv:1811.08661](https://arxiv.org/abs/1811.08661) [GitHub](https://github.com/pfjaeger/medicaldetectiontoolkit)

**Rethinking floating point for deep learning**. Jeff Johnson [Facebook AI Research] (2018) [arXiv:1811.01721](https://arxiv.org/abs/1811.01721) [Github](https://github.com/facebookresearch/deepfloat) [Blog](https://code.fb.com/ai-research/floating-point-math/)

**Quaternion Convolutional Neural Networks**. Xuanyu Zhu / Yi Xu / Hongteng Xu / Changjian Chen. [Shanghai Jiao Tong University & Duke University] (ECCV2018) ([pdf](http://openaccess.thecvf.com/content_ECCV_2018/html/Xuanyu_Zhu_Quaternion_Convolutional_Neural_ECCV_2018_paper.html))

**Why scatter plots suggest causality, and what we can do about it**. C T. Bergstrom, J D. West [University of Washington] (2018) [arXiv:1809.09328](https://arxiv.org/abs/1809.09328)

**Human activity recognition based on time series analysis using U-Net**. Y Zhang, Y Zhang, Z Zhang, J Bao, Y Song [Beijing University of Posts and Telecommunications & AIdong Super AI] (2018). [arXiv:1809.08113](https://arxiv.org/abs/1809.08113)

**Backprop Evolution**. M Alber, I Bello, B Zoph, P Kindermans, P Ramachandran, Q Le [TU Berlin & Google Brain] (2018) [arXiv:1808.01974](https://arxiv.org/abs/1808.01974)

**Smooth Loss Functions for Deep Top-k Classification**. L Berrada, A Zisserman, M. P Kumar [University of Oxford] (2018) [arXiv:1802.07595](https://arxiv.org/abs/1802.07595) [Github](https://github.com/oval-group/smooth-topk)

- Batch Normalization
  - **Mean-field Analysis of Batch Normalization**. M Wei, J Stokes, D J Schwab [Northwestern University & Tunnel & The City University of New York] (2019)  [arXiv:1903.02606](https://arxiv.org/abs/1903.02606) [OpenReview.net](https://openreview.net/forum?id=B1eSg3C9Ym)
  - **Generalized Batch Normalization: Towards Accelerating Deep Neural Networks**. X Yuan, Z Feng, M Norton, X Li [University of Florida & Naval Postgraduate School] (2018) [arXiv:1812.03271](https://arxiv.org/abs/1812.03271)

  - **How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)**. S Santurkar, D Tsipras, A Ilyas, A Madry [MIT] (2018) [arXiv:1805.11604](https://arxiv.org/abs/1805.11604) [YouTube](https://www.youtube.com/watch?v=ZOabsYbmBRM) [Reddit](https://www.reddit.com/r/MachineLearning/comments/8n4eot/r_how_does_batch_normalization_help_optimization/) [Notes from SALU](https://shaoanlu.wordpress.com/2018/07/12/notes-for-paper-how-does-batch-normalization-help-optimization-no-it-is-not-about-internal-covariate-shift/)
- Fenchel-Young Losses

  - **Learning with Fenchel-Young Losses**. M Blondel, A F. T. Martins, V Niculae [NTT Communication Science Laboratories & Unbabel & Instituto de Telecomunica¬∏coes & Instituto de Telecomunica¬∏coes] (2019) [arXiv:1901.02324](https://arxiv.org/abs/1901.02324)

  - **Learning Classifiers with Fenchel-Young Losses: Generalized Entropies, Margins, and Algorithms**. M Blondel, A F. T. Martins, V Niculae [NTT Communication Science Laboratories & Instituto de Telecomunicacoes] (2018) [arXiv:1805.09717](https://arxiv.org/abs/1805.09717) [Github](https://github.com/mblondel/fenchel-young-losses)




## „ÄΩÔ∏è ODE & PDE

**Data Driven Governing Equations Approximation Using Deep Neural Networks**. T Qin, K Wu, D Xiu [The Ohio State University] (2018) [arXiv:1811.05537](https://arxiv.org/abs/1811.05537)

**Neural Ordinary Differential Equations**. Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud [University of Toronto, Canada] (NeurIPS 2018 | best paper) [arXiv:1806.07366](https://arxiv.org/abs/1806.07366) [Github](https://github.com/rtqichen/torchdiffeq) [Blog](https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/) [Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s/ZEIsyV-0aTvYn6K8GyANPA)(„ÄêÁ°¨Ê†∏NeruIPS 2018ÊúÄ‰Ω≥ËÆ∫ÊñáÔºå‰∏Ä‰∏™Á•ûÁªè‰∫ÜÁöÑÂ∏∏ÂæÆÂàÜÊñπÁ®ã„ÄëËøôÊòØ‰∏ÄÁØáÁ•ûÂ•áÁöÑËÆ∫ÊñáÔºå‰ª•Ââç‰∏ÄÂ±Ç‰∏ÄÂ±ÇÂè†Âä†ÁöÑÁ•ûÁªèÁΩëÁªú‰ºº‰πéÁ™ÅÁÑ∂ÂèòÂæóËøûÁª≠‰∫ÜÔºåÂèçÂêë‰º†Êí≠‰πü‰ºº‰πé‰∏çÂÜçÈúÄË¶Å‰∏ÄÁÇπ‰∏ÄÁÇπÂæÄÂâç‰º†„ÄÅ‰∏ÄÂ±Ç‰∏ÄÂ±ÇÊõ¥Êñ∞ÂèÇÊï∞‰∫Ü„ÄÇ ) 

„Äê„ÄäNeural Ordinary Differential Equations„ÄãËÆ∫ÊñáËß£ËØª„Äë„ÄäPaper Summary: Neural Ordinary Differential Equations„Äãby Branislav Holl√§nder http://t.cn/EGPEh8y 

summary by Adrian Colyer http://t.cn/EqANCZ0

„ÄêÁ•ûÁªèÂ∏∏ÂæÆÂàÜÊñπÁ®ãÁöÑPyTorchÂÆûÁé∞‰∏éÂàÜÊûê(Jupyter Notebooks)„Äë‚ÄôNeural ODEs - Jupyter notebook with Pytorch implementation and investigation of Neural Ordinary Differential Equations' by Mikhail Surtsukov GitHub: http://t.cn/Ef3Qkw4 

„ÄêÁ•ûÁªèÂ∏∏ÂæÆÂàÜÊñπÁ®ã‰∏éÂØπÊäóÊîªÂáª„Äë„ÄäNeural Ordinary Differential Equations and Adversarial Attacks„Äãby Rajat Vadiraj Dwaraknath http://t.cn/EqW9Anb 

„ÄêÁ•ûÁªèÂæÆÂàÜÊñπÁ®ã„Äë„ÄäNeural Differential Equations - YouTube„Äãby Siraj Raval http://t.cn/EqWCSBN 

**Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations**. M Raissi [Brown University] (2018) [arXiv:1804.07010](https://arxiv.org/abs/1804.07010)





---



## :atom_symbol: Physics Related

**Deep learning approach based on dimensionality reduction for designing electromagnetic nanostructures**. Yashar Kiarashinejad, Sajjad Abdollahramezani, Ali Adibi [Georgia Institute of Technology] (2019) [arXiv:1902.03865](https://arxiv.org/abs/1902.03865)

**The Calabi-Yau Landscape: from Geometry, to Physics, to Machine-Learning**. Y He (2018) [arXiv:1812.02893](https://arxiv.org/abs/1812.02893)

**DeepSphere: Efficient spherical Convolutional Neural Network with HEALPix sampling for cosmological applications**. N Perraudin, M Defferrard, T Kacprzak, R Sgier [aSwiss Data Science Center (SDSC) & EPFL & ETH Zurich] (2018) [arXiv:1810.12186](https://arxiv.org/abs/1810.12186)

**Toward an AI Physicist for Unsupervised Learning**. T Wu, M Tegmark [MIT] (2018) [arXiv:1810.10525](https://arxiv.org/abs/1810.10525)

**Using Machine Learning to Predict the Evolution of Physics Research**. W Liu, S Saganowski, P Kazienko, S A Cheong [Nanyang Technological University & Wroc≈Çaw University of Science and Technology] (2018) [arXiv:1810.12116](https://arxiv.org/abs/1810.12116)

**hep-th**. Y He, V Jejjala, B D. Nelson [University of London & Nankai University & Northeastern University] (2018) [arXiv:1807.00735](https://arxiv.org/abs/1807.00735) [comment]

**Deep Fluids: A Generative Network for Parameterized Fluid Simulations**. B Kim, VC Azevedo, N Thuerey, T Kim, M Gross‚Ä¶ [ETH Zurich & Technical University of Munich & Pixar Animation Studios] (2019) [arXiv:1806.02071](https://arxiv.org/abs/1806.02071) [Blog](http://www.byungsoo.me/project/deep-fluids/)

**Physics-guided Neural Networks (PGNNs)**. Anuj Karpatne, William Watkins, Jordan Read, Vipin Kumar [University of Minnesota] (2017) [arXiv:1710.11431](https://arxiv.org/abs/1710.11431)



## :books: Review, Survey & Lecture Notes

**A State-of-the-Art Survey on Deep Learning Theory and Architectures**. [PDF](https://www.mdpi.com/2079-9292/8/3/292)

**Gradient Descent based Optimization Algorithms for Deep Learning Models Training**. J Zhang [Information Fusion and Mining Laboratory] (2019) [arXiv:1903.03614](https://arxiv.org/abs/1903.03614)

**Deep Learning for Image Super-resolution: A Survey**. Z Wang, J Chen, S C.H. Hoi [Singapore Management University & South China University of Technology] (2019) [arXiv:1902.06068](https://arxiv.org/abs/1902.06068) [‰∏ìÁü•](https://mp.weixin.qq.com/s/E1RxsUjQN2ufEP4k3OZyCA)

**Word Embeddings: A Survey**. F Almeida, G Xex√©o [Federal University of Rio de Janeiro] (2019) [arXiv:1901.09069](https://arxiv.org/abs/1901.09069)

**Fitting A Mixture Distribution to Data: Tutorial**. B Ghojogh, A Ghojogh, M Crowley, F Karray [University of Waterloo & Shiraz University of Technology] (2019) [arXiv:1901.06708](https://arxiv.org/abs/1901.06708)

**A Survey of the Recent Architectures of Deep Convolutional Neural Networks**. A Khan, A Sohail, U Zahoora, A S Qureshi [PIEAS] (2019) [arXiv:1901.06032](https://arxiv.org/abs/1901.06032) [Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650756087&idx=4&sn=39c71d8bd5d210badad2de235d7f3cae)

**Artificial Neural Networks**. B. Mehlig [University of Gothenburg, Sweden] (2019) [arXiv:1901.05639](https://arxiv.org/abs/1901.05639) [Reddit](https://www.reddit.com/r/MachineLearning/comments/ah9afa/190105639_artificial_neural_networks/)

**Optimization Models for Machine Learning: A Survey**. C Gambella, B Ghaddar, J Naoum-Sawaya [IBM Research Ireland & University of Western Ontario] (2019) [arXiv:1901.05331](https://arxiv.org/abs/1901.05331)

**Deep Learning for Anomaly Detection: A Survey**. R Chalapathy, S Chawla [University of Sydney & Qatar Computing Research Institute (QCRI)] (2019) [arXiv:1901.03407](https://arxiv.org/abs/1901.03407)

**Revisiting Self-Supervised Visual Representation Learning**. Alexander Kolesnikov, Xiaohua Zhai, Lucas Beyer [Google Brain] (2019) [arXiv:1901.09005](https://arxiv.org/abs/1901.09005) [Github](https://github.com/google/revisiting-self-supervised) [Reddit](https://www.reddit.com/r/MachineLearning/comments/altr5e/r_revisiting_selfsupervised_visual_representation/)

**How Generative Adversarial Networks and Their Variants Work: An Overview**. Yongjun Hong, Uiwon Hwang, Jaeyoon Yoo, Sungroh Yoon [Seoul National University] (2017, 2018v9) [arXiv:1711.05914](https://arxiv.org/abs/1711.05914)

**A Survey on Multi-output Learning**. D Xu, Y Shi, I W. Tsang, Y Ong, C Gong, X Shen [ University of Technology Sydney & Nanyang Technological University] (2019) [arXiv:1901.00248](https://arxiv.org/abs/1901.00248)

**Analysis Methods in Neural Language Processing: A Survey**. Y Belinkov, J Glass [MIT] (2018) [arXiv:1812.08951](https://arxiv.org/abs/1812.08951) [Github](https://github.com/boknilev/nlp-analysis-methods) [Blog](https://boknilev.github.io/nlp-analysis-methods/)

**Neural Approaches to Conversational AI**. J Gao, M Galley, L Li [Microsoft Research & Google Brain] (2018) [arXiv:1809.08267](https://arxiv.org/abs/1809.08267)

**Recent Advances in Autoencoder-Based Representation Learning**. M Tschannen, O Bachem, M Lucic [ETH Zurich & Google AI] (2018) [arXiv:1812.05069](https://arxiv.org/abs/1812.05069)

**Learning From Positive and Unlabeled Data: A Survey**. J Bekker, J Davis [KU Leuven] (2018) [arXiv:1811.04820](https://arxiv.org/abs/1811.04820)

**Analyzing biological and artificial neural networks: challenges with opportunities for synergy?**. David G.T. Barrett, Ari S. Morcos, Jakob H. Macke [DeepMind; Technical University of Munich, Germany] (2018) [arXiv:1810.13373](https://arxiv.org/abs/1810.13373)

**Model Selection Techniques -- An Overview**. J Ding, V Tarokh, Y Yang [University of Minnesota & Duke University] (2018) [arXiv:1810.09583](https://arxiv.org/abs/1810.09583)

**Deep Learning with the Random Neural Network and its Applications**. Y Yin [Imperial College] (2018) [arXiv:1810.08653](https://arxiv.org/abs/1810.08653)

**The Frontiers of Fairness in Machine Learning**. A Chouldechova, A Roth [CMU & University of Pennsylvania] (2018) [arXiv:1810.08810](https://arxiv.org/abs/1810.08810)

**Applications of Deep Reinforcement Learning in Communications and Networking: A Survey**. N C Luong, D T Hoang, S Gong, D Niyato, P Wang, Y Liang, D I Kim [Nanyang Technological University & University of Technology Sydney & Chinese Academy of Sciences] (2018) [arXiv:1810.07862](https://arxiv.org/abs/1810.07862)

**A Survey on Deep Learning: Algorithms, Techniques, and Applications**. Pouyanfar S, Sadiq S, Yan Y, et al [ACM Computing Surveys (CSUR)] (2018) ([pdf](https://dl.acm.org/citation.cfm?id=3234150)) ([‰∏ìÁü•](https://mp.weixin.qq.com/s/AQrgvjFPXUpqfqQQgOFN9A))

**A Tale of Three Probabilistic Families: Discriminative, Descriptive and Generative Models**. Y N Wu, R Gao, T Han, S Zhu [UCLA] (2018) [arXiv:1810.04261](https://arxiv.org/abs/1810.04261)

**Deep learning for time series classification: a review**. H I Fawaz, G Forestier, J Weber, L Idoumghar, P Muller [Universit√© Haute Alsace] (2018) [arXiv:1809.04356](https://arxiv.org/abs/1809.04356)

**A Survey on Deep Transfer Learning**. C Tan, F Sun, T Kong, W Zhang, C Yang, C Liu [Tsinghua University] (2018) [arXiv:1808.01974](https://arxiv.org/abs/1808.01974)

**Generalization Error in Deep Learning**. D Jakubovitz, R Giryes, M R. D. Rodrigues [Tel Aviv University & University College London] (2018) [arXiv:1808.01174](https://arxiv.org/abs/1808.01174)

**How convolutional neural network see the world - A survey of convolutional neural network visualization methods**. Z Qin, F Yu, C Liu, X Chen [George Mason University & Clarkson University] (2018) [arXiv:1804.11191](https://arxiv.org/abs/1804.11191)

**An Introduction to Image Synthesis with Generative Adversarial Nets**. He Huang, Philip S. Yu, Changhu Wang [University of Illinois at Chicago & ByteDance AI Lab] (2019) [arXiv:1803.04469](https://arxiv.org/abs/1803.04469)

**Visual Interpretability for Deep Learning: a Survey**. Quanshi Zhang, Song-Chun Zhu [] (2018) [arXiv:1802.00614](https://arxiv.org/abs/1802.00614)

**Deep Learning for Time-Series Analysis**. John Gamboa [University of Kaiserslautern, Germany] (2017) [arXiv:1701.01887](https://arxiv.org/abs/1701.01887)

**Deep Learning in Neural Networks: An Overview**. Ju Ãàrgen Schmidhuber [University of Lugano & SUPSI, Switzerland] (2014) [arXiv:1404.7828](https://arxiv.org/abs/1404.7828)

**Curriculum learning**. Y Bengio, J Louradour, R Collobert, J Weston [Montreal, Fabert, Princeton] (2009) [PDF](https://dl.acm.org/ft_gateway.cfm?id=1553380&ftid=628555&dwn=1&CFID=26059340&CFTOKEN=33e02b66c0c6c7d5-638CF26B-B60E-C2DB-D77FEA4F75987BD4)

- Graph Neural Networks

  **A Comprehensive Survey on Graph Neural Networks**. Z Wu, S Pan, F Chen, G Long, C Zhang, P S. Yu [University of Technology Sydney & Monash University & University of Illinois at Chicago] (2019) [arXiv:1901.00596](https://arxiv.org/abs/1901.00596)

  **Graph Neural Networks: A Review of Methods and Applications**. J Zhou, G Cui, Z Zhang, C Yang, Z Liu, M Sun [Tsinghua University] (2018) [arXiv:1812.08434](https://arxiv.org/abs/1812.08434)

  **Deep Learning on Graphs: A Survey**. Z Zhang, P Cui, W Zhu [Tsinghua University] (2018) [arXiv:1812.04202](https://arxiv.org/abs/1812.04202)



## :framed_picture: Figure Design & Dimension Reduction

**Deep Learning Multidimensional Projections**. Mateus Espadoto, Nina S. T. Hirata and Alexandru C. Telea (2019) [arXiv:1902.07958](https://arxiv.org/abs/1902.07958)

**CFUN: Combining Faster R-CNN and U-net Network for Efficient Whole Heart Segmentation**. Z Xu, Z Wu, J Feng [Tsinghua University] (2018) [arXiv:1812.04914](https://arxiv.org/abs/1812.04914) [GitHub](https://github.com/Wuziyi616/CFUN)

**Deep Paper Gestalt**. J Huang [Virginia Tech] (2018) [arXiv:1812.08775](https://arxiv.org/abs/1812.08775) [GitHub](https://github.com/vt-vl-lab/paper-gestalt) [YouTube](https://www.youtube.com/watch?v=yQLsZLf02yg) [Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650754320&idx=1&sn=d6691ecd6adb98fa3c29986cf1efeebc&chksm=871a896eb06d00780f96e27b3ce3ee321c45ec21d30107f3e3778b5f674e172855a29fbcc70f&token=576114296&lang=zh_CN#rd)

**A Tutorial on Distance Metric Learning: Mathematical Foundations, Algorithms and Software**. J L Su√°rez, S Garc√≠a, F Herrera [University of Granada] (2018) [arXiv:1812.05944](https://arxiv.org/abs/1812.05944)

**Improving Generalization for Abstract Reasoning Tasks Using Disentangled Feature Representations**. X Steenbrugge, S Leroux, T Verbelen, B Dhoedt [Ghent University] (2018) [arXiv:1811.04784](https://arxiv.org/abs/1811.04784)

**UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction**. Leland McInnes and John Healy [Tutte Institute for Mathematics and Computing] (2018) [arXiv:1802.03426](https://arxiv.org/abs/1802.03426) [Github](https://github.com/lmcinnes/umap)







---

> Need to be reviewed.....

**Entropic GANs meet VAEs: A Statistical Approach to Compute Sample Likelihoods in GANs**. Y Balaji, H Hassani, R Chellappa, S Feizi [University of Maryland & University of Pennsylvania] (2018) [arXiv:1810.04147](https://arxiv.org/abs/1810.04147) [comment]

**Analyzing the Noise Robustness of Deep Neural Networks**. M Liu, S Liu, H Su, K Cao, J Zhu [Tsinghua University] (2018) [arXiv:1810.03913](https://arxiv.org/abs/1810.03913) [comment]

**Deep convolutional Gaussian processes**. K Blomqvist, S Kaski, M Heinonen [Aalto university] (2018) [arXiv:1810.03052](https://arxiv.org/abs/1810.03052) [GitHub](https://github.com/kekeblom/DeepCGP) [comment]

**Learning with Random Learning Rates**. L Blier, P Wolinski, Y Ollivier [Facebook AI Research & Universite Paris Sud] (2018) [arXiv:1810.01322](https://arxiv.org/abs/1810.01322) [Github](https://github.com/leonardblier/alrao) [Blog](https://leonardblier.github.io/alrao/) [comment]

**Interpreting Adversarial Robustness: A View from Decision Surface in Input Space**. F Yu, C Liu, Y Wang, X Chen [George Mason University & Clarkson University & Northeastern University] (2018) [arXiv:1810.00144](https://arxiv.org/abs/1810.00144) [comment]

**Spurious samples in deep generative models: bug or feature?**. B K√©gl, M Cherti, A Kazak√ßƒ± [CNRS/Universite Paris-Saclay & PSL Research University] (2018) [arXiv:1810.01876](https://arxiv.org/abs/1810.01876) [comment]

**Inhibited Softmax for Uncertainty Estimation in Neural Networks**. M Mo≈ºejko, M Susik, R Karczewski [Sigmoidal] (2018)  [arXiv:1810.01861](https://arxiv.org/abs/1810.01861) [GitHub](https://github.com/MSusik/Inhibited-softmax) [comment]

**Deep processing of structured data**. ≈Å Maziarka, M ≈ömieja, A Nowak, J Tabor, ≈Å Struski, P Spurek [Jagiellonian University] (2018) [arXiv:1810.01989](https://arxiv.org/abs/1810.01989) [comment]

**Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow**. Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, Sergey Levine [University of California, Berkeley] (2018) [arXiv:1810.00821](https://arxiv.org/abs/1810.00821)

**Taming VAEs**. D J Rezende, F Viola [DeepMind] (2018) [arXiv:1810.00597](https://arxiv.org/abs/1810.00597) [comment]

**Adversarial Attacks and Defences: A Survey**. A Chakraborty, M Alam, V Dey, A Chattopadhyay, D Mukhopadhyay [Indian Institute of Technology & The Ohio State University & Nanyang Technological University] (2018) [arXiv:1810.00069](https://arxiv.org/abs/1810.00069) [comment]

**Over-Optimization of Academic Publishing Metrics: Observing Goodhart's Law in Action**. M Fire, C Guestrin [University of Washington] (2018) [arXiv:1809.07841](https://arxiv.org/abs/1809.07841) [comment]

**On the loss landscape of a class of deep neural networks with no bad local valleys**. Q Nguyen, M C Mukkamala, M Hein [Saarland University & University of T√ºbingen] (2018) [arXiv:1809.10749](https://arxiv.org/abs/1809.10749) [comment]

**Conditional WaveGAN**. Chae Young Lee, Anoop Toffy, Gue Jun Jung, Woo-Jin Han (2018) [GitHub](https://github.com/acheketa/cwavegan) [arXiv:1809.10636](https://arxiv.org/abs/1809.10636)

**An analytic theory of generalization dynamics and transfer learning in deep linear networks**. A K. Lampinen, S Ganguli [Stanford University] (2018) [arXiv:1809.10374](https://arxiv.org/abs/1809.10374) [comment]

**Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning**. N Frazier-Logue, S J Hanson [Rutgers University] (2018) [arXiv:1808.03578](https://arxiv.org/abs/1808.03578) [comment]

**Grassmannian Learning: Embedding Geometry Awareness in Shallow and Deep Learning**. J Zhang, G Zhu, R W. H Jr., a K Huang [The University of Hong Kong] (2018) [arXiv:1808.02229](https://arxiv.org/abs/1808.02229) [comment]

**Is Robustness the Cost of Accuracy? -- A Comprehensive Study on the Robustness of 18 Deep Image Classification Models**. D Su, H Zhang... [IBM Research & University of California, Davis & MIT] (2018) [arXiv:1808.01688](https://arxiv.org/abs/1808.01688) [GitHub](https://github.com/huanzhang12/Adversarial_Survey) [comment]

**Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining**. Xia Li, Jianlong Wu, Zhouchen Lin, Hong Liu, Hongbin Zha. [Peking University] (2018) [arXiv:1807.05698](https://arxiv.org/abs/1807.05698) [GitHub](https://github.com/XiaLiPKU/RESCAN) [comment]

**Toward Convolutional Blind Denoising of Real Photographs**. S Guo, Z Yan, K Zhang, W Zuo, L Zhang [Harbin Institute of Technology & The Hong Kong Polytechnic University] (2018) [arXiv:1807.04686](https://arxiv.org/abs/1807.04686) [Github](https://github.com/GuoShi28/CBDNet) [comment]

**Seamless Nudity Censorship: an Image-to-Image Translation Approach based on Adversarial Training**. MD More, DM Souza, J Wehrmann, RC Barros (2018) [ResearchGate](https://www.researchgate.net/profile/Jonatas_Wehrmann/publication/325746502_Seamless_Nudity_Censorship_an_Image-to-Image_Translation_Approach_based_on_Adversarial_Training/links/5b2c7950aca2720785d66732/Seamless-Nudity-Censorship-an-Image-to-Image-Translation-Approach-based-on-Adversarial-Training.pdf) [comment]

**Classification and Geometry of General Perceptual Manifolds**. SY Chung, DD Lee, H Sompolinsky [Harvard University] (2018) [PRX](https://journals.aps.org/prx/abstract/10.1103/PhysRevX.8.031003) [comment]

**The GAN Landscape: Losses, Architectures, Regularization, and Normalization**. K Kurach, M Lucic, X Zhai, M Michalski, S Gelly [Google Brain] (2018) [arXiv:1807.04720](https://arxiv.org/abs/1807.04720) [Github](https://github.com/google/compare_gan) [comment]

**Troubling Trends in Machine Learning Scholarship**. Z C. Lipton, J Steinhardt [Stanford University] (2018) [arXiv:1807.03341](https://arxiv.org/abs/1807.03341) [comment]

**On the Spectral Bias of Deep Neural Networks**. N Rahaman, D Arpit, A Baratin, F Draxler, M Lin, F A. Hamprecht, Y Bengio, A Courville [Heidelberg University & MILA] (2018) [arXiv:1806.08734](https://arxiv.org/abs/1806.08734) [comment]

**Opening the black box of deep learning**. D Lei, X Chen, J Zhao [Shanghai University] (2018) [arXiv:1805.08355](https://arxiv.org/abs/1805.08355) [comment]

**Foundations of Sequence-to-Sequence Modeling for Time Series**. V Kuznetsov, Z Mariet [Google Research & MIT] (2018) [arXiv:1805.03714](https://arxiv.org/abs/1805.03714)







---

[ËøîÂõûÂà∞È¶ñÈ°µ](../index.html) | [ËøîÂõûÂà∞È°∂ÈÉ®](./APaperADay.html)


<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://iphysresearch.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

<br>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
<br>

<script type="application/json" class="js-hypothesis-config">
  {
    "openSidebar": false,
    "showHighlights": true,
    "theme": classic,
    "enableExperimentalNewNoteButton": true
  }
</script>
<script async src="https://hypothes.is/embed.js"></script>



