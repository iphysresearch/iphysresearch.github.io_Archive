<attachment contenteditable="false" data-atts="%5B%5D" data-aid=".atts-f3d9a49f-bcd4-4e60-be97-f09f59c700a9"></attachment><p class="ql-align-center">IPhysResearch<a href="http://iphysresearch.github.io" target="_blank"><img src="https://i.loli.net/2018/07/11/5b44e3a6a798a.jpg" alt="Background"></a></p><h1 class="ql-align-center">ğŸº Teaching is Learning, Writing is Thinking ğŸº</h1><hr><p class="ql-align-center"><strong>Physics | Gravitational Waves | Machine Learning | Deep Learning</strong></p><p class="ql-align-center"><a href="http://iphysresearch.github.io" target="_blank"><img src="https://img.shields.io/badge/Update-2018.8.17-green.svg?style=plastic" alt="Update"></a><a href="https://github.com/iphysresearch" target="_blank"><img src="https://img.shields.io/github/followers/iphysresearch.svg?style=social&amp;label=Follow" alt="GITHUB"></a><a href="http://weibo.com/IPhysresearch" target="_blank"><img src="https://img.shields.io/badge/Weibo-@iPHYSresearch-blue.svg?style=plastic" alt="Tweet"></a><a href="https://twitter.com/Herb_hewang" target="_blank"><img src="https://img.shields.io/twitter/url/https/github.com/iphysresearch/iphysresearch.github.io.svg?style=social" alt="Tweet"></a></p><p>&nbsp;</p><p><br></p><p><a href="about:blank" target="_blank">Welcomeï¼</a></p><p><a href="about:blank" target="_blank">About</a></p><p><a href="about:blank" target="_blank">How to comment</a></p><p><a href="about:blank" target="_blank">My Learning Notes on ...</a></p><p><a href="about:blank" target="_blank">Data Science Courses</a></p><p><a href="about:blank" target="_blank">CS231n</a></p><p><a href="about:blank" target="_blank">Books</a></p><p><a href="about:blank" target="_blank">Data Analysis in Gravitational Wave Detection</a></p><p><a href="about:blank" target="_blank">Paper Summary</a></p><p><a href="about:blank" target="_blank">ğŸŒˆ GW astronomy</a></p><p><a href="about:blank" target="_blank">ğŸ„ Survey &amp; Review</a></p><p><a href="about:blank" target="_blank">ğŸƒ ImageNet Evolution</a></p><p><a href="about:blank" target="_blank">ğŸ¥… Model</a></p><p><a href="about:blank" target="_blank">â›· Optimization</a></p><p><a href="about:blank" target="_blank">My Blog Posts</a></p><p><a href="about:blank" target="_blank">My Github Projects</a></p><p>&nbsp;</p><p><br></p><h1>Welcomeï¼</h1><h2>About</h2><p>Thanks for visitingï¼</p><p>I'm a PhD candidate majoring in theoretical physics. I love to share knowledge and have a keen passion for scientific research on data analysis of <strong><em>gravitational-wave</em></strong>(GW) detection and <strong><em>deep learning</em></strong> (DL) technologies.</p><p>Most of blog posts and notes here are written in <strong>Chinese</strong> and any future updates for <em>completion</em>. After some years of study, I wish I could accumulate a sufficient body of knowledge and achieve a view of my own on. Thus,&nbsp;as <em>S.&nbsp;Chandrasekhar</em>&nbsp;notes, "I have the urge to present my point of view <em>ab initio</em>, in a coherent account with order, form, and structure."</p><blockquote>"My scientific work has followed a certain pattern motivated, principally, by <em>a quest after perspectives</em>"â€”â€” S. Chandrasekhar</blockquote><p>This site is currently <strong>under construction</strong> and I will make updates weekly and look forward to resuming blog posts in the fall.</p><p><br></p><h2>How to comment</h2><p>With use of the&nbsp;<a href="https://hypothes.is/" target="_blank">hypothes.is</a>&nbsp;extension (right-sided), you can highlight, annote any comments and discuss these notes inline<em>at any pages</em>and <em>posts</em>.</p><p><em>Please Feel Free</em>&nbsp;to Let Me Know and <em>Share</em>&nbsp;it Here.</p><p>&nbsp;</p><p><br></p><hr><h1>My Learning Notes on ...</h1><blockquote>â€œ<em>Men Learn While They Teach</em>â€ â€”â€” Seneca.</blockquote><h2>Data Science Courses</h2><h3>CS231n</h3><ul><li>From lecture video and slices</li><li class="ql-indent-1"><a href="about:blank" target="_blank">Lecture 1. Computer vision overview &amp; Historical context</a></li><li class="ql-indent-1"><a href="about:blank" target="_blank">Lecture 2. Image Classification &amp; K-nearest neighbor</a></li><li class="ql-indent-1"><a href="about:blank" target="_blank">Lecture 3. Loss Functions and Optimization</a></li><li class="ql-indent-1">Lecture 4. Introduction to Neural Networks</li><li class="ql-indent-1">Lecture 5. Convolutional Neural Networks</li><li class="ql-indent-1">Lecture 6. Training Neural Networks, part I</li><li class="ql-indent-1">Lecture 7. Training Neural Networks, part II</li><li class="ql-indent-1">Lecture 8. Deep Learning Hardware and Software</li><li class="ql-indent-1">Lecture 9. CNN Architectures</li><li class="ql-indent-1">Lecture 10. Recurrent Neural Networks</li><li class="ql-indent-1">Lecture 11. Detection and Segmentation</li><li class="ql-indent-1">Lecture 12. Generative Models</li><li class="ql-indent-1">Lecture 13. Visualizing and Understanding</li><li class="ql-indent-1">Lecture 14. Deep Reinforcement Learning</li><li class="ql-indent-1">Lecture 15.</li><li>From course notes</li><li><br></li><li>è®²ä¹‰ç¬”è®°ç®€ä»‹<a href="about:blank" target="_blank">å›¾åƒåˆ†ç±»</a>L1/L2 distances, hyperparameter search, cross-validation<a href="about:blank" target="_blank">çº¿æ€§åˆ†ç±»</a>parameteric approach, bias trick, hinge loss, cross-entropy loss, L2 regularization, web demo<a href="about:blank" target="_blank">æœ€ä¼˜åŒ–</a>optimization landscapes, local search, learning rate, analytic/numerical gradient&nbsp;&nbsp;</li><li>Others</li><li class="ql-indent-1"><a href="about:blank" target="_blank">ä¸€æ®µå…³äºç¥ç»ç½‘ç»œçš„æ•…äº‹</a>ï¼ˆ<strong>Original</strong>ï¼Œ30671å­— + å¤šå›¾ï¼‰</li></ul><p>&nbsp;</p><p><br></p><h2>Books</h2><ul><li><a href="about:blank" target="_blank">Python åŸºç¡€æ•™ç¨‹ï¼ˆç¬¬3ç‰ˆï¼‰</a>ï¼ˆ<strong>Annotations</strong>ï¼‰</li></ul><p>&nbsp;</p><p><br></p><h2>Data Analysis in Gravitational Wave Detection</h2><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p><br></p><hr><h1>Paper Summary</h1><blockquote><strong>Please note that these posts are for my future self to review the materials on these papers without reading them all over again.</strong> (Inspired by <a href="https://jaedukseo.me" target="_blank">Jae Duk Seo </a> and also refering to <a href="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap" target="_blank">Deep Learning Papers Reading Roadmap</a> &amp; <a href="https://github.com/terryum/awesome-deep-learning-papers" target="_blank">Awesome - Most Cited Deep Learning Papers</a>)</blockquote><p>&nbsp;</p><p>I have keys but no locks. I have space but no room. You can enter but can't leave. What am I?</p><p>A keyboard.</p><p>&nbsp;</p><p><br></p><h2>ğŸŒˆ GW astronomy</h2><ul><li><br></li></ul><p>&nbsp;</p><p><br></p><h2>ğŸ„ Survey &amp; Review</h2><ul><li>[Paper Summary] LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. "<strong>Deep learning</strong>." <strong>(Three Giants' Survey)</strong></li></ul><h2>ğŸƒ ImageNet Evolution</h2><blockquote>Deep Learning broke out from here</blockquote><ul><li>[Paper Summary] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "<strong>Imagenet classification with deep convolutional neural networks</strong>." (2012). <strong>(AlexNet, Deep Learning Breakthrough)</strong></li><li>[Paper Summary] Simonyan, Karen, and Andrew Zisserman. "<strong>Very deep convolutional networks for large-scale image recognition</strong>." (2014).<strong>(VGGNet,Neural Networks become very deep!)</strong></li><li>[Paper Summary] Szegedy, Christian, et al. "<strong>Going deeper with convolutions</strong>." (2015).<strong>(GoogLeNet)</strong></li><li>[Paper Summary] He, Kaiming, et al. "<strong>Deep residual learning for image recognition</strong>." (2015).<strong>(ResNet,Very very deep networks, CVPR best paper)</strong></li></ul><h2>ğŸ¥… Model</h2><ul><li>[Paper Summary] Hinton, Geoffrey E., et al. "<strong>Improving neural networks by preventing co-adaptation of feature detectors</strong>." (2012). <strong>(Dropout)</strong></li><li>[Paper Summary] Srivastava, Nitish, et al. "<strong>Dropout: a simple way to prevent neural networks from overfitting</strong>." (2014)</li><li>[Paper Summary] Ioffe, Sergey, and Christian Szegedy. "<strong>Batch normalization: Accelerating deep network training by reducing internal covariate shift</strong>." (2015).<strong>(An outstanding Work in 2015)</strong></li><li>[Paper Summary] Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. "<strong>Layer normalization</strong>." (2016).<strong>(Update of Batch Normalization)</strong></li><li>[Paper Summary] Courbariaux, Matthieu, et al. "<strong>Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 orâˆ’1</strong>." <strong>(New Model,Fast)</strong></li><li>[Paper Summary] Jaderberg, Max, et al. "<strong>Decoupled neural interfaces using synthetic gradients</strong>." (2016). <strong>(Innovation of Training Method,Amazing Work)</strong></li><li>[Paper Summary] Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. "Net2net: Accelerating learning via knowledge transfer."(2015).<strong>(Modify previously trained network to reduce training epochs)</strong></li><li>[Paper Summary] Wei, Tao, et al. "<strong>Network Morphism.</strong>" (2016). <strong>(Modify previously trained network to reduce training epochs)</strong></li></ul><p>&nbsp;</p><p><br></p><h2>â›· Optimization</h2><ul><li>[Paper Summary] Sutskever, Ilya, et al. "<strong>On the importance of initialization and momentum in deep learning</strong>." (2013) <strong>(Momentum optimizer)</strong></li><li>[Paper Summary] Kingma, Diederik, and Jimmy Ba. "<strong>Adam: A method for stochastic optimization</strong>." (2014). <strong>(Maybe used most often currently)</strong></li><li>[Paper Summary] Andrychowicz, Marcin, et al. "<strong>Learning to learn by gradient descent by gradient descent</strong>." (2016).<strong>(Neural Optimizer,Amazing Work)</strong></li><li>[Paper Summary] Han, Song, Huizi Mao, and William J. Dally. "<strong>Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding</strong>." (2015). <strong>(ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup)</strong></li><li>[Paper Summary] Iandola, Forrest N., et al. "<strong>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 1MB model size</strong>." (2016).<strong>(Also a new direction to optimize NN,DeePhi Tech Startup)</strong></li></ul><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p><br></p><hr><h1>My Blog Posts</h1><ul><li><a href="about:blank" target="_blank">2018å¹´ä¸ªäººè®¡åˆ’å’Œç›®æ ‡</a></li><li><a href="about:blank" target="_blank">æ•°æ®ç§‘å­¦å…¥é—¨ä¹‹æˆ‘è°ˆ(2017)</a></li><li><a href="about:blank" target="_blank">S_Dbw èšç±»è¯„ä¼°æŒ‡æ ‡ï¼ˆä»£ç å…¨è§£æï¼‰</a></li><li><a href="about:blank" target="_blank">Training Neural Networks with Mixed Precision: Theory and Practice</a></li></ul><p>&nbsp;</p><p><br></p><hr><h1>My Github Projects</h1><h4><a href="https://github.com/iphysresearch/DataSciComp/" target="_blank">DataSciComp</a> <a href="https://github.com/iphysresearch/DataSciComp/watchers" target="_blank"><img src="https://img.shields.io/github/watchers/iphysresearch/DataSciComp.svg?style=social" alt="Github Watch Badge"></a><a href="https://github.com/iphysresearch/DataSciComp/stargazers" target="_blank"><img src="https://img.shields.io/github/stars/iphysresearch/DataSciComp.svg?style=social" alt="Github Star Badge"></a></h4><blockquote>A collection of popular Data Science Competitions</blockquote><h4><a href="https://github.com/iphysresearch/TOP250movie_douban" target="_blank">TOP250movie_douban</a> <a href="https://github.com/iphysresearch/TOP250movie_douban/watchers" target="_blank"><img src="https://img.shields.io/github/watchers/iphysresearch/TOP250movie_douban.svg?style=social" alt="Github Watch Badge"></a><a href="https://github.com/iphysresearch/TOP250movie_douban/stargazers" target="_blank"><img src="https://img.shields.io/github/stars/iphysresearch/TOP250movie_douban.svg?style=social" alt="Github Star Badge"></a></h4><blockquote>TOP250è±†ç“£ç”µå½±çŸ­è¯„ï¼šScrapy çˆ¬è™«+æ•°æ®æ¸…ç†/åˆ†æ+æ„å»ºä¸­æ–‡æ–‡æœ¬æƒ…æ„Ÿåˆ†ææ¨¡å‹</blockquote><h4><a href="https://github.com/iphysresearch/S_Dbw_validity_index" target="_blank">S_Dbw_validity_index</a> <a href="https://github.com/iphysresearch/S_Dbw_validity_index/watchers" target="_blank"><img src="https://img.shields.io/github/watchers/iphysresearch/S_Dbw_validity_index.svg?style=social" alt="Github Watch Badge"></a><a href="https://github.com/iphysresearch/S_Dbw_validity_index/stargazers" target="_blank"><img src="https://img.shields.io/github/stars/iphysresearch/S_Dbw_validity_index.svg?style=social" alt="Github Star Badge"></a></h4><blockquote>S_Dbw validity index | ä»£ç å…¨è§£æï¼Œå¯å¦è§<a href="about:blank" target="_blank">åšæ–‡</a>ã€‚</blockquote><p>&nbsp;</p><p>&nbsp;</p><p><br></p><hr><p class="ql-align-center"><a href="http://iphysresearch.github.io" target="_blank"><img src="https://i.loli.net/2018/07/11/5b44d8c9d094f.jpeg" alt="Background"></a></p><blockquote>æœ¬ç«™ç‚¹å†…å®¹ç³»æœ¬ä½œè€…åŸåˆ›ï¼Œå¦‚æœ‰ä»»ä½•çŸ¥è¯†äº§æƒã€ç‰ˆæƒé—®é¢˜æˆ–ç†è®ºé”™è¯¯ï¼Œè¿˜è¯·æŒ‡æ­£ã€‚</blockquote><blockquote>è½¬è½½è¯·æ³¨æ˜åŸä½œè€…åŠå‡ºå¤„ï¼Œè°¢è°¢é…åˆã€‚</blockquote><blockquote><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank"><img src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" alt="Creative Commons License"></a></blockquote><blockquote><a href="http://iphysresearch.github.io" target="_blank">IPhysResearch</a>Â·<a href="http://iphysresearch.github.io" target="_blank">åœŸè±†</a>Â·<a href="http://iphysresearch.github.io" target="_blank">Herb</a>Â·<a href="http://iphysresearch.github.io" target="_blank">He Wang</a> Â© 2018 (under construction)</blockquote><p> { "openSidebar": false, "showHighlights": true, "theme": classic, "enableExperimentalNewNoteButton": true }&nbsp;</p><p><br></p>