---
title: CS231n Lecture.13
date: 2018-09-06
---

[返回到上一页](./index.html)

---

[TOC]

> CS231n 课程的官方地址：http://cs231n.stanford.edu/index.html
>
> 该笔记根据的视频课程版本是 [Spring 2017](https://www.bilibili.com/video/av17204303/?p=27)(BiliBili)，PPt 资源版本是 [Spring 2018](http://cs231n.stanford.edu/syllabus.html).
>
> 另有该 Lecture 13. 扩展讲义资料：
>
> - [DeepDream](https://github.com/google/deepdream)
> - [neural-style](https://github.com/jcjohnson/neural-style)
> - [fast-neural-style](https://github.com/jcjohnson/fast-neural-style)



# Lecture 13. Visualizing and Understanding



ConvNets 的内部究竟是怎样的呢？



## First Layer: Visualize Filters

此处有三篇 papers：

> Krizhevsky, “One weird trick for parallelizing convolutional neural networks”, arXiv 2014
> He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
> Huang et al, “Densely Connected Convolutional Networks”, CVPR 2017

观察第一层的卷积核。你可以看到它们都在寻找有向边（oriented edges），比如明暗线条。从不同的角度和位置来观察输入图像，我们可以看到完全相反的颜色，比如绿色和粉色，或者橙色和蓝色灯相反的颜色。

![image-20180906094354703](assets/image-20180906094354703.png)

- Q：为什么要可视化卷积核的权重是为了让你知道卷积核在寻找什么？
  - 这个直觉来源于模板匹配和内积。想象一下，如果你有些模板向量，然后你通过模板向量和一些任意的数据之间的点积得到了标量输出。然后当这两个向量相互匹配输入将在范数约束的条件下被最大化。所以，不管你在什么时候进行内积，你正在进行内积的对象是内积的结果最大化的因素。所以这就是为什么我们要可视化权重以及它们能够提供给我们第一卷积层寻找的东西的原因。 

中间层权重可视化后的可解释性就差很多了。

- 所有的可视化图像，我们都把权值设定在 0 到 255 这个区间内。实际上，那些权值可以是无限的（任意的），它们可以在任意范围之内。但是为了得到比较好的可视化效果，并且（没有限定权值范围的）可视化也没有考虑到这些层的偏置，所以我们需要对它们进行缩放。所以你们应该记住模型可视化的恰当时机。





## Last Layer

- 可以考虑用 L2 Nearest neighbors in feature space 来可视化。

- t-sne 非线性降维方法（不同于主成分分析 PCA）

此处一篇 paper：

> Van der Maaten and Hinton, “Visualizing Data using t-SNE”, JMLR 2008

![image-20180906101211034](assets/image-20180906101211034.png)

See high-resolution versions at http://cs.stanford.edu/people/karpathy/cnnembed/



## Visualizing Activations

虽然中间层的权重可视化可解释性很不好。但实际上，可视化中间层的激活映射图（activation maps），在某些情况下是具备可解释性的。

此处一篇 paper：

> Yosinski et al, “Understanding Neural Networks Through Deep Visualization”, ICML DL Workshop 2014
>
> http://yosinski.com/deepvis

![image-20180906103404102](assets/image-20180906103404102.png)

![ezgif.com-video-to-gif-2](assets/ezgif.com-video-to-gif-2.gif)



## Maximally Activation Patches

- 可视化输入图像中什么类型的图像块可以最大限度地激活？

此处一篇 paper：

> Springenberg et al, “Striving for Simplicity: The All Convolutional Net”, ICLR Workshop 2015

选取卷积层中的某个通道。通过卷积神经网络运行很多的图像，对于每一个图像，记录它们的卷积特征，然后可以观察到那个特征映射图的部分，已经被我们图像的数据集最大地激活。因为这是卷积层，卷积层中的每个神经元在输入部分都有一些小的感受野，每个神经元的（管辖部分）并不是整个图像，它们只针对于这个图像的子集合，然后我们要做的是，从这个庞大的图像数据集中，可视化来自该特顶层特定特征的最大激活的图像块。然后我们可以根据这些激活块，在特定层的激活程度来解决这个问题。

![image-20180906110005206](assets/image-20180906110005206.png)



##  Which pixels matter: Saliency vs Occlusion

- 弄清楚究竟是输入图像的哪个部分，导致神经网络做出分类的决定。
  1. 排除实验！（Occlusion）

  此处一篇 paper：

	> Zeiler and Fergus, “Visualizing and Understanding Convolutional Networks”, ECCV 2014

![image-20180906110703406](assets/image-20180906110703406.png)





2. 显著图（Saliency）

   > Simonyan, Vedaldi, and Zisserman, “Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps”, ICLR Workshop 2014.

   计算相对于输入图像中各像素对应的类预测值的梯度。在一阶近似意义上，对于每个输入图像的每个像素，如果我们进行小小的扰动，那么相应类的分类分值会有多大的变化。这是另一种用来解决输入图像的哪个部分的像素用于分类。

   ![image-20180906120619768](assets/image-20180906120619768.png)

   ![image-20180906120629403](assets/image-20180906120629403.png)



## Intermediate Features via (guided) backprop

引导式反向传播

这里有两篇 papers：

> Zeiler and Fergus, “Visualizing and Understanding Convolutional Networks”, ECCV 2014
> Springenberg et al, “Striving for Simplicity: The All Convolutional Net”, ICLR Workshop 2015

![image-20180906120954176](assets/image-20180906120954176.png)



![image-20180906121127951](assets/image-20180906121127951.png)

![image-20180906121153534](assets/image-20180906121153534.png)

- 关于引导式反向传播或计算显著图的意见非常有趣的事情是：总有一个固定输入图像的函数，这个函数告诉我们对于一个固定输入的图像，输入图像的哪个像素或哪个部分影响了神经元的分值。



## Visualizing CNN features: Gradient Ascent

另一个你可能会问的问题是：如果我们在一些输入图像上移除这种依赖性，那什么类型的输入会激活这个神经元。

我们可以通过**梯度上升（Gradient Ascent）**算法来解决这个问题。记住我们总是在训练卷积神经网络时，使用梯度下降来使（函数）损失最小化。而现在，我们想要固定训练的卷积神经网络的权重，并且在图像的像素上执行梯度上升来合成图像，以尝试和最大化某些中间神经元和类的分值。

在执行梯度上升的过程中，我们不再优化神经网路中保持不变的权重。相反，我们试图改变一些图像的像素，使这个神经元的值或这个类的分值最大化。除此之外，我们需要一些正则项。在看到正则项尝试阻止神经网络权重过拟合训练数据之前，我们需要类似的东西来防止我们生成的图像过拟合特定网络的特性。所以，这里我们经常会加入一些正则项，我们需要具备两个特定属性的生成图像：

1. 我们想要最大程度地激活一些分值或神经元的值；
2. 我们希望这个生成图像看起来是自然的，即我们想要生成图像具备在自然图像中的统计数据。这些主管的正则项是强制生成图像看起来像是自然图像的东西。

![](https://i.loli.net/2018/09/06/5b90ab8865c42.png)

步骤通常来说很简单：

![image-20180906122928554](assets/image-20180906122928554.png)

- Q：如果不使用任何的正则化会怎样？
  - 那么你将很有可能得到分值最大化并且已经分好的类，但是通常它看起来不像任何事物，就像随机噪声。

![image-20180906123941510](assets/image-20180906123941510.png)

有人增加了一些令人印象深刻的正则化，在下面的 paper 中：

> Yosinski et al, “Understanding Neural Networks Through Deep Visualization”, ICML DL Workshop 2014.

除了 L2 范数约束之外。另外我们还定义在优化过程中，对图像进行高速模糊处理，我们同时也将一些低梯度的小的像素值修改为 0。可以看到这是一个投射梯度上升算法，我们定期投射具备良好属性的生成图像到更好的图像集中。举个例子，进行高斯模糊处理后，图像获得特殊平滑性。所以，对图像进行高斯模糊处理后，更容易获得清晰的图像。

![](https://i.loli.net/2018/09/06/5b90afb2e9430.png)

除了对最后分值，我们也可以最大化某个中间层的其中一个神经元的分值：

![image-20180906124622572](assets/image-20180906124622572.png)



下面是一个针对性解决多模态问题的 paper：

> Nguyen et al, “Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks”, ICML Visualization for Deep Learning Workshop 2016.

在这篇论文中，他们尝试在优化的过程中考虑多模态问题，即对每一个类运行聚类算法以使这些类分成不同的模型，然后用接近这些模型其中之一的类进行初始化。那么当你这么做的事情，你就可能考虑多模态形式。

![](https://i.loli.net/2018/09/06/5b90b2b08a8e1.png)

![image-20180906125530125](assets/image-20180906125530125.png)



你还可以添加更加强大的先验图像，然后生成非常漂亮的图像。这些生成图像都是通过最大化一些图像类的分值得到的。一般的想法是，尝试优化图像在 FC6 潜在空间中的表示，而不是直接优化输入图像的像素。他们需要使用特征反演网络。

多说无益，给你 paper：

> Nguyen et al, “Synthesizing the preferred inputs for neurons in neural networks via deep generator networks,” NIPS 2016

重点是：当你开始在自然图像建模时，添加先验图像，你最终可以得到一些非常真实的图像，它们让你了解到神经网络究竟在寻找什么（特征）。





## Fooling Images / Adversarial Examples

1. Start from an arbitrary image 
2. Pick an arbitrary class 
3. Modify the image to maximize the class 
4. Repeat until network is fooled 

多说无益，自己看 Goodfellow 的讲座：

> Check out [Ian Goodfellow’s lecture](https://www.youtube.com/watch?v=CIfsB_EYsVI) from last year （[Bilibili](https://www.bilibili.com/video/av17204303/?p=35&spm_id_from=333.788.multi_page.5)）



- Q：了解中间神经元，对我们了解最终的分类有什么帮助？
  - 实际上，试图对中间神经元进行可视化是深度学习里备受批评的领域。比如，你有一个大的黑盒子神经网络，你通过梯度上升来训练它并且得到了一个很好的值，那很好，但是我们并不信任这个神经网络，因为我们作为人类并不理解，它是如何作出决策的。很多这种类型的可视化技术被开发来试图从人类的角度来理解并解决，比如为什么这些网络分类的类别在逐渐地增多的问题，因为如果将深层神经网络与其他机器运行技术进行对比，（你会发现）线性模型更容易被解释，因为你可以观察权值并且理解每个特征对决策的影响程度，比如随机森林或决策树，一些其他的机器学习模型可以通过它们本身的特性，变得比这个黑盒子卷积神经网络更具有可解释性。很多回应这种质疑的声音认为，虽然它们是大型复杂的模型，但是它们仍然在做一些有趣的具备可解释性的事情，它们不是完全随意地分类事物，相反，它们在做有意义的事情。















---

[返回到上一页](./index.html) | [返回到顶部](./cs231n_13.html)

---
<br>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
<br>

<script type="application/json" class="js-hypothesis-config">
  {
    "openSidebar": false,
    "showHighlights": true,
    "theme": classic,
    "enableExperimentalNewNoteButton": true
  }
</script>
<script async src="https://hypothes.is/embed.js"></script>


