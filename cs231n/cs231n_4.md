---
title: CS231n Lecture.4
date: 2018-08-21
---

[è¿”å›åˆ°ä¸Šä¸€é¡µ](./index.html)

---

[TOC]

> CS231n è¯¾ç¨‹çš„å®˜æ–¹åœ°å€ï¼šhttp://cs231n.stanford.edu/index.html
>
> è¯¥ç¬”è®°æ ¹æ®çš„è§†é¢‘è¯¾ç¨‹ç‰ˆæœ¬æ˜¯ [Spring 2017](https://www.bilibili.com/video/av17204303/?p=9)(BiliBili)ï¼ŒPPt èµ„æºç‰ˆæœ¬æ˜¯ [Spring 2018](http://cs231n.stanford.edu/syllabus.html).
>
> å¦æœ‰è¯¥ Lecture 3. æ‰©å±•è®²ä¹‰èµ„æ–™ï¼š
>
> - [backprop notes](http://cs231n.github.io/optimization-2) [[ä¸­è¯‘ç‰ˆ](./CS231n_backprop_notes.html)]
>
> - [linear backprop example](http://cs231n.stanford.edu/handouts/linear-backprop.pdf) 
>
> - [derivatives notes](http://cs231n.stanford.edu/handouts/derivatives.pdf) (optional)  
>
> - [Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) (optional) 
>
>   related: [1](http://colah.github.io/posts/2015-08-Backprop/), [2](http://neuralnetworksanddeeplearning.com/chap2.html), [3](https://www.youtube.com/watch?v=q0pm3BrIUFo) (optional)



# Lecture 4. **Introduction to Neural Networks** 

è¯¾ç¨‹åˆšä¸€å¼€å§‹ï¼Œå°å§å§çš„é¢œå€¼å°±ç«‹é©¬å¼€å§‹å¸ç²‰ã€‚ã€‚ã€‚

![](https://i.loli.net/2018/08/22/5b7d80395431d.png)

## Backpropagation

åºŸè¯ä¸å¤šè¯´ï¼Œæ€»ç»“äº†ä¹‹å‰è®²çš„æ•…äº‹åã€‚ç”¨è®¡ç®—å›¾æ€»ç»“äº†çº¿æ€§è®¡ç®—å’ŒæŸå¤±å‡½æ•°åï¼Œå°±ç«‹é©¬å¼€å§‹å¼•å‡ºæœ¬ lecture ä¸­æœ€é‡è¦çš„ç®—æ³•ï¼šåå‘ä¼ æ’­ç®—æ³•ã€‚

è™½ç„¶åå­—æ˜¯åå‘ä¼ æ’­ç®—æ³•ï¼ˆbackpropagationï¼‰ï¼Œä½†å…¶å®èƒŒåçš„é€»è¾‘æ˜¯è¿‡äºç®€å•çš„ï¼š**é“¾å¼æ³•åˆ™**å˜›ã€‚æ­¤å¤„ä¸å†èµ˜è¿°ï¼Œæ¬¢è¿ç›´æ¥é˜…è¯»ï¼š[ä¸€æ®µå…³äºç¥ç»ç½‘ç»œçš„æ•…äº‹](./cs231n_story_MLP.html#header-n166)ã€‚

- Qï¼šä¸ºä»€ä¹ˆä¸ç”¨å¾®ç§¯åˆ†ç†è®ºç›´æ¥è®¡ç®—å‚æ•°çš„æ¢¯åº¦å‘¢ï¼Ÿ

  - å½“é¢å¯¹å¤æ‚è¡¨è¾¾å¼çš„æ—¶å€™ï¼Œå°†å…¶åˆ†è§£ä¸ºä¸€äº›è®¡ç®—èŠ‚ç‚¹ï¼Œå¯ä»¥éå¸¸ç®€å•çš„è®¡ç®—å¾—å‡ºæ¢¯åº¦ã€‚

- Noteï¼šå…³äºèŠ‚ç‚¹ä»¬çš„éƒ¨åˆ†ç»„åˆå¯ä»¥å¾ˆå¥½çš„å®ç°æ›´ç®€æ´æ›´å¿«é€Ÿçš„æ¢¯åº¦è®¡ç®—å›¾è¡¨è¾¾ã€‚æ¯”å¦‚è¯´ï¼Œsigmoid function $\sigma(x)=\frac{1}{1+e^{-x}}$ å°±æœ‰ï¼š
  $$
  \frac{d\sigma(x)}{dx}=\frac{e^{-x}}{(1+e^{-x})^2}=\Big(\frac{1+e^{-x}-1}{1+e^{-x}}\Big)\Big(\frac{1}{1+e^{-x}}\Big)=(1-\sigma(x))\sigma(x)
  $$





ç»è¿‡å‡ ä¸ªå°ä¾‹å­çš„è€ƒéªŒï¼Œæˆ‘ä»¬ä¼¼ä¹å¯ä»¥æ€»ç»“å‡ºè§„å¾‹äº†ï¼š

![](https://i.loli.net/2018/08/23/5b7d93e09a259.png)

- <u>Patterns in backward flow</u>
  - **add** gate: gradient distributor. æ¢¯åº¦åˆ†å¸ƒå™¨ï¼šåå‘åˆ†å‘å’Œä¼ é€’ç›¸åŒçš„æ¢¯åº¦å€¼
  - **max** gate: gradient router. æ¢¯åº¦è·¯ç”±å™¨ï¼šåå‘ä¼ é€’åœ¨å‰å‘ä¸­è¾ƒå¤§å€¼æ–¹å‘çš„æ¢¯åº¦å€¼
  - **mul** gate: gradient switcher. æ¢¯åº¦è½¬æ¢/ç¼©æ”¾å™¨ï¼šæ ¹æ®å¦ä¸€ä¸ªåˆ†æ”¯çš„å€¼å¯¹æ¢¯åº¦è¿›è¡Œç¼©æ”¾

- è¿™æ—¶å€™æœ‰ä¸ªå­¦ç”Ÿæäº†ä¸€ä¸ªé—®é¢˜ï¼šè¿™æ¢¯åº¦æ•°å­—ç»è¿‡å¦‚æ­¤ä¸€ç•ªè®¡ç®—åï¼Œé‚£å¦‚ä½•æ›´æ–°å‚æ•°å‘¢ï¼Ÿ
  - å…¶å®ï¼Œæˆ‘çš„ç†è§£æ˜¯ï¼šä¸Šé¢ğŸ‘†å›¾åƒä¸­æœ€å·¦ä¾§å¾—åˆ°çš„ä¸åŒçš„çº¢è‰²æ¢¯åº¦æ•°å­—å°±æ˜¯æš—ç¤ºäº†è¿™äº›å‚æ•°ç©¶ç«Ÿå¯¹æœ€åçš„æŸå¤±å€¼åœ¨åŠ¨æ€çš„è¿­ä»£æ›´æ–°ä¸­æ˜¯æ€æ ·çš„é™æ€å˜åŒ–ã€‚æœ‰çš„ç»å¯¹å€¼æ•°å­—å¤§ï¼Œä¸å°±æ˜¯è¯´ä¸«çš„å‚æ•°åº”è¯¥å¤šç»™ç‚¹åŠ›ï¼Œç»å¯¹å€¼æ•°å­—å°çš„å°±æ˜¯è¯´è¿™ä¸ªå‚æ•°å¯ä»¥å…ˆâ€œæŒ‰å…µä¸åŠ¨â€ï¼Œæ•°å­—çš„ç¬¦å·è‡ªç„¶ä¹Ÿå°±ç›´æ¥å†³å®šäº†è¯¥å‚æ•°å¯¹æŸå¤±å€¼å˜åŒ–çš„æ–¹å‘ã€‚é‚£ä¹ˆæ‰€è°“çš„å‚æ•°æ›´æ–°å°±æ˜¯æœ€ä¼˜åŒ–é—®é¢˜äº†ï¼Œçœ‹ä½ æ€ä¹ˆåˆ©ç”¨è¿™ä¸ªæ¢¯åº¦æ•°å­—äº†ã€‚



æ•°å­—çš„å‰å‘/åå‘ä¼ æ’­è¯´å®Œåï¼Œå°±è¦æ¨å¹¿åˆ°çŸ¢é‡çš„å‰å‘/åå‘ä¼ æ’­äº†ã€‚

å…¶å®ä¸å¯æ€•ï¼Œä¸å°±æ˜¯å¯¹ç§°çš„é›…å…‹æ¯”çŸ©é˜µå˜›~ ï¼ˆ[Wiki](https://zh.wikipedia.org/wiki/é›…å¯æ¯”çŸ©é˜µ)ï¼‰

- Always check: The gradient with respect to a variable should have the same shape as the variable.



åé¢å°±çœŸæ²¡å•¥å†…å®¹äº†ã€‚å¯ä»¥ç®€å•çš„æ€»ç»“ä¸ºï¼šéœ€è¦è‡ªå·±åŠ¨ç¬”ç¢ç£¨ï¼Œè‡ªå·±åŠ¨æ‰‹ä»£ç æ‰èƒ½å½»åº•ç†è§£ã€‚

è¿˜æœ‰æ‰©å±•èµ„æ–™æ˜¯éå¸¸å¹²çš„~ ï¼Œä½ æ‡‚æ»´ï¼



## Neural Networks

è¯¾ç¨‹åˆšä¸€å¼€å§‹å°±å¼ºè°ƒï¼šwithout the brain stuffï¼ç®€å•åœ°è¯´ï¼Œå°±æ˜¯ä¸è¦è·Ÿæˆ‘æ‰¯ä»€ä¹ˆç”Ÿç‰©ç¥ç»å…ƒå•¥çš„~ è¿™ä¸ªæ¦‚å¿µæ—©å·²ä¸ä¹‹åˆ†é“æ‰¬é•³~

éšåçš„ä¸¤å±‚ç¥ç»ç½‘ç»œè®²çš„æœ‰äº›ç¨€é‡Œç³Šæ¶‚ï¼Œè§†é¢‘é‡Œå±…ç„¶æ²¡æœ‰è¯´æ¸…æ¥šéçº¿æ€§å°±æ˜¯ $\max$ å‡½æ•°ï¼Œä¸å¦‚çœ‹æˆ‘è‡ªå·±å†™çš„è¿™ä¸€éƒ¨åˆ†ï¼ˆ[ä¸€ä¸ªç¥ç»å…ƒçš„æœ¬äº‹](./cs231n_story_MLP.html#header-n59)ï¼‰ã€‚åˆæ˜¯é‚£å¥è¯ï¼Œæ‰¯åŠå¤©æ¦‚å¿µå’Œæµç¨‹å›¾ï¼Œéƒ½ä¸å¦‚ç›´æ¥çœ‹ä»£ç æ¥çš„å®åœ¨ï¼š

```python
import numpy as np
from numpy.random import randn

N, D_in, H, D_out = 64, 1000, 100, 10
x, y = randn(N, D_in), randn(N, D_out)
w1, w2 = randn(D_in, H), randn(H, D_out)

for t in range(2000):
    h = 1 / (1 + np.exp(-x.dot(w1)))
    y_pred = h.dot(w2)
    loss = np.square(y_pred - y).sum()
    print(t, loss)
    
    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = h.T.dot(grad_y_pred)
    grad_d = grad_y_pred.dot(w2.T)
    grad_w1 = x.T.dot(grad_h * h * (1 - h))
    
    w1 -= 1e-4 * grad_w1
    w2 -= 1e-4 * grad_w2
```

ä¸Šé¢æ˜¯ä¸€ä¸ªä¸¤å±‚ç¥ç»ç½‘ç»œï¼Œæœ‰ä¸€ä¸ªå•éšå±‚çš„ç»“æ„ã€‚ä½œä¸šå°±æ˜¯ä»¥æ­¤ä½œä¸ºçµæ„Ÿï¼Œä¹Ÿå†™ä¸€ä¸ªä¸¤å±‚å…¨è¿æ¥çš„ç¥ç»ç½‘ç»œã€‚

æ¥ä¸‹æ¥çš„å†…å®¹ï¼ˆé™¤äº†ç¥ç»å…ƒç±»æ¯”ï¼‰ï¼Œå°±å¼€å§‹çœŸæ­£æœ‰äº›å¹²è´§å‡ºæ¥äº†ï¼Œä¸»è¦æ˜¯è¿™ä¸ªå›¾ï¼š

![](https://i.loli.net/2018/08/24/5b7eed913ab48.png)



è¿™å°±æ˜¯ä¸€ä¸ªç¥ç»å…ƒçš„æœ¬äº‹ã€‚æ—¢æœ‰çº¿æ€§å˜æ¢ï¼Œä¹Ÿæœ‰éçº¿æ€§åŠ æŒã€‚å·®ä¸å¤šæ˜¯æœ€ä¸€èˆ¬æ„ä¹‰çš„åŸºç¡€è®¡ç®—å•å…ƒã€‚ä»£ç å¯ä»¥å¦‚ä¸‹è¡¨ç¤ºï¼š

```python
class Neuron:
    # ...
    def neuron_tick(input):
        """ assume inputs and weights are 1-D numpy arrays and bias is a number"""
        cell_body_sum = np.sum(inputs * self.weights) + self.bias
        firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function
        return firing_rate
```

å¾ˆå¿«åœ°ï¼Œå°å§å§åˆå¼€å§‹è­¦å‘Šä¸è¦éšä¾¿å’Œç”Ÿç‰©å­¦æ‰¯ä¸Šå…³ç³»ï¼

> Be very careful with your brain analogies!

- Biological Neuronsï¼š
  - Many different types
  - Dendrites can perform complex non-linear computations
  - Synapses are not a singal weight but a complex non-linear dynamical system
  - Rate code may not be adequate

### Activation functions

è§†é¢‘é‡Œåˆ—å‡ºäº†ä¸€æ³¢å¸¸è§çš„æ¿€æ´»å‡½æ•°ï¼Œä½†æ˜¯å±…ç„¶æ²¡æœ‰ç»†è®²ï¼Œç›´æ¥é£è¿‡å»äº†ï¼š

![](https://i.loli.net/2018/08/24/5b7eef8c7772e.png)

ç„¶åï¼Œå¼ºè°ƒäº†ä¸åŒå±‚çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œçš„å«æ³•ï¼š

![](https://i.loli.net/2018/08/24/5b7eefd02183d.png)

æœ€åï¼Œåˆè¯´å…¶å®æˆ‘ä»¬åˆšåˆšä»£ç æ˜¯é’ˆå¯¹ä¸€ä¸ªç¥ç»å…ƒçš„è®¡ç®—ï¼Œå…¶å®æˆ‘ä»¬å¯ä»¥æ¨å¹¿åˆ°ä¸€å±‚ç¥ç»å…ƒçš„æ“ä½œã€‚æ¯”å¦‚ä¸Šå›¾å³é¢çš„3å±‚ç¥ç»ç½‘ç»œï¼ˆä¸¤ä¸ªéšè—å±‚ï¼‰ï¼Œåˆ©ç”¨çŸ©é˜µä¹˜æ³•ï¼Œå¯ä»¥è¿™ä¹ˆå†™ä»£ç ï¼š

```python
# forward-pass of a 3-layer neural network:
f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)
x = np.random.randn(3, 1) # random input vector of three numbers (3x1)
h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)
h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)
out = np.dot(W3, h2) + b3
```

å†ç„¶åã€‚ã€‚ã€‚è¿™èŠ‚è¯¾å°±ç»“æŸå•¦ï¼ï¼ã€‚ã€‚ã€‚











---

[è¿”å›åˆ°ä¸Šä¸€é¡µ](./index.html) | [è¿”å›åˆ°é¡¶éƒ¨](./cs231n_4.html)

---
<br>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
<br>

<script type="application/json" class="js-hypothesis-config">
  {
    "openSidebar": false,
    "showHighlights": true,
    "theme": classic,
    "enableExperimentalNewNoteButton": true
  }
</script>
<script async src="https://hypothes.is/embed.js"></script>
