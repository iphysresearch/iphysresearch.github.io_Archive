---
title: CS231n Lecture.4
date: 2018-08-21
---

[返回到首页](./index.html)

---

[TOC]

> CS231n 课程的官方地址：http://cs231n.stanford.edu/index.html
>
> 该笔记根据的视频课程版本是 [Spring 2017](https://www.bilibili.com/video/av17204303/?p=9)(BiliBili)，PPt 资源版本是 [Spring 2018](http://cs231n.stanford.edu/syllabus.html).
>
> 另有该 Lecture 3. 扩展讲义资料：
>
> - [backprop notes](http://cs231n.github.io/optimization-2) [[中译版](./CS231n_backprop_notes.html)]
>
> - [linear backprop example](http://cs231n.stanford.edu/handouts/linear-backprop.pdf) 
>
> - [derivatives notes](http://cs231n.stanford.edu/handouts/derivatives.pdf) (optional)  
>
> - [Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) (optional) 
>
>   related: [1](http://colah.github.io/posts/2015-08-Backprop/), [2](http://neuralnetworksanddeeplearning.com/chap2.html), [3](https://www.youtube.com/watch?v=q0pm3BrIUFo) (optional)



# Lecture 4. **Introduction to Neural Networks** 

课程刚一开始，小姐姐的颜值就立马开始吸粉。。。

![](https://i.loli.net/2018/08/22/5b7d80395431d.png)

## Backpropagation

废话不多说，总结了之前讲的故事后。用计算图总结了线性计算和损失函数后，就立马开始引出本 lecture 中最重要的算法：反向传播算法。

虽然名字是反向传播算法（backpropagation），但其实背后的逻辑是过于简单的：**链式法则**嘛。此处不再赘述，欢迎直接阅读：[一段关于神经网络的故事](./cs231n_story_MLP.html#header-n166)。

- Q：为什么不用微积分理论直接计算参数的梯度呢？
  - 当面对复杂表达式的时候，将其分解为一些计算节点，可以非常简单的计算得出梯度。

- Note：关于节点们的部分组合可以很好的实现更简洁更快速的梯度计算图表达。比如说，sigmoid function $\sigma(x)=\frac{1}{1+e^{-x}}$ 就有：
  $$
  \frac{d\sigma(x)}{dx}=\frac{e^{-x}}{(1+e^{-x})^2}=\Big(\frac{1+e^{-x}-1}{1+e^{-x}}\Big)\Big(\frac{1}{1+e^{-x}}\Big)=(1-\sigma(x))\sigma(x)
  $$




经过几个小例子的考验，我们似乎可以总结出规律了：

![](https://i.loli.net/2018/08/23/5b7d93e09a259.png)

- <u>Patterns in backward flow</u>
  - **add** gate: gradient distributor. 梯度分布器：反向分发和传递相同的梯度值
  - **max** gate: gradient router. 梯度路由器：反向传递在前向中较大值方向的梯度值
  - **mul** gate: gradient switcher. 梯度转换/缩放器：根据另一个分支的值对梯度进行缩放

- 这时候有个学生提了一个问题：这梯度数字经过如此一番计算后，那如何更新参数呢？
  - 其实，我的理解是：上面👆图像中最左侧得到的不同的红色梯度数字就是暗示了这些参数究竟对最后的损失值在动态的迭代更新中是怎样的静态变化。有的绝对值数字大，不就是说丫的参数应该多给点力，绝对值数字小的就是说这个参数可以先“按兵不动”，数字的符号自然也就直接决定了该参数对损失值变化的方向。那么所谓的参数更新就是最优化问题了，看你怎么利用这个梯度数字了。



数字的前向/反向传播说完后，就要推广到矢量的前向/反向传播了。

其实不可怕，不就是对称的雅克比矩阵嘛~ （[Wiki](https://zh.wikipedia.org/wiki/雅可比矩阵)）

- Always check: The gradient with respect to a variable should have the same shape as the variable.



后面就真没啥内容了。可以简单的总结为：需要自己动笔琢磨，自己动手代码才能彻底理解。

还有扩展资料是非常干的~ ，你懂滴！



## Neural Networks











---

[返回到首页](./index.html) | [返回到顶部](./cs231n_4.html)

---
<br>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
<br>

<script type="application/json" class="js-hypothesis-config">
  {
    "openSidebar": false,
    "showHighlights": true,
    "theme": classic,
    "enableExperimentalNewNoteButton": true
  }
</script>
<script async src="https://hypothes.is/embed.js"></script>
