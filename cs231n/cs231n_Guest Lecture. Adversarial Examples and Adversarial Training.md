---
title: CS231n Guest Lecture. Adversarial Examples and Adversarial Training
date: 2018-09-15
---

[返回到上一页](./index.html)

---

[TOC]

> CS231n 课程的官方地址：http://cs231n.stanford.edu/index.html
>
> 该笔记根据的视频课程版本是 [Spring 2017](https://www.bilibili.com/video/av17204303/?p=35)(BiliBili)，PPt 资源版本是 [Spring 2017](http://cs231n.stanford.edu/2017/syllabus).
>

# Guest Lecture. Adversarial Examples and Adversarial Training


- Slide...
<iframe src="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture16.pdf" style="width:1000px; height:800px;" width="100%" height=100%>This browser does not support PDFs. Please download the PDF to view it: <a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture16.pdf">Download PDF</a></iframe>



我今天要给你们介绍一下对抗样本和对抗训练。

![](https://i.loli.net/2018/09/15/5b9c5ac37539a.png)

回顾一下，先来讲讲，对抗样本是什么。再来解释一下对抗样本为何会涌现出来，或者说为什么它们是有用的。我会讲一点对抗样本如何构成现实世界的安全威胁，包括使用对抗手段来损坏基于机器学习的系统。我会讲一下目前的防御手段，但大部分的防御手段仍然是开放的研究问题。我希望你们其中的某些人可以去深入研究。最后我会讲一下如何使用对抗样本，来提高其他机器学习算法的性能，就算是你只想构建一个不需要面临现实世界对抗的机器学习算法。

---

![](https://i.loli.net/2018/09/15/5b9c5ae472b89.png)

看（上面）这张大图和这次讲座的课题，我相信在场的大多对机器学习在各种不同的项目上的惊人功力和巨大成就有所耳闻。以前这些问题都不能用软件编程解决，多亏了深度学习、卷积网络、梯度下降，这些问题都得以解决，这些技术都效果非常好。一直到几年前，这些技术才真正开始有效，

大概在2013年，我们看到深度学习在很多问题上达到了人类水平。可以看到卷积网络在识别目标和图片时，在这些基准集上得分和人差不多，人类和机器达到同一水平，一部分原因是一般人区别不了阿拉斯基哈士奇和西伯利亚哈士奇。但抛开这些基准集中的特殊情况，应用在目标识别中的深度学习，在2013年左右就已经赶上人类水平了。同一年，我们也见证了目标识别在人脸方面应用也和人类达到了同一水平，就是说忽然之前，我们就有可以和你我识别陌生人能力媲美的能识别人脸的计算机了。在辨别朋友和家人的人脸方面，你肯定会比计算机强，但你在辨别那些你没怎么打过交道的人的时候，计算机在2013年就和我们差不多了。我们还见证了2013年计算机在识别照片上的字体也和人类水平差不多，甚至导致我们无法使用验证码来区分网页用户是人类还是机器了，因为卷积网络在识别模糊文字上比人类还厉害。

所以现在深度学习的效果那么好，尤其是计算机视觉的效果那么好，所以现在能够让计算机犯错误的情况是越来越少了。但在2013年以前，没有人会对计算机犯错误感到惊讶，计算机犯错误是个常态而不是例外，

所以今天我们要讲的是深度学习算法造成的罕见的错误。这个主题都不是一个正儿八经的学习主题，直到算法大部分情况下效果都不错的时候。现在人们研究的是算法崩坏的情况，这其实是例外而不会常规。

---

![](https://i.loli.net/2018/09/15/5b9c5db910f1f.png)

对抗样本是那种用心构造出来会被分错类的样本。大部分情况下，我们会构造一个对人类观察者来说，看起来和原始图片不可区分的新图片。

我（上面）给你展示一个关于熊猫的例子。左边是一个熊猫，没有经过任何修正，在 imagenet 数据集上训练好的卷积网络，能够识别出这是一个熊猫。有意思的是这个模型对于这个判断没有十足的把握，大概有60%的概率认为这张图片是一个熊猫。如果我们确切地计算如果对图片做一个修正，使得卷积网络在识别修正后的图片犯错误，我们找到一个由（上图）中间的图像给出的移动所有像素的最优方向。在人们看来这很像是噪声，但它其实不是噪声，也是用心构造出来的一个网络参数的函数，其实这个函数有着复杂的结构。如果我们把构造出来的攻击图像乘上一个非常小的系数，然后加到原始的熊猫图片上，我们就可以得到一个人类看起来和原来熊猫看起来完全没有区别的图片。实际上幻灯片（上图）右边的熊猫和左边的熊猫就是看不出来区别。但当我们把这张新图片给卷积网络看时，我们使用32位浮点数，这个显示器只能展示8位颜色分辨率（256种颜色）。我们只改变那么一丢丢，甚至小到都无法影响其中最小的8位，但影响到了32位浮点数表示中的其他24位。但这一点点的区别就足够欺骗卷积网络，这次它把熊猫认成一只长臂猿。

另一件有意思的事情是，它还不只是改变了分类，还不仅仅是碰到决策边界（熊猫和长臂猿的类别边界），然后就那么跨过去（分错类）了而已。卷积网络居然还对它错误的分类预测有了很高的置信度，就是说网络认为右边图片是一只长臂猿的置信度，比原始图片是一只熊猫的置信度还搞。网络认为（上图）右边的图片99.9%的概率是一只长臂猿，就在刚才，卷积网络还觉得左边的图片有 1/3 的可能不是一只熊猫，现在它居然就几乎敢肯定右边是一只长臂猿了。

以前曾经有人研究各种可以欺骗不同的机器学习模型的计算攻击手段，至少是2004年开始的，或者更早。很长时间以来，我们做到了欺骗垃圾邮件检测器。大约在2013年，Battista Biggio 发现可以用这种方式来欺骗神经网络，在差不多时间，我的同事 Christian Szegedy 发现可以用这种攻击来对抗深度神经网络，只要用一个搜索图片输入的优化算法。

---

![](https://i.loli.net/2018/09/15/5b9c620f39854.png)

我今天会给你们聊很多我自己在这个主题上的一些后续工作。我前几年花了很多心思来理解为什么这些攻击是可行的，为什么那么轻易就骗过了卷积网络。当我的同时 Christian 和 Battista Biggio 在差不多时候自己独立地发现了这个现象，他试图做的其实是一个可视化结果。他不是在研究安全性，也不是在研究如何欺骗一个神经网络。相反，他有了一个可以很好地识别目标的卷积网络，他想知道这个网络到底是如何工作的。于是他就想到他可以拿一个场景的图片，比如说船的照片，然后他逐步地变换图片，直到网络认为，那是一架飞机。在形变过程中，看起来像飞机背后的天空或许可能是背景编程蓝色了，又或者船长长出翅膀了看起来像飞机。你可以从中得到结论，即卷积网络到底是用蓝天还是用翅膀来识别飞机的。然而事实并不是这样的，你从（上图）左往右，从上往下看，这里展示的每个分块都是基于对数概率的梯度上升，根据卷积网络模型，输入的是一架飞机，我们根据图像的输入的梯度来看。你大概已经习惯了模型参数梯度这一套东西，可以用反向传播的方法来计算输入图片的梯度是和计算参数梯度一样的流程。 

（上图）左上角这个船的动画，我们可以看到五个版图看上去基本上差不多，梯度下降看起来对图片一点影响都没有。但到最后一张图片，网络已经完全确信那是一架飞机，当你第一次写这种实验的代码时，尤其是你不知道会发生什么的时候，就感觉你的脚本里有 bug，那就把同样的图片一遍一遍地重复播放，我第一这样做的时候，我不敢相信正在发生的事，我不得不用 numpy 打开图片比较它们的区别确认它们确实是有区别的。我这里展示几个不同的动画，有船，车，猫和卡车。只有一个就是猫的图片，我看到真的有变化，猫脸的颜色有一点点变化，可能变得有一点点像飞机的金属色了。除开这个，其他的动画我是真的一点区别都没看出来，我也都没看出一丢丢飞机的意思。所以梯度下降并不是把输入变成飞机，而是发现了一张可以欺骗网络的，可以让网络认为输入是飞机的图片。如果我们是恶意的攻击者，我们都不用努力研究如何欺骗网络，我们只要要求网络给我们一张飞机的图片，它就会给我们一张图片，这张图片能够让它误以为这是一张飞机的图片。

---

![](https://i.loli.net/2018/09/15/5b9c659d31ca5.png)

当 Chrisian 第一次发表这个成果的时候，很多文章一起涌现出来，诸如 《The Flaw Looking At Every Deep Neural Network》或《Deep Learning has Deep Flasws》 之类的标题的文章。记住这些容易犯错的脆弱点很重要，把它运用在几乎每个我们已经研究过的机器学习算法上，比如说其中有 RBF 算法和配比函数密度估计，它们可以一定程度上抵抗这种影响，但即使是非常简单的机器学习算法，在对抗样本上也非常容易出错。

在（上面）这张图片里，我展示了当我们攻击一个线性模型的时候会发生什么。这根本不是一个深度学习算法，只是一个浅层的 softmax 模型，你就乘上一个矩阵加一个偏移向量，运用 softmax 函数，这样你就得到了10个 MNIST 分类的概率分布。在（上图）左上角，以 9 这个数字的图片开始，从左到右，从上到下，我慢慢地把它变成0。我画黄框的地方，模型认为这很可能是0。我忘记了我对于高可能性定义的那个阈值，我记得大概是0.9左右。接下来我们看第二行，我把它慢慢变成1，第二个黄框表明，我们成功欺骗到模型，使它认为那大概率是1。接下来看余下的图片，从左到右，从上到下，我们试验了二三四等等。直到右下角，我们有一个 9 用黄框标出来了。它看起来就是一个9，但就是因为它看起来像 9，完全是因为我们整个过程都围绕 9 的。我们基本上没有改变你数字的图像，至少人类是看不出区别，但已经成功欺骗了 MNIST 上的 10 个类。

这个线性模型太好骗了。

---

![](https://i.loli.net/2018/09/15/5b9c698256856.png)

除了线性模型，我们还发现了













---

[返回到上一页](./index.html) | [返回到顶部](./cs231n_Guest Lecture. Adversarial Examples and Adversarial Training.html)

---
<br>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
<br>

<script type="application/json" class="js-hypothesis-config">
  {
    "openSidebar": false,
    "showHighlights": true,
    "theme": classic,
    "enableExperimentalNewNoteButton": true
  }
</script>
<script async src="https://hypothes.is/embed.js"></script>