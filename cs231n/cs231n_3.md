---
title: CS231n Lecture.3
date: 2018-08-20
---

[返回到首页](../index.html)

---

[TOC]

> CS231n 课程的官方地址：http://cs231n.stanford.edu/index.html
>
> 该笔记根据的视频课程版本是 [Spring 2017](https://www.bilibili.com/video/av17204303/?p=7)(BiliBili)，PPt 资源版本是 [Spring 2018](http://cs231n.stanford.edu/syllabus.html).
>
> 另有该 Lecture 3. 扩展讲义资料：
>
> - [linear classification notes](http://cs231n.github.io/linear-classify)（[中译版](./CS231n_linear_classification_note.html)）
> - 



# Lecture 3. Loss Functions and Optimization

之前的内容已经讲了如何对每个样本图片进行线性参数化计算，最后让每个样本图片在各个分类上都有一个得分（数字）。那么如何使每一个样本的得分结果是正确的，并且要更加正确呢？这时候就需要定义**损失函数（loss function）** 来量化得分究竟有多么的正确，以及自动寻找最佳的参数使得损失函数的结果极值化的过程就是所谓的**优化（optimization）**。

小哥说了一句英文让我感觉很帅，默默的抄下来以后自己也可以用得上：

> ![](https://i.loli.net/2018/08/20/5b799ebf4cf80.png)
>
> So this loss function has kind of a funny functional form, so we'll walk through it in a bit more, in quite a bit of detail over the next couple of slides.
>
> 那么损失函数的样子还蛮有意思的，我们稍微费点心思来看看，接下来几张幻灯片里诸多细节。

很快小哥讲了一个 SVM 的多分类损失函数，企图用经典机器学习的一个简单例子让同志们了解损失函数究竟是如何计算和判断的。不过，从学生反应和弹幕的情况来看，听的人们似乎都很懵逼。。。。咱先把严格的定义抄录下来:

- Loss:

  - Given a dataset of examples $\{(x_i,y_i)\}^N_{i=1}$, where $x_i$ is image and $y_i$ is (integer) label.

  - Loss over the dataset is a sum of loss over examples:
    $$
    L=\frac{1}{N}\sum_iL_i(f(x_i,W),y_i)
    $$





- Multiclass SVM loss: （also Hinge Loss 合页损失）

  - Using the shorthand for the scores vector: $s=f(x_i,W)$

  - The SVM loss has the form:
    $$
    \begin{align}
    L_i  &= \sum_{j\neq y_i} 
    \left\{\begin{matrix}
    0 & \text{if } s_{y_i } \geq s_j +1 \\ 
     s_j-s_{y_i}+1& \text{otherwise}
    \end{matrix}\right.\\
    &= \sum_{j\neq y_i}\max(0,s_j-s_{y_i}+1)
    \end{align}
    $$
    ![](https://i.loli.net/2018/08/20/5b79a46dc5477.png)

  - Q: 为啥合页损失里的阈值要取1？

    - 其实这是可以任意选择的值。因为我们并不真正关心损失函数中得分的绝对数值，而是关心得分的相对数值，只关心正确分类能不能远远大于不正确分类的分数。所以实际上如果把整个 W 参数放大或缩小，那么所有的得分也都会相应地放大或缩小。（有详细推导）

  - Q: 合页损失的最大值和最小值？

    - 最小值是0（对应于所有的分类里，正确的得分都非常大），最大值是无穷大。

  - Q: 若初始化的参数非常的小，呈现较小的均匀分布的值，于是初期所有的得分都近乎为0并且差不多相等，那么合页损失预计会如何？

    - 损失函数值为分类的数量减去1.（代码调试策略，预期损失函数的值）

  - Q: 假如合页损失中的求和针对的是全部类别，包括 $j=y_i$，会怎样呢？

    - 损失函数增加了1

  - Q: 假如合页损失中的求和换成为取平均值，会怎样呢？

    - 不会变化。（因为取平均值和求和之间只相差一个倍数，即样本数目）

  - Q: 假如合页损失有一个平方操作：$ \sum_{j\neq y_i}\max(0,s_j-s_{y_i}+1)^2$，会怎样呢？

    - 会变得不同。（变成非线性了）

  

  - 废话不多说，直接上代码：

    ```python
    def L_i_vectorized(x, y, W):
        scores = W.dot(x)
        margins = np.maximum(0, scores - scores[y] + 1)
        margins[y] = 0
        loss_i = np.sum(margins)
        return loss_i
    ```




接下来的问题就很有趣了，关于实现相同效果的算法参数的唯一性问题。


- Q: Suppose that we found a W such that L=0. Is this W unique?
  - No! 2W is also has L=0!

既然不唯一，那么该如何选择 W 呢？















---

[返回到首页](../index.html) | [返回到顶部](./cs231n_3.html)

---
<br>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
<br>
<script type="application/json" class="js-hypothesis-config">
  {
    "openSidebar": false,
    "showHighlights": true,
    "theme": classic,
    "enableExperimentalNewNoteButton": true
  }
</script>
<script async src="https://hypothes.is/embed.js"></script>