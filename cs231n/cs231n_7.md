---
title: CS231n Lecture.7
date: 2018-08-27
---

[返回到上一页](./index.html)

---

[TOC]



> CS231n 课程的官方地址：http://cs231n.stanford.edu/index.html
>
> 该笔记根据的视频课程版本是 [Spring 2017](https://www.bilibili.com/video/av17204303/?p=16)(BiliBili)，PPt 资源版本是 [Spring 2018](http://cs231n.stanford.edu/syllabus.html).
>
> 另有该 Lecture 7. 扩展讲义资料：
>
> - **Tips and tricks for tuning NNs** [slides](https://docs.google.com/presentation/d/183aCHcSq-YsaokZrqI3khuy_zPbehG-XgkyA6L5W4t4/edit?usp=sharing)
> - [Neural Nets notes 3](http://cs231n.github.io/neural-networks-3/) ([中译版](./CS231n_Neural_Nets_notes_3.html))



# Lecture 7. Training Neural Networks, part 2





## More normalization (new topic in Spring 2018)







## Fancier optimization

之前学的 SGD 优化算法似乎很容易理解，但是它有很多问题：



### Problems with SGD

1. 损失函数对于不同参数梯度的敏感程度差异很大的话，损失值的变化在这种情况下会表现很坏：

   > Loss function has high **condition number**: ratio of largest to smallest singular value of the **Hessian** matrix is large. (海森矩阵中最大奇异值和最小奇异值之比)

   直观来看，损失函数像是个玉米🌽。梯度的方向并不是与局部最小值的直线，而是走 nasty zigzagging，所谓**之字形**运动。事实上，在高维参数空间中，这种情况很普遍。

   ![](https://i.loli.net/2018/08/27/5b8401d7d3290.png)

2. 存在**局部极小值**或**鞍点**（非凸优化问题）。

   - 梯度下降会 stuck 在局部极小值。

     ![](https://i.loli.net/2018/08/27/5b840313db264.png)

   - 在高维参数空间上，鞍点远比局部极小值麻烦的多。这意味着在鞍点处，某些方向上损失会增加，某些方向上会减小。这种情况在高维参数空间中会发生得更加频繁，基本上几乎在任何点上都会发生。然而在局部极小值点上，任何一个方向前进损失都会变大。事实上在考虑这种很高维的问题时，看起来似乎这种情况非常稀少。

   - 有时候问题并非恰好在鞍点上，也可能在鞍点附近。因为梯度实在太小，学习的前进会非常缓慢。

     ![](https://i.loli.net/2018/08/27/5b84032545c86.png)

   此处可引一篇 paper：Dauphin et al, “Identifying and attacking the saddle point problem in high-dimensional non-convex optimization”, NIPS 2014

3. **Stochastic**（随机性）是 SGD 的另一个问题。

   通常我们并不会在实践中真的一个一个实例的扔到网络中跑损失值和梯度估计，毕竟太慢了~ 而是通常用 mini-batch 方法来对梯度进行有噪声估计。可以实验，带有均匀噪声的 SGD 会对当前的附近损失空间造成一定扭曲，从而可能实际上需要花费很长时间才能得到极小值。

- Q：使用 GD （也就是 full batch gradient descent）的话，可以解决上述三大问题么？
  - 不会。有时候网络本身就存在明确的随机性，所以仍是一个问题；鞍点问题也仍会存在。



### SGD + Momentum

作为对比，先贴出 SGD：

![](https://i.loli.net/2018/08/27/5b840e0b85137.png)

然后，作为对比，贴出 SGD+Momentum 的两种等价表示：

![](https://i.loli.net/2018/08/27/5b840e411ba53.png)

在 SGD+Momentum 算法中，$v, \rho$ 分别作为“速度”和“摩擦系数”一般的存在，其中 $\rho$ 是一个新的超参数。

- Build up "velocity" as a running mean of gradients
- $\rho$ gives "friction" ; typically $\rho=0.9 \text{ or } 0.99$
- 可以较好的解决局部最小和鞍点的问题；效果表现明显比 SGD 更要好。

此处有 paper 一篇：Sutskever et al, “On the importance of initialization and momentum in deep learning”, ICML 2013




## Regularization







## Transfer Learning



















---

[返回到上一页](./index.html) | [返回到顶部](./cs231n_7.html)

---
<br>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
<br>

<script type="application/json" class="js-hypothesis-config">
  {
    "openSidebar": false,
    "showHighlights": true,
    "theme": classic,
    "enableExperimentalNewNoteButton": true
  }
</script>
<script async src="https://hypothes.is/embed.js"></script>