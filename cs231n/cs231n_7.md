---
title: CS231n Lecture.7
date: 2018-08-27
---

[返回到上一页](./index.html)

---

[TOC]



> CS231n 课程的官方地址：http://cs231n.stanford.edu/index.html
>
> 该笔记根据的视频课程版本是 [Spring 2017](https://www.bilibili.com/video/av17204303/?p=16)(BiliBili)，PPt 资源版本是 [Spring 2018](http://cs231n.stanford.edu/syllabus.html).
>
> 另有该 Lecture 7. 扩展讲义资料：
>
> - **Tips and tricks for tuning NNs** [slides](https://docs.google.com/presentation/d/183aCHcSq-YsaokZrqI3khuy_zPbehG-XgkyA6L5W4t4/edit?usp=sharing)
> - [Neural Nets notes 3](http://cs231n.github.io/neural-networks-3/) ([中译版](./CS231n_Neural_Nets_notes_3.html))



# Lecture 7. Training Neural Networks, part 2





## More normalization (new topic in Spring 2018)







## Fancier optimization

之前学的 SGD 优化算法似乎很容易理解，但是它有很多问题：



### Problems with SGD

1. 损失函数对于不同参数梯度的敏感程度差异很大的话，损失值的变化在这种情况下会表现很坏：

   > Loss function has high **condition number**: ratio of largest to smallest singular value of the **Hessian** matrix is large. (海森矩阵中最大奇异值和最小奇异值之比)

   直观来看，损失函数像是个玉米🌽。梯度的方向并不是与局部最小值的直线，而是走 nasty zigzagging，所谓**之字形**运动。事实上，在高维参数空间中，这种情况很普遍。

   ![](https://i.loli.net/2018/08/27/5b8401d7d3290.png)

2. 存在**局部极小值**或**鞍点**（非凸优化问题）。

   - 梯度下降会 stuck 在局部极小值。

     ![](https://i.loli.net/2018/08/27/5b840313db264.png)

   - 在高维参数空间上，鞍点远比局部极小值麻烦的多。这意味着在鞍点处，某些方向上损失会增加，某些方向上会减小。这种情况在高维参数空间中会发生得更加频繁，基本上几乎在任何点上都会发生。然而在局部极小值点上，任何一个方向前进损失都会变大。事实上在考虑这种很高维的问题时，看起来似乎这种情况非常稀少。

   - 有时候问题并非恰好在鞍点上，也可能在鞍点附近。因为梯度实在太小，学习的前进会非常缓慢。

     ![](https://i.loli.net/2018/08/27/5b84032545c86.png)

   此处可引一篇 paper：Dauphin et al, “Identifying and attacking the saddle point problem in high-dimensional non-convex optimization”, NIPS 2014

3. **Stochastic**（随机性）是 SGD 的另一个问题。

   通常我们并不会在实践中真的一个一个实例的扔到网络中跑损失值和梯度估计，毕竟太慢了~ 而是通常用 mini-batch 方法来对梯度进行有噪声估计。可以实验，带有均匀噪声的 SGD 会对当前的附近损失空间造成一定扭曲，从而可能实际上需要花费很长时间才能得到极小值。

- Q：使用 GD （也就是 full batch gradient descent）的话，可以解决上述三大问题么？
  - 不会。有时候网络本身就存在明确的随机性，所以仍是一个问题；鞍点问题也仍会存在。



### SGD + Momentum

作为对比，先贴出 SGD：

![](https://i.loli.net/2018/08/27/5b840e0b85137.png)

然后，作为对比，贴出 SGD+Momentum 的两种等价表示：

![](https://i.loli.net/2018/08/27/5b840e411ba53.png)

在 SGD+Momentum 算法中，$v, \rho$ 分别作为“速度”和“摩擦系数”一般的存在，其中 $\rho$ 是一个新的超参数。

之前提到过，SGD方法的一个缺点是其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定。Momentum算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力。

- Build up "velocity" as a running mean of gradients，是一个最近梯度加权平均的平滑移动，会随着最近的梯度权重越来越大。
- $\rho$ gives "friction" ; typically $\rho=0.9 \text{ or } 0.99$
- 可以较好的解决局部最小和鞍点的问题；效果表现明显比 SGD 更要好。

此处有 paper 一篇：Sutskever et al, “On the importance of initialization and momentum in deep learning”, ICML 2013

- Q：“速度”的初始值一般怎么取？
  - 基本上都是初始化到 0，它不算是超参数。





### Nesterov Momentum

这是在 SGD+Momentum 的基础上一个轻微变化，叫做 Nesterov accelerated gradient 或者 Nesterov momentum。

此处有 paper 2 篇：Nesterov, “A method of solving a convex programming problem with convergence rate O(1/k^2)”, 1983 和 Nesterov, “Introductory lectures on convex optimization: a basic course”, 2004 。

![](https://i.loli.net/2018/08/28/5b854c2324689.png)

梯度动量更新都是在红色点处，把历史梯度信息综合而成的“速度”在当前点与梯度结合，然后做出实际的更新运动。而 Nesterov 加速梯度下降是先在当前的 step 处用历史梯度信息综合而成的“速度”进行一次模拟更新运动，用运动后的梯度再与之前的“速度”结合做出实际当前 step 处的梯度更新运动。

公式就是这样的：

![](https://i.loli.net/2018/08/28/5b854ea097bca.png)

从后一个经过换元的新公式来看，我们可以说 Nesterov 动量包含了当前速度向量和先前速度向量的误差修正。

而且，你可以看到带动量的 SGD 和 Nesternov 动量的一个不同就是：由于 Nesternov 有校正因子的存在，与常规方法相比它不会那么剧烈的越过局部极小值点。

小哥开始了自问自答：

- Q：实际情况是如果你的局部极小点在一个非常窄的盆地里呢？上述的两种优化方法带来的“速度”能否让你越过这个局部极小点呢？

  - 最近这方面的理论研究有一些。但事实上这些非常极端的局部极值（所谓的坏点）我们的算法甚至不会经过这些点。因为实际上如果你遇到了一个非常极端的极值点，那么事实上你的训练很有可能已经过拟合了。如果我们能够扩大我们的训练数据集到两倍，那么整个优化函数的形状都会改变，以至于这个非常极端的极值点会消失，前提是如果我们收集更多的训练数据的话，我们可以得到的一个直觉判断就是我们愿意去靠近一个相对平缓的极值点，因为这样的极值点往往随着训练数据的变化有更好的鲁棒性。这样的平缓极值点往往针对测试数据有更好的泛化能力。

    某种意义上来说，跳过这些非常尖锐的极值点，这实际上是带动量的 SGD 的一个特性，而不是一个 bug。



### AdaGrad

AdaGrad 的核心思想是：在优化的过程中，需要保持一个在训练过程中的每一部梯度的平方和的持续估计。

与速度项不同的是，现在我们有了一个梯度平方项。在训练时，我们会一直累加当前梯度的平方到这个梯度平方项（历史上所有参数维度的梯度平方和）。当我们在更新我们的参数向量时，我们会除以这个梯度平方项。

![](https://i.loli.net/2018/08/28/5b8554e338f33.png)

- Q：对于 high condition number 时（如上图），AdaGrad 是如何缓解问题的呢？
  - 这个思想就是如果我们有两个坐标轴，沿其中一个轴我们有很高的梯度，而另一个轴方向却有很小的梯度，那么我们随着我们累加小梯度的平方，我们会在最后更新参数向量时除以一个很小的数字，从而加速了在小梯度对应的一个维度上的学习速度。然后在另一个维度方向上，由于梯度变得特别大，我们会除以一个非常大的数，所以我们会降低这个维度方向（Zigzaging 方向）上的训练速度。
- Q：当 t（时间）越来越大的时候，在训练过程中使用 AdaGrad 会发生什么问题？
  - 步长会变得越来越小。在学习目标是一个凸函数的情况下，有理论证明这个AdaGrad优化策略效果很好。因为当你接近极值点时，你会逐渐的慢下来最后达到收敛。这点是AdaGrad在凸函数情况下的一个很好的特性。但是在非凸函数的情况下，事情会变得复杂，因为当你到达一个局部的极值点时，使用 AdaGrad 会让你在这里被困住，从而使得训练过程无法再进行下去。
  - 所以，对 AdaGrad 有一个变体叫做 RMSProp。



### RMSProp

在这个算法中，我们依然计算梯度的平方，但是我们并不是仅仅简单的在训练中累加梯度平方，而是我们会让平方梯度按照一定比率下降。

![](https://i.loli.net/2018/08/28/5b855a040b834.png)

此处引用的是Tieleman and Hinton, 2012。

它看起来就和动量优化法很像，除了我们是给梯度的平方加上动量，而不是给梯度本身。 

所以说。。总之啦。。。我们不倾向于使用 AdaGrad。。。





















## Regularization







## Transfer Learning



















---

[返回到上一页](./index.html) | [返回到顶部](./cs231n_7.html)

---
<br>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
<br>

<script type="application/json" class="js-hypothesis-config">
  {
    "openSidebar": false,
    "showHighlights": true,
    "theme": classic,
    "enableExperimentalNewNoteButton": true
  }
</script>
<script async src="https://hypothes.is/embed.js"></script>